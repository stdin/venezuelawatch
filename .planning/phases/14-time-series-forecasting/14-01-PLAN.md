---
phase: 14-time-series-forecasting
plan: 01
type: execute
---

<objective>
Set up BigQuery ETL pipeline to sync entity risk data from PostgreSQL for Vertex AI training.

Purpose: Vertex AI Forecasting requires training data in BigQuery. Create the BigQuery dataset, table schema, and scheduled ETL to keep data synced.
Output: BigQuery table `intelligence.entity_risk_training_data` populated with historical entity risk scores, refreshed daily.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-time-series-forecasting/14-CONTEXT.md
@.planning/phases/14-time-series-forecasting/14-RESEARCH.md

**Tech stack available:**
- Google Cloud SDK (gcloud CLI)
- BigQuery Python client (google-cloud-bigquery)
- PostgreSQL with TimescaleDB (Cloud SQL)
- Django ORM for entity data models

**Constraining decisions:**
- Phase 1: TimescaleDB hypertables for time-series event data
- Phase 6: Entity model with risk_score field
- Phase 6: EntityMention model with mentioned_at timestamps

**From RESEARCH.md:**
- Vertex AI requires narrow (long) format: entity_id, mentioned_at, risk_score
- Daily granularity recommended
- Max 100 columns, 1K-100M rows, 100GB dataset
- ETL Option B chosen: BigQuery Federated Query (simpler than Dataflow)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BigQuery dataset and table schema</name>
  <files>backend/forecasting/__init__.py, backend/forecasting/bigquery_setup.py</files>
  <action>
    Create `backend/forecasting/` module with `bigquery_setup.py` script. Use google-cloud-bigquery client to:

    1. Create dataset `intelligence` (location: us-central1, skip if exists)
    2. Create table `entity_risk_training_data` with schema:
       - entity_id STRING NOT NULL (time series identifier)
       - mentioned_at TIMESTAMP NOT NULL (time column, daily granularity)
       - risk_score FLOAT64 NOT NULL (target for forecasting)
       - sanctions_risk FLOAT64 (dimensional breakdown)
       - political_risk FLOAT64
       - economic_risk FLOAT64
       - supply_chain_risk FLOAT64

    3. Add table description: "Entity risk scores aggregated daily for Vertex AI forecasting training"

    Use google.cloud.bigquery.Client() with default credentials (ADC). Schema as bigquery.SchemaField list. Set table.time_partitioning on mentioned_at (DAY) for query performance.

    Make script idempotent (check existence before creating). Add --project flag to specify GCP project.
  </action>
  <verify>
    1. Run: python backend/forecasting/bigquery_setup.py --project venezuelawatch
    2. Check: gcloud bigquery tables describe intelligence.entity_risk_training_data shows schema
    3. Verify: 7 columns present (entity_id, mentioned_at, risk_score, 4 risk dimensions)
  </verify>
  <done>BigQuery dataset and table created with correct schema. Script runs idempotently (can run multiple times safely). Table is partitioned by mentioned_at for query efficiency.</done>
</task>

<task type="auto">
  <name>Task 2: Set up scheduled query for PostgreSQL → BigQuery ETL</name>
  <files>backend/forecasting/etl_query.sql, backend/forecasting/setup_etl.py</files>
  <action>
    Create ETL using BigQuery Scheduled Queries with Federated Query (EXTERNAL_QUERY) to pull from PostgreSQL.

    1. Create `etl_query.sql` with SQL query:
       ```sql
       CREATE OR REPLACE TABLE `intelligence.entity_risk_training_data` AS
       SELECT
         CAST(em.entity_id AS STRING) as entity_id,
         TIMESTAMP(DATE(em.mentioned_at)) as mentioned_at,
         AVG(e.risk_score) as risk_score,
         AVG(CAST(em.event__risk_dimensions->>'sanctions' AS FLOAT64)) as sanctions_risk,
         AVG(CAST(em.event__risk_dimensions->>'political' AS FLOAT64)) as political_risk,
         AVG(CAST(em.event__risk_dimensions->>'economic' AS FLOAT64)) as economic_risk,
         AVG(CAST(em.event__risk_dimensions->>'supply_chain' AS FLOAT64)) as supply_chain_risk
       FROM EXTERNAL_QUERY(
         'projects/PROJECT_ID/locations/us-central1/connections/CONNECTION_ID',
         '''SELECT
              entity_id,
              mentioned_at,
              entities_entity.risk_score,
              events_event.risk_dimensions
            FROM entities_entitymention
            JOIN entities_entity ON entities_entity.id = entities_entitymention.entity_id
            JOIN events_event ON events_event.id = entities_entitymention.event_id
            WHERE mentioned_at >= CURRENT_DATE - INTERVAL ''90 days'' '''
       ) AS em
       JOIN EXTERNAL_QUERY(...) AS e ON e.id = em.entity_id
       GROUP BY entity_id, DATE(mentioned_at)
       ORDER BY entity_id, mentioned_at;
       ```

    2. Create `setup_etl.py` to configure BigQuery connection to Cloud SQL:
       - Use google.cloud.bigquery_connection_v1 to create CloudSqlProperties connection
       - Connection name: `projects/venezuelawatch/locations/us-central1/connections/postgresql-conn`
       - Database type: POSTGRES
       - Instance: Cloud SQL instance connection name from env (CLOUD_SQL_INSTANCE)
       - Database: venezuelawatch
       - Username/password: From Secret Manager (POSTGRES_USER, POSTGRES_PASSWORD secrets)

    3. Add scheduled query setup in setup_etl.py:
       - Use google.cloud.bigquery_datatransfer_v1.DataTransferServiceClient
       - Schedule: "every day 02:00" (daily at 2 AM UTC)
       - Query: Load from etl_query.sql, replace PROJECT_ID and CONNECTION_ID placeholders
       - Destination dataset: intelligence

    NOTE: Federated queries require BigQuery Connection API enabled. Script should check and provide error message if not enabled: "Enable BigQuery Connection API: gcloud services enable bigqueryconnection.googleapis.com"
  </action>
  <verify>
    1. Run: python backend/forecasting/setup_etl.py --project venezuelawatch
    2. Check: bq ls --transfer_config --project=venezuelawatch shows scheduled query
    3. Test: Manually trigger transfer config, verify data appears in BigQuery table
    4. Confirm: SELECT COUNT(*) FROM intelligence.entity_risk_training_data returns > 0
  </verify>
  <done>Scheduled query configured to run daily at 2 AM UTC. PostgreSQL connection configured with Cloud SQL credentials from Secret Manager. Manual test successfully loads entity risk data aggregated by day. Query pulls last 90 days of history for model training.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] BigQuery dataset `intelligence` exists
- [ ] Table `entity_risk_training_data` has correct schema (7 columns)
- [ ] Table is partitioned by mentioned_at
- [ ] Scheduled query configured and visible in transfer configs
- [ ] Manual test run loads data successfully
- [ ] Data aggregated daily (one row per entity per day)
</verification>

<success_criteria>
- All tasks completed
- BigQuery infrastructure created
- ETL pipeline configured to run daily
- Initial data load successful
- Ready for Vertex AI model training in next plan
</success_criteria>

<output>
After completion, create `.planning/phases/14-time-series-forecasting/14-01-SUMMARY.md`:

# Phase 14 Plan 1: BigQuery ETL Setup Summary

**BigQuery ETL pipeline operational: PostgreSQL entity risk data syncing daily to intelligence.entity_risk_training_data table**

## Accomplishments

- Created BigQuery dataset and table with Vertex AI-compatible schema
- Configured PostgreSQL federated query connection via BigQuery Connection API
- Set up scheduled daily ETL at 2 AM UTC (90-day rolling window)
- Successfully loaded initial entity risk history aggregated by day

## Files Created/Modified

- `backend/forecasting/__init__.py` - New forecasting module
- `backend/forecasting/bigquery_setup.py` - BigQuery infrastructure setup script
- `backend/forecasting/etl_query.sql` - Federated query for PostgreSQL → BigQuery ETL
- `backend/forecasting/setup_etl.py` - Scheduled query and connection configuration

## Decisions Made

- Chosen ETL Option B (BigQuery Federated Query) over Dataflow for simplicity
- Daily 2 AM UTC schedule chosen (low-traffic period, completes before business hours)
- 90-day rolling window provides sufficient training history without excessive storage costs
- Daily aggregation (vs hourly) aligns with Vertex AI's recommended granularity

## Issues Encountered

[Document any issues, or "None"]

## Next Step

Ready for 14-02-PLAN.md: Vertex AI Training Infrastructure
</output>

---
phase: 14-time-series-forecasting
plan: 02
type: execute
---

<objective>
Train Vertex AI forecasting model with TiDE architecture on entity risk data and deploy endpoint for predictions.

Purpose: Vertex AI AutoML will train a state-of-the-art TiDE model on historical entity risk scores, automatically selecting hyperparameters and architecture. Deploy as endpoint for on-demand forecasting.
Output: Trained Vertex AI model deployed to endpoint, ready for Django API integration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-time-series-forecasting/14-CONTEXT.md
@.planning/phases/14-time-series-forecasting/14-RESEARCH.md
@.planning/phases/14-time-series-forecasting/14-01-SUMMARY.md

**Tech stack available:**
- google-cloud-aiplatform SDK
- BigQuery table `intelligence.entity_risk_training_data` (from Plan 1)
- GCP project with Vertex AI API enabled

**From RESEARCH.md:**
- TiDE model: 10x faster training than previous Vertex AI models
- 30-day forecast horizon recommended
- Daily granularity with 80/10/10 train/val/test split
- Dimensional risk columns as features (sanctions_risk, political_risk, etc.)
- Training takes 1-4 hours depending on data size
- Endpoint deployment: n1-standard-4 with autoscaling (min 1, max 10 replicas)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Vertex AI SDK and create training script</name>
  <files>backend/requirements.txt, backend/forecasting/train_model.py</files>
  <action>
    1. Add to requirements.txt:
       ```
       google-cloud-aiplatform>=1.114.0
       ```

    2. Create `backend/forecasting/train_model.py` with training workflow:

    ```python
    from google.cloud import aiplatform
    import argparse

    def train_entity_risk_model(project_id: str, location: str = 'us-central1'):
        """Train Vertex AI forecasting model on BigQuery data."""

        aiplatform.init(project=project_id, location=location)

        # Create dataset from BigQuery table
        dataset = aiplatform.TimeSeriesDataset.create(
            display_name='entity-risk-forecasting',
            bq_source=f'bq://{project_id}.intelligence.entity_risk_training_data',
            time_column='mentioned_at',
            time_series_identifier_column='entity_id',
            target_column='risk_score'
        )

        # Define training job with AutoML
        training_job = aiplatform.AutoMLForecastingTrainingJob(
            display_name='entity-risk-tide-model',
            optimization_objective='minimize-rmse',
            column_specs={
                'sanctions_risk': 'numeric',
                'political_risk': 'numeric',
                'economic_risk': 'numeric',
                'supply_chain_risk': 'numeric'
            }
        )

        # Train model (1-4 hours)
        model = training_job.run(
            dataset=dataset,
            target_column='risk_score',
            time_column='mentioned_at',
            time_series_identifier_column='entity_id',
            forecast_horizon=30,  # 30 days
            data_granularity_unit='day',
            data_granularity_count=1,
            training_fraction_split=0.8,
            validation_fraction_split=0.1,
            test_fraction_split=0.1,
            budget_milli_node_hours=1000,  # Auto-scales training time
        )

        print(f"Model trained: {model.resource_name}")
        print(f"Model display name: {model.display_name}")

        return model

    if __name__ == '__main__':
        parser = argparse.ArgumentParser()
        parser.add_argument('--project', required=True, help='GCP project ID')
        parser.add_argument('--location', default='us-central1', help='GCP region')
        args = parser.parse_args()

        train_entity_risk_model(args.project, args.location)
    ```

    Use google-cloud-aiplatform.TimeSeriesDataset for forecasting datasets. AutoMLForecastingTrainingJob automatically selects TiDE architecture. Budget of 1000 milli-node-hours allows sufficient training time (1-4 hours typical). Forecast horizon=30 matches CONTEXT.md requirement (30-day trajectories).

    Install dependencies: pip install -r backend/requirements.txt
  </action>
  <verify>
    1. Check: pip list | grep google-cloud-aiplatform shows >=1.114.0
    2. Validate: python -c "from google.cloud import aiplatform; print('OK')" succeeds
    3. Check: python backend/forecasting/train_model.py --help shows usage
  </verify>
  <done>Vertex AI SDK installed. Training script created with AutoML forecasting job configuration. Script accepts project and location arguments. Ready to initiate training (manual step due to 1-4 hour duration).</done>
</task>

<task type="checkpoint:human-action" gate="blocking">
  <action>Initiate Vertex AI model training (1-4 hour process)</action>
  <instructions>
    I've created the training script. Training takes 1-4 hours and costs ~$50-100.

    To start training:
    1. Run: python backend/forecasting/train_model.py --project venezuelawatch
    2. Monitor: Visit https://console.cloud.google.com/vertex-ai/training/training-pipelines
    3. Wait: Training completes (console shows "Succeeded" status)

    The script will print the model resource name when complete.
  </instructions>
  <verification>Check Vertex AI console shows model with "Succeeded" status</verification>
  <resume-signal>Type "done" and paste the model resource name when training completes</resume-signal>
</task>

<task type="auto">
  <name>Task 2: Deploy model endpoint for predictions</name>
  <files>backend/forecasting/deploy_endpoint.py, backend/config/settings/base.py</files>
  <action>
    Create `backend/forecasting/deploy_endpoint.py` to deploy trained model:

    ```python
    from google.cloud import aiplatform
    import argparse

    def deploy_forecasting_endpoint(
        project_id: str,
        model_name: str,
        location: str = 'us-central1'
    ):
        """Deploy trained model to endpoint for predictions."""

        aiplatform.init(project=project_id, location=location)

        # Get trained model
        model = aiplatform.Model(model_name)

        # Deploy to endpoint with autoscaling
        endpoint = model.deploy(
            machine_type='n1-standard-4',
            min_replica_count=1,
            max_replica_count=10,
            accelerator_type=None,  # No GPU needed for TiDE inference
            traffic_percentage=100,
            deploy_request_timeout=1800,  # 30 min timeout for deployment
        )

        print(f"Endpoint deployed: {endpoint.resource_name}")
        print(f"Endpoint display name: {endpoint.display_name}")
        print(f"\nAdd to settings:")
        print(f"VERTEX_AI_ENDPOINT_ID = '{endpoint.resource_name}'")

        return endpoint

    if __name__ == '__main__':
        parser = argparse.ArgumentParser()
        parser.add_argument('--project', required=True)
        parser.add_argument('--model', required=True, help='Model resource name from training')
        parser.add_argument('--location', default='us-central1')
        args = parser.parse_args()

        deploy_forecasting_endpoint(args.project, args.model, args.location)
    ```

    After deployment succeeds, add to `backend/config/settings/base.py`:
    ```python
    # Vertex AI Forecasting
    VERTEX_AI_ENDPOINT_ID = env.str('VERTEX_AI_ENDPOINT_ID', default='')
    VERTEX_AI_PROJECT_ID = env.str('GCP_PROJECT_ID', default='venezuelawatch')
    VERTEX_AI_LOCATION = env.str('VERTEX_AI_LOCATION', default='us-central1')
    ```

    Deployment creates endpoint with autoscaling (1-10 replicas) on n1-standard-4 machines. No GPU needed for TiDE inference. Traffic_percentage=100 routes all requests to this model version. Deployment takes 10-15 minutes.

    Run: python backend/forecasting/deploy_endpoint.py --project venezuelawatch --model [MODEL_RESOURCE_NAME]
  </action>
  <verify>
    1. Check: Deployment script completes without errors
    2. Verify: gcloud ai endpoints list --region=us-central1 shows endpoint
    3. Confirm: Console shows endpoint "Ready" status
    4. Check: backend/config/settings/base.py contains VERTEX_AI_ENDPOINT_ID setting
  </verify>
  <done>Model deployed to Vertex AI endpoint with autoscaling. Endpoint resource name stored in Django settings. Endpoint ready to receive prediction requests. Settings configured for Django integration in next plan.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] google-cloud-aiplatform SDK installed (>=1.114.0)
- [ ] Training script created and validated
- [ ] Model training completed successfully (console shows "Succeeded")
- [ ] Endpoint deployed and in "Ready" state
- [ ] VERTEX_AI_ENDPOINT_ID configured in Django settings
- [ ] Endpoint visible in gcloud ai endpoints list
</verification>

<success_criteria>
- All tasks completed
- Vertex AI model trained with TiDE architecture
- Model deployed to autoscaling endpoint (1-10 replicas)
- Endpoint resource name captured for Django integration
- Ready for API implementation in next plan
</success_criteria>

<output>
After completion, create `.planning/phases/14-time-series-forecasting/14-02-SUMMARY.md`:

# Phase 14 Plan 2: Vertex AI Training Infrastructure Summary

**Vertex AI TiDE model trained and deployed: entity risk forecasting endpoint operational with autoscaling**

## Accomplishments

- Installed Vertex AI SDK and created training/deployment scripts
- Trained AutoML forecasting model on 90 days of entity risk history
- Model automatically selected TiDE architecture (10x faster training)
- Deployed endpoint with n1-standard-4 autoscaling (1-10 replicas)
- Configured endpoint settings in Django for API integration

## Files Created/Modified

- `backend/requirements.txt` - Added google-cloud-aiplatform>=1.114.0
- `backend/forecasting/train_model.py` - AutoML training workflow
- `backend/forecasting/deploy_endpoint.py` - Endpoint deployment script
- `backend/config/settings/base.py` - Added VERTEX_AI_ENDPOINT_ID, VERTEX_AI_PROJECT_ID, VERTEX_AI_LOCATION

## Decisions Made

- TiDE model automatically selected by AutoML (SOTA for time-series)
- 30-day forecast horizon matches CONTEXT.md requirements
- n1-standard-4 machine type (no GPU needed for inference)
- Autoscaling 1-10 replicas balances cost and availability
- 80/10/10 train/val/test split for robust model evaluation

## Issues Encountered

[Document training duration, any warnings, model metrics, or "None"]

## Next Step

Ready for 14-03-PLAN.md: Django Forecasting API
</output>

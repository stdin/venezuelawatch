---
phase: 14.2-gdelt-bigquery-native
plan: 01
title: Migrate to GDELT Native BigQuery Dataset
created: 2026-01-10
---

# Phase 14.2 Plan 01: GDELT Native BigQuery Migration

## Context

**Current Implementation:**
- Custom GDELT DOC API v2.0 polling (15-min intervals)
- Query: "(venezuela OR maduro OR caracas)"
- Fetches up to 250 articles per request
- Maps to events and stores in our BigQuery dataset
- Limited data: title, URL, publish date only

**GDELT Native BigQuery Dataset:**
- Dataset: `gdelt-bq.gdeltv2`
- Tables: `events_partitioned`, `eventmentions_partitioned`, `gkg_partitioned`
- Updated every 15 minutes (same as API)
- Venezuela filter: `ActionGeo_CountryCode='VE'`
- Performance: `_PARTITIONTIME` for efficient queries
- Rich data: 2,300+ themes/emotions, 65 languages, quotes, images

**Benefits of Migration:**
1. **Richer data**: Access full GDELT event schema (58 fields) vs DOC API (3-4 fields)
2. **Better performance**: Direct BigQuery federation vs API polling + deduplication
3. **Lower latency**: Query on-demand vs 15-min polling cycle
4. **Multi-language**: 65 languages translated in real-time
5. **Historical access**: Query any time range, not just last 15 minutes
6. **Simpler architecture**: Eliminate custom ingestion task
7. **Advanced features**: Themes, emotions, quotes, actors, geographic precision
8. **Cost efficiency**: No API rate limits, pay only for queries scanned

## Objective

Replace custom GDELT DOC API ingestion with BigQuery federated queries to `gdelt-bq.gdeltv2`, unlocking richer event data and eliminating ingestion infrastructure.

## Current State

- GDELT ingestion via DOC API: `backend/data_pipeline/tasks/gdelt_tasks.py:ingest_gdelt_events()`
- Celery Beat schedule: Every 15 minutes
- Data stored in: `{project}.{dataset}.events` table
- Deduplication: Query BigQuery for existing URLs (7-day window)
- Post-processing: LLM analysis for risk/severity scoring

## Target State

- GDELT events queried directly from `gdelt-bq.gdeltv2.events_partitioned`
- Federated BigQuery queries with `ActionGeo_CountryCode='VE'` filter
- Scheduled BigQuery job (Cloud Scheduler) to sync events every 15 minutes
- Rich event data: themes, emotions, actors, QuadClass, GoldsteinScale
- Eliminate `ingest_gdelt_events` Celery task
- Maintain same downstream processing (LLM analysis, entity extraction)

## Tasks

### Task 1: Create GDELT BigQuery federation service

**What:** New service layer for querying GDELT native BigQuery dataset

**Files:**
- `backend/api/services/gdelt_bigquery_service.py` (create)

**Implementation:**
```python
"""
GDELT Native BigQuery service for querying gdelt-bq.gdeltv2 dataset.

Provides methods for fetching Venezuela-related events from GDELT's native
BigQuery dataset with efficient partitioning and rich event data.
"""
from google.cloud import bigquery
from django.conf import settings
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class GDELTBigQueryService:
    """Service for querying GDELT native BigQuery dataset."""

    def __init__(self):
        self.client = bigquery.Client(project=settings.GCP_PROJECT_ID)
        self.gdelt_project = "gdelt-bq"
        self.gdelt_dataset = "gdeltv2"

    def get_venezuela_events(
        self,
        start_time: datetime,
        end_time: datetime,
        limit: int = 1000
    ) -> List[Dict[str, Any]]:
        """
        Get Venezuela-related events from GDELT native BigQuery.

        Uses events_partitioned table with:
        - _PARTITIONTIME for efficient querying
        - ActionGeo_CountryCode='VE' for Venezuela
        - Actor1CountryCode or Actor2CountryCode='VE' for Venezuelan actors

        Args:
            start_time: Start of time range
            end_time: End of time range
            limit: Max events to return

        Returns:
            List of event dicts with GDELT schema
        """
        query = f"""
            SELECT
                GLOBALEVENTID,
                SQLDATE,
                MonthYear,
                Year,
                FractionDate,
                Actor1Code,
                Actor1Name,
                Actor1CountryCode,
                Actor1Type1Code,
                Actor2Code,
                Actor2Name,
                Actor2CountryCode,
                Actor2Type1Code,
                IsRootEvent,
                EventCode,
                EventBaseCode,
                EventRootCode,
                QuadClass,
                GoldsteinScale,
                NumMentions,
                NumSources,
                NumArticles,
                AvgTone,
                Actor1Geo_Type,
                Actor1Geo_FullName,
                Actor1Geo_CountryCode,
                Actor1Geo_Lat,
                Actor1Geo_Long,
                Actor2Geo_Type,
                Actor2Geo_FullName,
                Actor2Geo_CountryCode,
                Actor2Geo_Lat,
                Actor2Geo_Long,
                ActionGeo_Type,
                ActionGeo_FullName,
                ActionGeo_CountryCode,
                ActionGeo_Lat,
                ActionGeo_Long,
                DATEADDED,
                SOURCEURL
            FROM `{self.gdelt_project}.{self.gdelt_dataset}.events_partitioned`
            WHERE _PARTITIONTIME >= @start_time
            AND _PARTITIONTIME < @end_time
            AND (
                ActionGeo_CountryCode = 'VE'
                OR Actor1CountryCode = 'VE'
                OR Actor2CountryCode = 'VE'
            )
            ORDER BY DATEADDED DESC
            LIMIT @limit
        """

        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter('start_time', 'TIMESTAMP', start_time),
                bigquery.ScalarQueryParameter('end_time', 'TIMESTAMP', end_time),
                bigquery.ScalarQueryParameter('limit', 'INT64', limit)
            ]
        )

        try:
            results = self.client.query(query, job_config=job_config).result()
            events = [dict(row) for row in results]
            logger.info(f"Fetched {len(events)} Venezuela events from GDELT BigQuery")
            return events
        except Exception as e:
            logger.error(f"Failed to query GDELT BigQuery: {e}", exc_info=True)
            raise

    def get_event_mentions(
        self,
        global_event_id: str,
        start_time: datetime,
        end_time: datetime
    ) -> List[Dict[str, Any]]:
        """
        Get all mentions of a specific GDELT event.

        Args:
            global_event_id: GDELT GLOBALEVENTID
            start_time: Start of time range
            end_time: End of time range

        Returns:
            List of mention dicts with URLs, tone, etc.
        """
        query = f"""
            SELECT
                GLOBALEVENTID,
                EventTimeDate,
                MentionTimeDate,
                MentionType,
                MentionSourceName,
                MentionIdentifier,
                SentenceID,
                Actor1CharOffset,
                Actor2CharOffset,
                ActionCharOffset,
                InRawText,
                Confidence,
                MentionDocLen,
                MentionDocTone
            FROM `{self.gdelt_project}.{self.gdelt_dataset}.eventmentions_partitioned`
            WHERE _PARTITIONTIME >= @start_time
            AND _PARTITIONTIME < @end_time
            AND GLOBALEVENTID = @event_id
            ORDER BY MentionTimeDate DESC
        """

        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter('start_time', 'TIMESTAMP', start_time),
                bigquery.ScalarQueryParameter('end_time', 'TIMESTAMP', end_time),
                bigquery.ScalarQueryParameter('event_id', 'STRING', global_event_id)
            ]
        )

        try:
            results = self.client.query(query, job_config=job_config).result()
            mentions = [dict(row) for row in results]
            return mentions
        except Exception as e:
            logger.error(f"Failed to query GDELT event mentions: {e}", exc_info=True)
            raise


# Singleton instance
gdelt_bigquery_service = GDELTBigQueryService()
```

**Acceptance:**
- Service can query `gdelt-bq.gdeltv2.events_partitioned`
- Venezuela filter works (ActionGeo, Actor1, Actor2 country codes)
- Efficient partitioning with `_PARTITIONTIME`
- Returns rich GDELT event data (58 fields)

---

### Task 2: Create GDELT sync task for periodic ingestion

**What:** Replace `ingest_gdelt_events` with BigQuery-to-BigQuery sync task

**Files:**
- `backend/data_pipeline/tasks/gdelt_sync_task.py` (create)
- `backend/data_pipeline/tasks/gdelt_tasks.py` (deprecate/comment out)

**Implementation:**
```python
"""
GDELT BigQuery sync task - replace DOC API polling with native BigQuery queries.

Syncs Venezuela-related events from gdelt-bq.gdeltv2 to our events table.
Runs every 15 minutes to match GDELT update frequency.
"""
import logging
import uuid
from typing import Dict, Any
from celery import shared_task
from django.utils import timezone
from datetime import timedelta

from data_pipeline.tasks.base import BaseIngestionTask
from api.services.gdelt_bigquery_service import gdelt_bigquery_service
from api.bigquery_models import Event as BigQueryEvent
from api.services.bigquery_service import bigquery_service

logger = logging.getLogger(__name__)


def get_intelligence_task():
    """Lazy import to avoid circular dependency."""
    from data_pipeline.tasks.intelligence_tasks import analyze_event_intelligence
    return analyze_event_intelligence


@shared_task(base=BaseIngestionTask, bind=True)
def sync_gdelt_events(self, lookback_minutes: int = 15) -> Dict[str, Any]:
    """
    Sync Venezuela events from GDELT native BigQuery to our dataset.

    Replaces custom DOC API polling with direct BigQuery federation.

    Args:
        lookback_minutes: How many minutes to look back (default: 15)

    Returns:
        {
            'events_created': int,
            'events_skipped': int,
            'events_fetched': int
        }
    """
    logger.info(f"Starting GDELT BigQuery sync (lookback: {lookback_minutes} minutes)")

    # Time range for query
    end_time = timezone.now()
    start_time = end_time - timedelta(minutes=lookback_minutes)

    try:
        # Fetch Venezuela events from GDELT native BigQuery
        gdelt_events = gdelt_bigquery_service.get_venezuela_events(
            start_time=start_time,
            end_time=end_time,
            limit=1000  # Higher than DOC API limit of 250
        )

        logger.info(f"Fetched {len(gdelt_events)} events from GDELT BigQuery")

        events_created = 0
        events_skipped = 0
        bigquery_events = []

        for gdelt_event in gdelt_events:
            # Check for duplicates using GLOBALEVENTID
            try:
                existing_query = f"""
                    SELECT COUNT(*) as count
                    FROM `{bigquery_service.project_id}.{bigquery_service.dataset_id}.events`
                    WHERE id = @event_id
                """
                from google.cloud import bigquery
                job_config = bigquery.QueryJobConfig(
                    query_parameters=[
                        bigquery.ScalarQueryParameter('event_id', 'STRING', str(gdelt_event['GLOBALEVENTID']))
                    ]
                )
                results = bigquery_service.client.query(existing_query, job_config=job_config).result()
                row = list(results)[0]
                if row.count > 0:
                    logger.debug(f"Skipping duplicate GDELT event: {gdelt_event['GLOBALEVENTID']}")
                    events_skipped += 1
                    continue
            except Exception as e:
                logger.error(f"Failed to check for duplicate: {e}")
                # Continue with insert
                pass

            # Map GDELT event to our BigQuery schema
            try:
                # Parse GDELT date format (YYYYMMDDHHMMSS)
                date_str = str(gdelt_event['DATEADDED'])
                event_date = timezone.datetime.strptime(date_str[:8], '%Y%m%d').replace(tzinfo=timezone.utc)

                # Generate title from actors and event code
                title = f"{gdelt_event.get('Actor1Name', 'Unknown')} - {gdelt_event.get('Actor2Name', 'Event')} ({gdelt_event.get('EventCode', '')})"

                # Map QuadClass to event_type
                quad_class = gdelt_event.get('QuadClass')
                event_type_map = {
                    1: 'political',  # Verbal Cooperation
                    2: 'political',  # Material Cooperation
                    3: 'political',  # Verbal Conflict
                    4: 'political'   # Material Conflict
                }
                event_type = event_type_map.get(quad_class, 'other')

                # Create BigQueryEvent
                bq_event = BigQueryEvent(
                    id=str(gdelt_event['GLOBALEVENTID']),  # Use GDELT ID
                    source_url=gdelt_event.get('SOURCEURL', ''),
                    mentioned_at=event_date,
                    created_at=timezone.now(),
                    title=title[:500],  # Truncate if needed
                    content=f"GDELT Event: {gdelt_event.get('EventCode', '')} - Tone: {gdelt_event.get('AvgTone', 0)}",
                    source_name='GDELT',
                    event_type=event_type,
                    location=gdelt_event.get('ActionGeo_FullName', 'Venezuela'),
                    risk_score=None,  # Computed by LLM
                    severity=None,    # Computed by LLM
                    metadata={
                        'goldstein_scale': gdelt_event.get('GoldsteinScale'),
                        'avg_tone': gdelt_event.get('AvgTone'),
                        'num_mentions': gdelt_event.get('NumMentions'),
                        'num_sources': gdelt_event.get('NumSources'),
                        'num_articles': gdelt_event.get('NumArticles'),
                        'quad_class': quad_class,
                        'actor1_code': gdelt_event.get('Actor1Code'),
                        'actor1_name': gdelt_event.get('Actor1Name'),
                        'actor2_code': gdelt_event.get('Actor2Code'),
                        'actor2_name': gdelt_event.get('Actor2Name'),
                        'event_code': gdelt_event.get('EventCode'),
                        'action_geo_lat': gdelt_event.get('ActionGeo_Lat'),
                        'action_geo_long': gdelt_event.get('ActionGeo_Long')
                    }
                )

                bigquery_events.append(bq_event)
                events_created += 1

            except Exception as e:
                logger.error(f"Failed to map GDELT event: {e}", exc_info=True)
                events_skipped += 1

        # Batch insert to BigQuery
        if bigquery_events:
            try:
                bigquery_service.insert_events(bigquery_events)
                logger.info(f"Inserted {len(bigquery_events)} events to BigQuery")

                # Dispatch LLM analysis for each event
                analyze_task = get_intelligence_task()
                for event in bigquery_events:
                    analyze_task.delay(event.id, model='fast')
                    logger.debug(f"Dispatched LLM analysis for GDELT event {event.id}")

            except Exception as e:
                logger.error(f"Failed to insert events to BigQuery: {e}", exc_info=True)
                raise

        result = {
            'events_created': events_created,
            'events_skipped': events_skipped,
            'events_fetched': len(gdelt_events)
        }

        logger.info(
            f"GDELT sync complete: {events_created} created, {events_skipped} skipped"
        )

        return result

    except Exception as e:
        logger.error(f"GDELT sync failed: {e}", exc_info=True)
        raise


# Deprecated: Old DOC API ingestion task
# from data_pipeline.tasks.gdelt_tasks import ingest_gdelt_events
# Use sync_gdelt_events instead
```

**Acceptance:**
- `sync_gdelt_events` fetches events from GDELT BigQuery
- Deduplication by GLOBALEVENTID
- Maps GDELT schema to our BigQuery events table
- Preserves LLM analysis dispatch
- Old `ingest_gdelt_events` deprecated

---

### Task 3: Update Celery Beat schedule and Cloud Scheduler

**What:** Replace GDELT DOC API task with BigQuery sync task

**Files:**
- `backend/venezuelawatch/settings.py` (modify CELERY_BEAT_SCHEDULE)
- `backend/config/cloud_scheduler.yaml` (modify if using Cloud Scheduler)

**Changes:**

**settings.py:**
```python
# OLD (remove):
# 'gdelt-ingestion': {
#     'task': 'data_pipeline.tasks.gdelt_tasks.ingest_gdelt_events',
#     'schedule': crontab(minute='*/15'),  # Every 15 minutes
# },

# NEW (add):
'gdelt-sync': {
    'task': 'data_pipeline.tasks.gdelt_sync_task.sync_gdelt_events',
    'schedule': crontab(minute='*/15'),  # Every 15 minutes
    'kwargs': {'lookback_minutes': 15},
},
```

**cloud_scheduler.yaml (if applicable):**
```yaml
# OLD (remove):
# - name: gdelt-ingestion
#   schedule: "*/15 * * * *"
#   timezone: UTC
#   http_target:
#     uri: https://your-app.com/api/trigger/gdelt-ingestion
#     http_method: POST

# NEW (add):
- name: gdelt-sync
  schedule: "*/15 * * * *"
  timezone: UTC
  http_target:
    uri: https://your-app.com/api/trigger/gdelt-sync
    http_method: POST
```

**Acceptance:**
- Celery Beat uses `sync_gdelt_events` instead of `ingest_gdelt_events`
- Schedule remains every 15 minutes
- Cloud Scheduler updated if applicable

---

### Task 4: Test and validate GDELT BigQuery migration

**What:** Run sync task manually, validate data quality, verify downstream processing

**Commands:**
```bash
# Run sync task manually
python manage.py shell
>>> from data_pipeline.tasks.gdelt_sync_task import sync_gdelt_events
>>> result = sync_gdelt_events(lookback_minutes=60)
>>> print(result)

# Verify events in BigQuery
python manage.py shell
>>> from api.services.bigquery_service import bigquery_service
>>> from django.utils import timezone
>>> from datetime import timedelta
>>> events = bigquery_service.get_recent_events(
...     start_date=timezone.now() - timedelta(hours=1),
...     end_date=timezone.now()
... )
>>> print(f"Found {len(events)} events")
>>> print(events[0] if events else "No events")

# Check LLM analysis triggered
# (Check Celery logs for analyze_event_intelligence tasks)
```

**Validation:**
- Events fetched from GDELT BigQuery
- Events inserted to our BigQuery dataset
- Event schema includes rich GDELT fields (metadata)
- LLM analysis triggered for risk/severity scoring
- No duplicate events created
- Performance acceptable (query time < 10s)

**Acceptance:**
- Manual sync succeeds with lookback_minutes=60
- Events appear in our BigQuery events table
- Metadata contains GDELT fields (GoldsteinScale, AvgTone, etc.)
- LLM analysis dispatched successfully
- `python manage.py check` passes

---

## Success Criteria

- [ ] `GDELTBigQueryService` created and can query `gdelt-bq.gdeltv2`
- [ ] `sync_gdelt_events` task created and tested
- [ ] Celery Beat schedule updated to use new sync task
- [ ] Manual test shows events syncing from GDELT BigQuery
- [ ] Rich GDELT metadata preserved (GoldsteinScale, AvgTone, actors, etc.)
- [ ] LLM analysis still triggered for new events
- [ ] Old `ingest_gdelt_events` deprecated (commented out)
- [ ] `python manage.py check` passes
- [ ] No regressions in API endpoints or chat tools

## Risks & Mitigations

**Risk:** GDELT BigQuery query costs exceed free tier
**Mitigation:** Use `_PARTITIONTIME` filter, limit to 15-min windows, monitor quota

**Risk:** GDELT schema changes break our mapping
**Mitigation:** Robust error handling, log failures, fallback to DOC API if needed

**Risk:** Deduplication by GLOBALEVENTID fails
**Mitigation:** Add secondary URL-based deduplication check

**Risk:** Performance degrades with large result sets
**Mitigation:** Limit queries to 1000 events, use pagination if needed

## Dependencies

- GCP BigQuery API enabled
- Access to `gdelt-bq.gdeltv2` dataset (public dataset, no auth needed)
- Existing BigQuery service layer (Phase 14.1)
- Celery Beat configured

## Notes

- GDELT BigQuery dataset is **public** - no authentication required beyond GCP project credentials
- GDELT updates every 15 minutes - match our sync frequency
- Rich event data unlocks future enhancements:
  - Theme/emotion analysis (2,300+ categories)
  - Actor network analysis (Actor1/Actor2)
  - Geographic precision (lat/long)
  - Tone sentiment (AvgTone field)
  - Conflict severity (QuadClass, GoldsteinScale)
- Future optimization: Query GKG table for even richer context (themes, images, quotes)
- Future optimization: Use `eventmentions_partitioned` to track media coverage over time

## Estimated Duration

2-3 hours (single plan, straightforward migration)

---
phase: 14.3-complete-event-migration-bigquery
plan: 01
type: execute
---

<objective>
Complete the event migration to BigQuery by updating all processing tasks (entity extraction, intelligence analysis, sanctions screening) to query BigQuery instead of PostgreSQL Event model.

Purpose: Achieve full polyglot persistence architecture where PostgreSQL handles transactional data (Django auth, Entity metadata) and BigQuery handles all time-series analytics (events, entity mentions, economic data). This completes the migration started in Phase 14.1 and enables Phase 15 correlation analysis across unified BigQuery data.

Output: All Celery processing tasks querying BigQuery, PostgreSQL Event model deprecated with documentation notice, end-to-end pipeline validated.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 14.1 BigQuery migration patterns (established patterns to follow)
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-02-SUMMARY.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-03-SUMMARY.md

# Existing files to migrate
@backend/data_pipeline/tasks/entity_extraction.py
@backend/data_pipeline/tasks/intelligence_tasks.py
@backend/data_pipeline/tasks/sanctions_tasks.py

# BigQuery service (migration target)
@backend/api/services/bigquery_service.py

**Current architecture:**
- Phase 14.1-02: Ingestion tasks (GDELT, ReliefWeb, FRED, UN Comtrade, World Bank) write to BigQuery ✅
- Phase 14.1-03: API views query BigQuery ✅
- Phase 14.2: GDELT uses native BigQuery dataset ✅
- **Phase 14.3 (this phase):** Processing tasks still read from PostgreSQL Event.objects ❌

**Migration target:**
- `entity_extraction.py`: Event.objects.get(id=event_id) → bigquery_service.get_event_by_id()
- `entity_extraction.py`: Event.objects.filter() batch queries → BigQuery queries
- `intelligence_tasks.py`: Event.objects queries → BigQuery queries
- `sanctions_tasks.py`: Event.objects queries → BigQuery queries

**Established patterns from Phase 14.1:**
- Use `bigquery_service.get_event_by_id(event_id)` for single event lookups
- Use parameterized BigQuery queries for filtering (prevent SQL injection)
- Return dict-based results (not Django model instances)
- Entity metadata remains in PostgreSQL (canonical_name, entity_type, aliases)
- Preserve existing processing logic, only change data source
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate entity extraction task to BigQuery</name>
  <files>backend/data_pipeline/tasks/entity_extraction.py, backend/api/services/bigquery_service.py</files>
  <action>
Update entity_extraction.py to query BigQuery instead of PostgreSQL Event model:

1. In `extract_entities_from_event()`:
   - Replace `Event.objects.get(id=event_id)` with `bigquery_service.get_event_by_id(event_id)`
   - Handle dict response instead of Django model (access fields as dict keys)
   - Preserve all processing logic (entity extraction, fuzzy matching, EntityMention creation)

2. In `extract_entities_batch()` (if exists):
   - Replace `Event.objects.filter()` with BigQuery query via bigquery_service
   - Add `bigquery_service.get_events_batch(event_ids)` method if needed
   - Maintain batch processing efficiency

3. BigQuery service additions (if needed):
   - Add `get_event_by_id(event_id: str) -> dict` if not already present
   - Return event dict with all fields needed by entity extraction (id, entities, llm_analysis, timestamp)
   - Use parameterized query: `WHERE id = @event_id`

**What to avoid:**
- Don't change EntityMention creation logic (still writes to PostgreSQL - reference data)
- Don't modify fuzzy matching or entity normalization algorithms
- Don't change Redis trending updates (deprecated in Phase 14.1-03)
</action>
  <verify>
Run entity extraction on test event:
```bash
python manage.py shell -c "
from data_pipeline.tasks.entity_extraction import extract_entities_from_event
# Use a real event ID from BigQuery
result = extract_entities_from_event.delay('test-event-id')
print(result.get())
"
```
Should return extraction results without PostgreSQL Event queries.
</verify>
  <done>
- entity_extraction.py imports bigquery_service
- All Event.objects calls replaced with bigquery_service calls
- Entity extraction works with BigQuery event data
- No errors or warnings
</done>
</task>

<task type="auto">
  <name>Task 2: Migrate intelligence and sanctions tasks to BigQuery</name>
  <files>backend/data_pipeline/tasks/intelligence_tasks.py, backend/data_pipeline/tasks/sanctions_tasks.py, backend/api/services/bigquery_service.py</files>
  <action>
Update intelligence_tasks.py and sanctions_tasks.py to query BigQuery:

**intelligence_tasks.py:**
1. In `analyze_event_intelligence()`:
   - Replace `Event.objects.get(id=event_id)` with `bigquery_service.get_event_by_id(event_id)`
   - Access event fields from dict (event['title'], event['content'], etc.)
   - Preserve LLM analysis logic, Claude API calls, response processing

2. In batch intelligence tasks:
   - Replace `Event.objects.filter()` with BigQuery queries
   - Add `bigquery_service.get_unanalyzed_events(cutoff_date, limit)` if needed
   - Maintain queue processing efficiency

**sanctions_tasks.py:**
1. In sanctions screening tasks:
   - Replace `Event.objects` queries with BigQuery
   - Add `bigquery_service.get_recent_events(days, filters)` if not present
   - Preserve OFAC API integration and SanctionsMatch creation (PostgreSQL)

**BigQuery service additions:**
- `get_unanalyzed_events(cutoff_date, limit)` - Query events where llm_analysis is null/empty
- `get_recent_events_for_sanctions(days)` - Events needing sanctions screening
- Use DATE partitioning for efficient queries: `WHERE DATE(mentioned_at) >= @cutoff_date`

**What to avoid:**
- Don't change LLM prompts or Claude API integration
- Don't modify sanctions scoring algorithms
- Don't change SanctionsMatch model (PostgreSQL reference data)
  </action>
  <verify>
Test intelligence task:
```bash
python manage.py shell -c "
from data_pipeline.tasks.intelligence_tasks import analyze_event_intelligence
result = analyze_event_intelligence.delay('test-event-id')
print(result.get())
"
```

Test sanctions task:
```bash
python manage.py shell -c "
from data_pipeline.tasks.sanctions_tasks import screen_event_for_sanctions
result = screen_event_for_sanctions.delay('test-event-id')
print(result.get())
"
```

Both should complete without PostgreSQL Event queries.
  </verify>
  <done>
- intelligence_tasks.py uses bigquery_service for all event reads
- sanctions_tasks.py uses bigquery_service for all event reads
- All Event.objects calls removed from both files
- LLM analysis and sanctions screening work correctly
- No PostgreSQL Event queries in Celery task execution
  </done>
</task>

<task type="auto">
  <name>Task 3: Integration testing and PostgreSQL Event deprecation</name>
  <files>backend/core/models.py</files>
  <action>
1. **End-to-end pipeline test:**
   - Run GDELT sync task (writes to BigQuery)
   - Verify entity extraction reads from BigQuery and creates EntityMentions
   - Verify intelligence analysis reads from BigQuery and updates llm_analysis
   - Verify sanctions screening reads from BigQuery and creates SanctionsMatches
   - Confirm no PostgreSQL Event table queries in processing pipeline

2. **PostgreSQL Event model deprecation:**
   - Add deprecation notice to Event model docstring in backend/core/models.py:
   ```python
   class Event(models.Model):
       """
       **DEPRECATED:** Event data now stored in BigQuery for time-series analytics.

       This Django model is retained for:
       - Django migrations compatibility
       - Historical reference
       - Potential future metadata needs

       **Do NOT use Event.objects for queries.** Use BigQueryService instead:
       - backend/api/services/bigquery_service.py

       Migration history:
       - Phase 14.1: Ingestion migrated to BigQuery
       - Phase 14.2: GDELT native BigQuery dataset
       - Phase 14.3: Processing tasks migrated to BigQuery (COMPLETE)

       Time-series event data from multiple sources.
       Originally designed for TimescaleDB hypertables, now in BigQuery.
       """
   ```

3. **Verification:**
   - Run full ingestion → processing → API pipeline test
   - Check BigQuery events table has recent data
   - Check PostgreSQL Event table is static (no new inserts)
   - Verify dashboard and chat still work (should use BigQuery via Phase 14.1-03 API migration)

**What to avoid:**
- Don't delete PostgreSQL Event model (keep for Django compatibility)
- Don't run migrations to drop Event table (keep for safety)
- Don't modify existing Event data (just stop using the table)
  </action>
  <verify>
Run end-to-end test:
```bash
python manage.py shell -c "
from data_pipeline.tasks.gdelt_sync_task import sync_gdelt_events
from data_pipeline.tasks.entity_extraction import extract_entities_from_event
from data_pipeline.tasks.intelligence_tasks import analyze_event_intelligence

# Sync GDELT events (writes to BigQuery)
sync_result = sync_gdelt_events(lookback_minutes=60)
print('Sync result:', sync_result)

# Get first event ID from BigQuery
from api.services.bigquery_service import bigquery_service
events = bigquery_service.get_recent_events(limit=1)
if events:
    event_id = events[0]['id']

    # Extract entities (reads from BigQuery)
    extract_result = extract_entities_from_event(event_id)
    print('Entity extraction:', extract_result)

    # Analyze intelligence (reads from BigQuery)
    intel_result = analyze_event_intelligence(event_id)
    print('Intelligence analysis:', intel_result)

print('✓ End-to-end pipeline works with BigQuery')
"
```

Check PostgreSQL Event table is static:
```bash
python manage.py shell -c "
from core.models import Event
from django.utils import timezone
from datetime import timedelta

recent_events = Event.objects.filter(
    timestamp__gte=timezone.now() - timedelta(hours=1)
).count()

print(f'Recent PostgreSQL Events (should be 0): {recent_events}')
"
```
  </verify>
  <done>
- End-to-end pipeline test passes (GDELT sync → entity extraction → intelligence → API)
- PostgreSQL Event model has deprecation notice in docstring
- PostgreSQL Event table has no new inserts in past hour
- All processing tasks read from BigQuery
- Dashboard and chat work correctly (Phase 14.1-03 API views already use BigQuery)
- Phase 14.3 migration complete
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] All processing tasks (entity_extraction, intelligence, sanctions) query BigQuery
- [ ] No Event.objects queries in data_pipeline/tasks/ except for test files
- [ ] End-to-end pipeline works: ingestion → BigQuery → processing → API → frontend
- [ ] PostgreSQL Event model has deprecation notice
- [ ] BigQuery events table receives new data, PostgreSQL Event table is static
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Processing tasks fully migrated to BigQuery
- PostgreSQL Event model deprecated with clear documentation
- Full polyglot persistence architecture operational (PostgreSQL for transactional, BigQuery for time-series)
- Phase 15 correlation analysis unblocked (all event data in BigQuery)
</success_criteria>

<output>
After completion, create `.planning/phases/14.3-complete-event-migration-bigquery/14.3-01-SUMMARY.md`:

# Phase 14.3 Plan 01: Complete Event Migration to BigQuery Summary

**Processing tasks migrated: Entity extraction, intelligence analysis, and sanctions screening now query BigQuery instead of PostgreSQL.**

## Accomplishments

- Migrated entity extraction task to read events from BigQuery
- Migrated intelligence analysis tasks to query BigQuery for event data
- Migrated sanctions screening to use BigQuery event queries
- Added BigQuery service methods: get_event_by_id, get_unanalyzed_events, get_recent_events_for_sanctions
- Deprecated PostgreSQL Event model with comprehensive docstring notice
- Validated end-to-end pipeline: GDELT sync → BigQuery → processing → API → frontend
- Achieved full polyglot persistence: PostgreSQL for transactional, BigQuery for all time-series

## Files Created/Modified

- `backend/data_pipeline/tasks/entity_extraction.py` - Migrated to BigQuery queries
- `backend/data_pipeline/tasks/intelligence_tasks.py` - Migrated to BigQuery queries
- `backend/data_pipeline/tasks/sanctions_tasks.py` - Migrated to BigQuery queries
- `backend/api/services/bigquery_service.py` - Added processing task query methods
- `backend/core/models.py` - Added Event model deprecation notice

## Decisions Made

- PostgreSQL Event model retained for Django compatibility (not deleted)
- Event table not dropped via migrations (kept for safety and rollback capability)
- EntityMention and SanctionsMatch remain in PostgreSQL (reference data, not time-series)
- Processing logic preserved exactly (only data source changed)

## Issues Encountered

[Document any issues or "None"]

## Next Phase Readiness

Phase 15 (Correlation & Pattern Analysis) is now unblocked:
- All event data consolidated in BigQuery (GDELT, ReliefWeb, FRED, UN Comtrade, World Bank)
- Entity risk time-series in BigQuery (from Phase 14.1)
- Economic indicators in BigQuery (FRED data)
- Unified query interface via BigQueryService
- Ready for cross-source correlation analysis

**Architecture complete:** Full polyglot persistence operational - PostgreSQL handles Django auth and Entity metadata, BigQuery handles all time-series analytics and event data.
</output>

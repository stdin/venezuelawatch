---
phase: 27-small-scale-end-to-end-pipeline-test-in-gcp
plan: 02
type: execute
---

<objective>
Test data ingestion by triggering GDELT sync for 1-2 weeks of historical data and validating events appear correctly in BigQuery.

Purpose: Prove that the ingestion layer works end-to-end (Cloud Scheduler → Cloud Functions → BigQuery streaming inserts).
Output: BigQuery events table populated with 1-2 weeks of GDELT data, validated for correctness.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-CONTEXT.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-01-SUMMARY.md

# GDELT BigQuery integration
@.planning/phases/14.2-gdelt-bigquery-native-migrate-to-gdelt-native-dataset/14.2-01-SUMMARY.md
@backend/api/services/gdelt_bigquery_service.py

# BigQuery schema
@backend/api/services/bigquery_service.py

**Test scope (from CONTEXT.md):**
- 1-2 weeks of historical GDELT data
- Small enough to keep test manageable and fast
- Sufficient to see patterns and entity relationships

**Key metrics to validate:**
- Event count (expect 100-1000 events for Venezuela over 1-2 weeks)
- Data quality (non-null required fields, valid JSON metadata)
- No errors in Cloud Function logs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Trigger GDELT sync for historical data</name>
  <files>N/A (Cloud Function invocation)</files>
  <action>Manually trigger the gdelt_sync_trigger Cloud Function to ingest 1-2 weeks of historical GDELT data. Use gcloud functions call or Cloud Console to invoke with lookback parameter (e.g., {"lookback_hours": 336} for 14 days). The function will query gdelt-bq.gdeltv2 dataset for Venezuela events (ActionGeo_CountryCode='VE', Actor1CountryCode='VE', Actor2CountryCode='VE') and stream insert to BigQuery venezuelawatch_analytics.events table. Monitor function execution logs for completion status. Avoid triggering Cloud Scheduler jobs yet (wait for Plan 03 full validation).</action>
  <verify>gcloud functions logs read gdelt_sync_trigger --limit=50 shows successful execution with event count logged</verify>
  <done>GDELT function executed successfully, logs show events processed and inserted to BigQuery</done>
</task>

<task type="auto">
  <name>Task 2: Validate BigQuery data quality</name>
  <files>N/A (BigQuery queries)</files>
  <action>Query BigQuery to validate ingested GDELT events. Run queries to check: (1) Event count in date range - SELECT COUNT(*) FROM `venezuelawatch_analytics.events` WHERE source_name='gdelt' AND mentioned_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY), (2) Sample events - SELECT event_id, title, mentioned_at, event_type, metadata FROM `venezuelawatch_analytics.events` WHERE source_name='gdelt' LIMIT 10, (3) Metadata structure - verify metadata.goldstein_scale, metadata.avg_tone, metadata.actor1_name exist and are non-null for sample events, (4) Partition coverage - verify events distributed across dates, not all on single day. Document results (event count, date range, sample metadata fields).</action>
  <verify>BigQuery queries return results showing 100-1000 events with valid metadata and distributed dates</verify>
  <done>BigQuery contains GDELT events with correct schema, metadata populated, events across multiple dates</done>
</task>

<task type="auto">
  <name>Task 3: Check for ingestion errors and data anomalies</name>
  <files>N/A (log analysis)</files>
  <action>Review Cloud Function logs and BigQuery data for errors or anomalies. Check: (1) Cloud Function logs - gcloud functions logs read gdelt_sync_trigger for any exceptions, rate limit errors, BigQuery insert failures, (2) Duplicate events - SELECT event_id, COUNT(*) FROM `venezuelawatch_analytics.events` WHERE source_name='gdelt' GROUP BY event_id HAVING COUNT(*) > 1 (should return 0 duplicates due to GLOBALEVENTID deduplication), (3) Missing required fields - SELECT COUNT(*) FROM `venezuelawatch_analytics.events` WHERE source_name='gdelt' AND (event_id IS NULL OR title IS NULL OR mentioned_at IS NULL) (should be 0), (4) Metadata parsing - verify JSON is valid and contains expected GDELT fields. Document any issues found.</action>
  <verify>No errors in logs, no duplicate event_ids, all required fields non-null, metadata JSON valid</verify>
  <done>Ingestion completed cleanly with no errors, duplicates, or data quality issues</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>1-2 weeks of GDELT data ingested into BigQuery via Cloud Functions</what-built>
  <how-to-verify>
    1. Query event count: Run BigQuery query to count events in date range
    2. Inspect sample events: Review 5-10 events for data quality (title, metadata fields, dates)
    3. Check logs: Review Cloud Function logs for any warnings or errors
    4. Verify no duplicates: Run duplicate check query (should return 0 rows)
    5. Confirm: Data looks correct, no anomalies, logs are clean
  </how-to-verify>
  <resume-signal>Type "approved" if data ingestion successful, or describe data quality issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] GDELT sync function executed successfully
- [ ] BigQuery events table contains 100-1000 events from last 1-2 weeks
- [ ] Metadata fields populated correctly (goldstein_scale, avg_tone, actor names)
- [ ] No duplicate events in BigQuery
- [ ] No errors in Cloud Function logs
</verification>

<success_criteria>

- All tasks completed
- Data ingestion verified by user
- Event count within expected range
- No data quality issues or errors
</success_criteria>

<output>
After completion, create `.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-02-SUMMARY.md`:

# Phase 27 Plan 02: Small-Scale Data Ingestion Test Summary

**[One-liner: X events ingested successfully over Y days]**

## Accomplishments

- Triggered GDELT sync for historical data
- Validated BigQuery data quality
- Confirmed no ingestion errors

## Files Created/Modified

[None - infrastructure testing only]

## Decisions Made

[Any data quality findings or configuration adjustments, or "None"]

## Issues Encountered

[Ingestion problems and resolutions, or "None"]

## Next Step

Ready for 27-03-PLAN.md (End-to-End Intelligence Pipeline Validation)
</output>

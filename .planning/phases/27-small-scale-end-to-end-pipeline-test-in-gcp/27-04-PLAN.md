---
phase: 27-small-scale-end-to-end-pipeline-test-in-gcp
plan: 04
type: execute
---

<objective>
Configure Pub/Sub push subscriptions and Cloud Tasks queue to wire the processing pipeline, trigger processing for existing 1000 GDELT events, and validate complete end-to-end data flow from ingestion to frontend visualization.

Purpose: Complete Phase 27 validation by making the intelligence processing pipeline operational.
Output: Fully functional end-to-end pipeline with processed events (risk scores, entities, graphs) visible in frontend.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-CONTEXT.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-01-SUMMARY.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-02-SUMMARY.md
@.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-03-SUMMARY.md

# Phase 18 processing architecture
@.planning/phases/18-gcp-native-pipeline-migration/18-02-SUMMARY.md

**Current State (from 27-03):**
- Infrastructure deployed: Cloud Run API, Cloud Functions, Pub/Sub topics, Cloud SQL, BigQuery
- Runtime configured: Secrets, IAM, 1Gi memory, Cloud SQL connection
- Data ingested: 1000 GDELT events in BigQuery (2026-01-10 to 2026-01-11)
- **Blocking issue:** Zero Pub/Sub subscriptions, no Cloud Tasks queue, zero events processed

**Processing Architecture (Phase 18):**
1. Event ingestion → Pub/Sub event-created topic
2. Pub/Sub push subscription → POST /api/internal/process-event (extract entities)
3. Entity extraction → Pub/Sub extract-entities topic
4. Pub/Sub push subscription → Cloud Tasks llm-analysis-queue
5. Cloud Tasks → POST /api/internal/analyze-intelligence (LLM risk analysis)
6. Results written to BigQuery metadata fields

**Required Infrastructure:**
- Pub/Sub push subscriptions (event-created, extract-entities, analyze-intelligence)
- Cloud Tasks queue (llm-analysis-queue in us-central1)
- Service account with invoker permissions for Cloud Run endpoints
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Cloud Tasks queue for LLM analysis</name>
  <files>GCP Cloud Tasks (no code changes)</files>
  <action>Create Cloud Tasks queue for LLM analysis processing. Run: gcloud tasks queues create llm-analysis-queue --location=us-central1 --max-dispatches-per-second=10 --max-concurrent-dispatches=5 to create queue with rate limiting (10 tasks/sec max, 5 concurrent). Verify: gcloud tasks queues describe llm-analysis-queue --location=us-central1 shows RUNNING state. Queue will receive tasks from Pub/Sub subscriptions for async LLM processing.</action>
  <verify>Cloud Tasks queue exists and is in RUNNING state</verify>
  <done>llm-analysis-queue created and operational</done>
</task>

<task type="auto">
  <name>Task 2: Create Pub/Sub push subscriptions to trigger processing handlers</name>
  <files>GCP Pub/Sub subscriptions (no code changes)</files>
  <action>Create Pub/Sub push subscriptions to wire processing pipeline. Get Cloud Run API URL: gcloud run services describe venezuelawatch-api --region=us-central1 --format="value(status.url)". Create 3 subscriptions: (1) gcloud pubsub subscriptions create process-event-sub --topic=event-created --push-endpoint={API_URL}/api/internal/process-event --ack-deadline=60 to trigger entity extraction on new events, (2) gcloud pubsub subscriptions create extract-entities-sub --topic=extract-entities --push-endpoint={API_URL}/api/internal/extract-entities --ack-deadline=60 for entity processing, (3) gcloud pubsub subscriptions create analyze-intelligence-sub --topic=analyze-intelligence --push-endpoint={API_URL}/api/internal/analyze-intelligence --ack-deadline=60 for LLM analysis. Grant invoker permissions: gcloud run services add-iam-policy-binding venezuelawatch-api --region=us-central1 --member="serviceAccount:service-{PROJECT_NUMBER}@gcp-sa-pubsub.iam.gserviceaccount.com" --role="roles/run.invoker" so Pub/Sub can invoke Cloud Run. Verify: gcloud pubsub subscriptions list shows 3 subscriptions with push configs.</action>
  <verify>3 Pub/Sub push subscriptions created with correct endpoints and invoker permissions granted</verify>
  <done>Processing pipeline wired: Pub/Sub topics → Cloud Run handlers</done>
</task>

<task type="auto">
  <name>Task 3: Trigger processing for existing 1000 GDELT events</name>
  <files>BigQuery events table, Pub/Sub topics</files>
  <action>Trigger processing for the 1000 existing GDELT events. Query event IDs: bq query --use_legacy_sql=false --format=csv "SELECT event_id FROM \`venezuelawatch-staging.venezuelawatch_analytics.events\` WHERE source_name='GDELT' LIMIT 1000" > /tmp/event_ids.csv. Publish to event-created topic to trigger pipeline: for each event_id in file, run gcloud pubsub topics publish event-created --message='{"event_id":"EVENT_ID","source":"gdelt"}' (batch in groups of 100 with sleep to avoid rate limits). Monitor: gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=venezuelawatch-api AND textPayload=~'process-event'" --limit=10 --freshness=5m to see processing activity. Expected: Pub/Sub → process-event → entity extraction → LLM analysis → BigQuery updates. Allow 5-10 minutes for processing to complete (LLM calls are rate-limited).</action>
  <verify>Pub/Sub messages published, Cloud Run logs show processing activity, BigQuery events have risk_score populated</verify>
  <done>1000 events processing triggered, pipeline operational</done>
</task>

<task type="auto">
  <name>Task 4: Validate processing results in BigQuery</name>
  <files>BigQuery events and entity_mentions tables</files>
  <action>Verify that processing completed successfully. Check event processing: bq query --use_legacy_sql=false "SELECT COUNT(*) as total, COUNTIF(JSON_VALUE(metadata, '$.risk_score') IS NOT NULL) as with_risk_score, COUNTIF(JSON_VALUE(metadata, '$.severity') IS NOT NULL) as with_severity FROM \`venezuelawatch-staging.venezuelawatch_analytics.events\` WHERE source_name='GDELT'" to confirm risk scores and severity populated. Check entity extraction: bq query --use_legacy_sql=false "SELECT COUNT(*) as total FROM \`venezuelawatch-staging.venezuelawatch_analytics.entity_mentions\` WHERE event_id IN (SELECT event_id FROM \`venezuelawatch-staging.venezuelawatch_analytics.events\` WHERE source_name='GDELT')" to verify entities extracted. Sample processed events: bq query --use_legacy_sql=false --format=pretty "SELECT event_id, title, JSON_VALUE(metadata, '$.risk_score') as risk_score, JSON_VALUE(metadata, '$.severity') as severity FROM \`venezuelawatch-staging.venezuelawatch_analytics.events\` WHERE source_name='GDELT' AND JSON_VALUE(metadata, '$.risk_score') IS NOT NULL LIMIT 10" to inspect results. Expected: >90% of events with risk scores, entities extracted, severity classifications assigned.</action>
  <verify>BigQuery shows processed events with risk_score, severity, and entity mentions</verify>
  <done>Processing pipeline validation complete, data quality confirmed</done>
</task>

<task type="auto">
  <name>Task 5: Test API endpoints with processed data</name>
  <files>N/A (API testing)</files>
  <action>Test API endpoints to verify processed data is accessible. Get Cloud Run URL: API_URL=$(gcloud run services describe venezuelawatch-api --region=us-central1 --format="value(status.url)"). Test endpoints: (1) curl "$API_URL/api/risk/events?limit=10" to fetch events with risk scores, (2) curl "$API_URL/api/entities/trending" to get entity leaderboard, (3) Get sample entity ID from response, curl "$API_URL/api/entities/{entity_id}" to fetch entity profile, (4) curl "$API_URL/api/graph/entities?entity_id={entity_id}&time_range=30d" to test graph generation. Verify responses return valid JSON with processed data (risk scores, entities, graph nodes/edges). Document sample responses for frontend testing.</action>
  <verify>All API endpoints return valid JSON with processed data</verify>
  <done>API endpoints validated, ready for frontend integration</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete end-to-end intelligence pipeline: ingestion → processing → API → frontend</what-built>
  <how-to-verify>
    1. **Check BigQuery processing results:**
       - Run: `bq query --use_legacy_sql=false "SELECT COUNT(*) as total, COUNTIF(JSON_VALUE(metadata, '$.risk_score') IS NOT NULL) as processed FROM \`venezuelawatch-staging.venezuelawatch_analytics.events\` WHERE source_name='GDELT'"`
       - Expected: ~900+ of 1000 events processed (>90%)

    2. **Test API endpoints:**
       - Events: `curl {CLOUD_RUN_URL}/api/risk/events?limit=5` (should return events with risk scores)
       - Entities: `curl {CLOUD_RUN_URL}/api/entities/trending` (should return entity leaderboard)
       - Graph: `curl {CLOUD_RUN_URL}/api/graph/entities?entity_id={id}&time_range=30d` (should return nodes/edges)

    3. **Optional: Test frontend locally:**
       - Update `frontend/.env`: `VITE_API_URL={CLOUD_RUN_URL}`
       - Run: `cd frontend && npm run dev`
       - Visit: http://localhost:5173/ (Dashboard should show events)
       - Visit: http://localhost:5173/entities (Entity leaderboard should populate)
       - Visit: http://localhost:5173/graph (Select entity, graph should render)

    4. **Verify no errors in Cloud Run logs:**
       - Run: `gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=venezuelawatch-api AND severity>=ERROR" --limit=20 --freshness=30m`
       - Expected: No critical processing errors

    5. **Confirm:**
       - Full data flow working (GDELT → BigQuery → processing → API)
       - Events have risk scores and severity
       - Entities extracted and queryable
       - Graph API returns valid data
  </how-to-verify>
  <resume-signal>Type "approved" if pipeline fully operational, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Cloud Tasks queue created and running
- [ ] 3 Pub/Sub push subscriptions created and configured
- [ ] 1000 events processing triggered via Pub/Sub
- [ ] >90% of events have risk_score and severity in BigQuery
- [ ] Entity mentions extracted and in BigQuery
- [ ] API endpoints return valid processed data
- [ ] User confirms full end-to-end pipeline operational
</verification>

<success_criteria>
- All tasks completed
- Processing pipeline operational (Pub/Sub → Cloud Run → BigQuery)
- Events processed with risk scores, severity, and entities
- API endpoints serving processed intelligence data
- Phase 27 validation complete - GCP deployment ready for production
</success_criteria>

<output>
After completion, create `.planning/phases/27-small-scale-end-to-end-pipeline-test-in-gcp/27-04-SUMMARY.md`:

# Phase 27 Plan 04: Processing Pipeline Configuration and Validation Summary

**[One-liner describing successful pipeline wiring and end-to-end validation]**

## Accomplishments

- Created Cloud Tasks queue for LLM analysis processing
- Configured Pub/Sub push subscriptions to Cloud Run handlers
- Triggered processing for 1000 existing GDELT events
- Validated risk scoring, entity extraction, and graph generation
- Confirmed complete end-to-end data flow operational

## Files Created/Modified

[GCP infrastructure only, or "None"]

## Decisions Made

[Configuration decisions for queue sizes, rate limits, etc., or "None"]

## Issues Encountered

[Processing errors, rate limit issues, or "None"]

## Next Phase Readiness

**Phase 27 Complete** - Small-scale end-to-end test successful. Pipeline fully validated:
- ✅ Infrastructure deployed (Cloud Functions, Cloud Run, Pub/Sub, Cloud Tasks, Cloud SQL, BigQuery)
- ✅ Data ingestion working (1000 GDELT events)
- ✅ Processing pipeline operational (risk scoring, entity extraction, LLM analysis)
- ✅ API endpoints serving intelligence data
- ✅ Frontend compatible with GCP backend

**Ready for:**
- Expanding to larger datasets (months of historical data)
- Enabling all Cloud Scheduler jobs for continuous ingestion
- Production launch preparation
- v1.3 milestone completion
</output>

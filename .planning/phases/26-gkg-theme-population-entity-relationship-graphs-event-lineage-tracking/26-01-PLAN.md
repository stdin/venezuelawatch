---
phase: 26-gkg-theme-population-entity-relationship-graphs-event-lineage-tracking
plan: 01
type: execute
---

<objective>
Build backend graph data service with community detection to identify high-risk entity clusters.

Purpose: Create the data layer for entity relationship visualization - fetch entities/relationships from PostgreSQL, apply Graphology Louvain algorithm to detect communities (e.g., sanctioned entity networks), and expose via API.
Output: Working graph API endpoint returning nodes/edges with community assignments and risk scores.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-gkg-theme-population-entity-relationship-graphs-event-lineage-tracking/26-RESEARCH.md
@.planning/phases/26-gkg-theme-population-entity-relationship-graphs-event-lineage-tracking/26-CONTEXT.md
@.planning/phases/25-update-system-to-follow-platform-design/25-02-SUMMARY.md

**Phase context (from CONTEXT.md):**
- Interactive entity relationship graph with auto-focused high-risk clusters
- Relationship quality: repeated co-occurrence, directional/weighted, verifiable from sources
- Don't hand-roll community detection - use Graphology Louvain algorithm (RESEARCH.md)

**Tech stack available:**
- Backend: Django + django-ninja, PostgreSQL
- Graph: Graphology (from RESEARCH.md - standard for JS graph algorithms)
- Entity models from Phase 6 (Entity, EntityMention tables)

**Constraining decisions:**
- Phase 6: Entity models with fuzzy matching, trending endpoints
- Phase 24: Entity linking at publish time via metadata.linked_entities
- Phase 25: P1-P4 severity, composite risk scoring with 5 components
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create graph builder service</name>
  <files>backend/data_pipeline/services/graph_builder.py</files>
  <action>
Create GraphBuilder service that fetches entity relationships from PostgreSQL:

**build_entity_graph(time_range: str, min_cooccurrence: int = 3) -> Dict[str, Any]:**
- Query EntityMention.objects to find entity co-occurrences (entities appearing in same event)
- Group by entity pairs, count co-occurrences, filter by min_cooccurrence threshold
- Build nodes list: {id: str(entity.id), label: entity.name, data: {risk_score, sanctions_status, entity_type, mention_count}}
- Build edges list: {id: source-target, source: entity_a_id, target: entity_b_id, weight: cooccurrence_count, data: {event_ids, strength}}
- Return {nodes: [...], edges: [...]}

Use .values() and .annotate(Count()) for efficient aggregation. Don't use raw SQL - ORM queries are sufficient and more maintainable.

**get_time_range() helper:** Parse time_range string ("7d", "30d", "90d") into datetime filter for mentioned_at field.

Follow Phase 6 entity models (Entity, EntityMention tables with mentioned_at timestamps).
  </action>
  <verify>python -c "from backend.data_pipeline.services.graph_builder import GraphBuilder; g = GraphBuilder(); result = g.build_entity_graph('30d'); print(f'Nodes: {len(result[\"nodes\"])}, Edges: {len(result[\"edges\"])}')" prints counts without errors</verify>
  <done>GraphBuilder service imports without errors, build_entity_graph() returns dict with nodes/edges lists, relationships based on co-occurrence counts</done>
</task>

<task type="auto">
  <name>Task 2: Implement community detection with Graphology Louvain</name>
  <files>backend/data_pipeline/services/graph_builder.py</files>
  <action>
Add detect_communities() method to GraphBuilder:

**detect_communities(nodes, edges) -> Dict[str, Any]:**
- Use Node.js subprocess to run Graphology Louvain algorithm (Python-JS bridge pattern)
- Create temp JSON file with graph data: {nodes: [{id, ...}], edges: [{source, target, weight}]}
- Write Node.js script that: imports graphology + graphology-communities-louvain, builds Graph, runs louvain.assign(), returns {communities: {node_id: community_id}, cluster_stats: {community_id: {avg_risk, node_count}}}
- Execute: subprocess.run(['node', script_path, json_path], capture_output=True)
- Parse JSON response, assign community field to each node
- Calculate cluster_stats (average risk score per community)
- Return {nodes: nodes_with_communities, high_risk_cluster: cluster_id_with_highest_avg_risk}

**Why Node.js subprocess:** Graphology is JavaScript library with no Python equivalent. Don't hand-roll Louvain - it's complex modularity optimization (RESEARCH.md). Node.js bridge is simpler than translating algorithm.

Store Node.js script in backend/data_pipeline/scripts/community_detection.js
  </action>
  <verify>Build graph with test data, run detect_communities(), verify nodes have 'community' field and high_risk_cluster is identified</verify>
  <done>detect_communities() assigns community IDs to nodes, identifies highest-risk cluster, Node.js script executes without errors</done>
</task>

<task type="auto">
  <name>Task 3: Create graph API endpoint</name>
  <files>backend/api/views/graph.py</files>
  <action>
Create GET /api/graph/entities endpoint using django-ninja:

**GraphRouter** with endpoint:
- GET /entities?time_range={range}&min_cooccurrence={min}
- Default: time_range="30d", min_cooccurrence=3
- Call GraphBuilder().build_entity_graph(time_range, min_cooccurrence)
- Call detect_communities(nodes, edges) to add community assignments
- Return GraphResponse schema: {nodes: [...], edges: [...], high_risk_cluster: str}

**GraphResponse schema:**
```python
class NodeData(Schema):
    risk_score: float
    sanctions_status: bool
    entity_type: str
    mention_count: int
    community: Optional[str] = None

class Node(Schema):
    id: str
    label: str
    data: NodeData

class EdgeData(Schema):
    event_ids: List[str]
    strength: int

class Edge(Schema):
    id: str
    source: str
    target: str
    weight: int
    data: EdgeData

class GraphResponse(Schema):
    nodes: List[Node]
    edges: List[Edge]
    high_risk_cluster: Optional[str] = None
```

Register router in backend/api/urls.py following Phase 1 router pattern.

Don't cache results yet - that's a performance optimization for later. Return fresh data each request for now.
  </action>
  <verify>curl http://localhost:8000/api/graph/entities returns JSON with nodes, edges, and high_risk_cluster fields</verify>
  <done>API endpoint returns graph data with community assignments, follows django-ninja schema patterns, registered in URL config</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] GraphBuilder service builds graph from EntityMention co-occurrences
- [ ] Community detection runs Graphology Louvain via Node.js subprocess
- [ ] GET /api/graph/entities returns nodes with community field
- [ ] high_risk_cluster identified based on average risk scores
- [ ] No Python errors, endpoint accessible
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Graph API endpoint functional
- Community detection identifies high-risk clusters automatically
- Ready for frontend visualization (Plan 26-02)
</success_criteria>

<output>
After completion, create `.planning/phases/26-gkg-theme-population-entity-relationship-graphs-event-lineage-tracking/26-01-SUMMARY.md`
</output>

---
phase: 21-mentions-tracking
plan: "03"
type: execute
---

<objective>
Deploy Cloud Function for daily mention tracking with Cloud Scheduler trigger.

Purpose: Automate daily spike detection across all Venezuela events.
Output: Deployed mention_tracker Cloud Function triggered daily at 2 AM UTC.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-mentions-tracking/DISCOVERY.md
@.planning/phases/21-mentions-tracking/21-CONTEXT.md
@.planning/phases/21-mentions-tracking/21-01-SUMMARY.md
@.planning/phases/21-mentions-tracking/21-02-SUMMARY.md
@.planning/phases/18-gcp-native-pipeline-migration/18-03-SUMMARY.md
@cloud_functions/gdelt_sync/main.py
@cloud_functions/gdelt_sync/requirements.txt

**From Phase 18**: GCP-native serverless patterns with Cloud Functions Gen2, Cloud Scheduler OIDC authentication, standalone functions with no Django dependencies
**From Phase 21-01**: GDELTMentionsService for querying mention statistics
**From Phase 21-02**: SpikeDetectionService for z-score classification
**From Discovery**: Daily batch processing at 2 AM UTC, 30-day lookback window, PostgreSQL bulk insert for spikes

**Tech stack available**:
- Cloud Functions Gen2 (serverless Python functions)
- Cloud Scheduler (cron-based triggers with OIDC auth)
- google-cloud-bigquery (BigQuery client)
- psycopg2 (direct PostgreSQL access, no Django ORM in Cloud Functions)

**Established patterns**:
- Standalone Cloud Functions with copied business logic (Phase 18)
- OIDC authentication from Cloud Scheduler (Phase 18)
- 512-900MB memory allocation based on task complexity (Phase 18)
- Direct PostgreSQL INSERT via psycopg2 (no Django ORM in functions)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Mention Tracker Cloud Function</name>
  <files>cloud_functions/mention_tracker/main.py, cloud_functions/mention_tracker/requirements.txt, cloud_functions/mention_tracker/services/gdelt_mentions_service.py, cloud_functions/mention_tracker/services/spike_detection_service.py</files>
  <action>
Create cloud_functions/mention_tracker/ directory with:

**main.py**:
```python
import functions_framework
from google.cloud import bigquery
import psycopg2
import os
from datetime import datetime, timedelta
from typing import List, Dict
import logging

from services.gdelt_mentions_service import gdelt_mentions_service
from services.spike_detection_service import spike_detection_service

logger = logging.getLogger(__name__)

@functions_framework.http
def mention_tracker(request):
    """
    Track mention spikes for Venezuela events.
    Triggered daily by Cloud Scheduler at 2 AM UTC.
    """
    try:
        # Step 1: Get Venezuela event IDs from BigQuery (last 30 days)
        event_ids = get_venezuela_event_ids()
        logger.info(f"Tracking mentions for {len(event_ids)} Venezuela events")

        # Step 2: Fetch mention statistics with rolling windows
        mention_stats = gdelt_mentions_service.get_mention_stats(
            event_ids=event_ids,
            lookback_days=30
        )
        logger.info(f"Fetched mention stats for {len(mention_stats)} event-days")

        # Step 3: Detect spikes using z-score analysis
        spikes = spike_detection_service.detect_spikes(mention_stats)
        logger.info(f"Detected {len(spikes)} spikes (z >= 2.0)")

        # Step 4: Store spikes in PostgreSQL
        if spikes:
            spike_count = store_spikes(spikes)
            logger.info(f"Stored {spike_count} new spikes")

        return {
            "status": "success",
            "events_tracked": len(event_ids),
            "spikes_detected": len(spikes),
            "spikes_stored": spike_count if spikes else 0
        }, 200

    except Exception as e:
        logger.error(f"Mention tracking failed: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}, 500


def get_venezuela_event_ids() -> List[str]:
    """Fetch Venezuela event IDs from last 30 days in BigQuery."""
    client = bigquery.Client(project=os.getenv('GCP_PROJECT_ID'))

    query = """
        SELECT DISTINCT CAST(GLOBALEVENTID AS STRING) AS event_id
        FROM `gdelt-bq.gdeltv2.events_partitioned`
        WHERE _PARTITIONTIME >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
        AND (
            ActionGeo_CountryCode = 'VE'
            OR Actor1CountryCode = 'VE'
            OR Actor2CountryCode = 'VE'
        )
    """

    results = client.query(query).result()
    return [row['event_id'] for row in results]


def store_spikes(spikes: List[Dict]) -> int:
    """Store detected spikes in PostgreSQL using bulk INSERT with ON CONFLICT."""
    conn = psycopg2.connect(
        host=os.getenv('DB_HOST'),
        port=os.getenv('DB_PORT', '5432'),
        database=os.getenv('DB_NAME'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD')
    )

    try:
        cursor = conn.cursor()

        # Bulk INSERT with ON CONFLICT DO NOTHING (unique_together constraint)
        insert_query = """
            INSERT INTO data_pipeline_mentionspike
            (event_id, spike_date, mention_count, baseline_avg, baseline_stddev,
             z_score, confidence_level, detected_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (event_id, spike_date) DO NOTHING
        """

        values = [
            (
                spike['event_id'],
                spike['spike_date'],
                spike['mention_count'],
                spike['baseline_avg'],
                spike['baseline_stddev'],
                spike['z_score'],
                spike['confidence_level'],
                datetime.utcnow()
            )
            for spike in spikes
        ]

        cursor.executemany(insert_query, values)
        inserted_count = cursor.rowcount
        conn.commit()

        return inserted_count

    finally:
        conn.close()
```

**requirements.txt**:
```
functions-framework==3.5.0
google-cloud-bigquery==3.11.4
psycopg2-binary==2.9.9
```

**services/gdelt_mentions_service.py**: Copy from backend/api/services/gdelt_mentions_service.py (100% business logic reuse per Phase 18 pattern)

**services/spike_detection_service.py**: Copy from backend/api/services/spike_detection_service.py (100% business logic reuse)

Critical: Use psycopg2 direct INSERT (not Django ORM) - Cloud Functions are standalone.
Critical: ON CONFLICT DO NOTHING for unique_together constraint (event_id, spike_date).
Critical: Environment variables: GCP_PROJECT_ID, DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD.
  </action>
  <verify>
gcloud functions describe mention_tracker --region=us-central1 shows deployed function, manual trigger via gcloud returns success status
  </verify>
  <done>
Cloud Function deployed with mention tracking logic, PostgreSQL bulk insert, and Venezuela event ID filtering. Manual test succeeds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Deploy Cloud Function and Create Scheduler Job</name>
  <files>cloud_functions/mention_tracker/deploy.sh, infrastructure/cloud_scheduler_jobs.sh</files>
  <action>
**Deploy Cloud Function** (cloud_functions/mention_tracker/deploy.sh):
```bash
#!/bin/bash
gcloud functions deploy mention_tracker \
  --gen2 \
  --runtime=python311 \
  --region=us-central1 \
  --source=. \
  --entry-point=mention_tracker \
  --trigger-http \
  --allow-unauthenticated=false \
  --memory=512MB \
  --timeout=540s \
  --set-env-vars=GCP_PROJECT_ID=venezuelawatch-447923 \
  --set-secrets=DB_HOST=projects/venezuelawatch-447923/secrets/DB_HOST:latest,\
DB_PORT=projects/venezuelawatch-447923/secrets/DB_PORT:latest,\
DB_NAME=projects/venezuelawatch-447923/secrets/DB_NAME:latest,\
DB_USER=projects/venezuelawatch-447923/secrets/DB_USER:latest,\
DB_PASSWORD=projects/venezuelawatch-447923/secrets/DB_PASSWORD:latest
```

Memory: 512MB (mentions query + spike processing, similar to gdelt_sync)
Timeout: 540s (9 min) for BigQuery windowed aggregation over 2.5B rows

**Cloud Scheduler Job**:
Add to infrastructure/cloud_scheduler_jobs.sh:
```bash
# Daily mention tracking at 2 AM UTC
gcloud scheduler jobs create http mention_tracker_daily \
  --location=us-central1 \
  --schedule="0 2 * * *" \
  --uri="https://us-central1-venezuelawatch-447923.cloudfunctions.net/mention_tracker" \
  --http-method=POST \
  --oidc-service-account-email=cloud-scheduler@venezuelawatch-447923.iam.gserviceaccount.com \
  --oidc-token-audience="https://us-central1-venezuelawatch-447923.cloudfunctions.net/mention_tracker" \
  --time-zone="UTC" \
  --description="Daily mention spike detection for Venezuela events"
```

Schedule: 2 AM UTC daily (0 2 * * *) - matches Phase 14 ETL schedule pattern
OIDC auth: Matches Phase 18 security pattern (not API keys)

Run both deploy.sh and update cloud_scheduler_jobs.sh, then execute scheduler job creation.
  </action>
  <verify>
gcloud scheduler jobs describe mention_tracker_daily --location=us-central1 shows job with correct schedule and OIDC config, gcloud scheduler jobs run mention_tracker_daily triggers function successfully
  </verify>
  <done>
Cloud Function deployed with 512MB memory and 9min timeout. Cloud Scheduler job created with daily 2 AM UTC trigger and OIDC authentication. Manual trigger succeeds.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Cloud Function deployed to us-central1
- [ ] Environment variables and secrets configured
- [ ] Cloud Scheduler job created with correct cron schedule
- [ ] Manual trigger via gcloud scheduler jobs run succeeds
- [ ] Function logs show successful spike detection and storage
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- mention_tracker Cloud Function operational
- Daily 2 AM UTC trigger configured
- No errors in deployment or manual test
</success_criteria>

<output>
After completion, create `.planning/phases/21-mentions-tracking/21-03-SUMMARY.md`:

# Phase 21 Plan 03: Mention Tracker Deployment Summary

**Daily automated spike detection with Cloud Function and Cloud Scheduler**

## Accomplishments

- Deployed mention_tracker Cloud Function with 512MB memory and 9min timeout
- Integrated GDELTMentionsService and SpikeDetectionService (100% business logic reuse)
- Created Cloud Scheduler job with daily 2 AM UTC trigger
- OIDC authentication for secure function invocation
- PostgreSQL bulk insert with ON CONFLICT for idempotent spike storage

## Files Created/Modified

- `cloud_functions/mention_tracker/main.py` - Mention tracking orchestration
- `cloud_functions/mention_tracker/requirements.txt` - Function dependencies
- `cloud_functions/mention_tracker/services/gdelt_mentions_service.py` - Copied from backend
- `cloud_functions/mention_tracker/services/spike_detection_service.py` - Copied from backend
- `cloud_functions/mention_tracker/deploy.sh` - Deployment script
- `infrastructure/cloud_scheduler_jobs.sh` - Scheduler job definition

## Decisions Made

- 512MB memory allocation (mentions query + spike processing overhead)
- 9-minute timeout for BigQuery windowed aggregation over 2.5B rows
- Daily 2 AM UTC schedule (aligned with Phase 14 ETL pattern)
- psycopg2 direct INSERT (no Django ORM in Cloud Functions per Phase 18 pattern)
- ON CONFLICT DO NOTHING for idempotent spike storage (unique_together constraint)

## Issues Encountered

[Record any issues]

## Next Phase Readiness

**Phase 21: Mentions Tracking - COMPLETE**

All plans finished:
- 21-01: MentionSpike model and GDELTMentionsService ✓
- 21-02: Spike detection logic with TDD ✓
- 21-03: Cloud Function deployment ✓

Backend infrastructure for mention spike detection operational. Phase 23 (Intelligence Pipeline Rebuild) will consume spike signals for enhanced risk scoring.

Ready for Phase 22 (Data Source Architecture) or next phase planning.
</output>

---
phase: 21-mentions-tracking
plan: "01"
type: execute
---

<objective>
Create PostgreSQL data model for mention spikes and BigQuery service for querying GDELT Mentions with rolling statistics.

Purpose: Establish data infrastructure for mention tracking and spike detection.
Output: MentionSpike Django model and GDELTMentionsService with windowed aggregation queries.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-mentions-tracking/DISCOVERY.md
@.planning/phases/21-mentions-tracking/21-CONTEXT.md
@.planning/phases/20-gkg-integration/20-02-SUMMARY.md
@backend/api/services/gdelt_gkg_service.py
@backend/api/services/gdelt_bigquery_service.py

**From Phase 20**: GKG integration patterns with BigQuery partition filtering and parameterized queries
**From Phase 14.3**: Polyglot persistence architecture (PostgreSQL for transactional, BigQuery for time-series)
**From Discovery**: GDELT eventmentions_partitioned table (2.5B rows, 532GB), 7-day rolling window for baseline, partition filtering mandatory

**Tech stack available**:
- google-cloud-bigquery (BigQuery client with parameterized queries)
- Django ORM (model definition, migrations, indexes)
- BigQuery window functions (STDDEV_POP, AVG, ROWS BETWEEN)

**Established patterns**:
- Partition filtering with `_PARTITIONTIME >= TIMESTAMP_SUB()` (Phase 20)
- Parameterized BigQuery queries for SQL injection prevention (Phase 14.1)
- PostgreSQL indexes for query performance (Phase 6)
- STRING event IDs in polyglot architecture (Phase 14.1)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MentionSpike Django Model</name>
  <files>backend/data_pipeline/models.py</files>
  <action>
Add MentionSpike model to data_pipeline/models.py with these fields:
- event_id (CharField, max_length=50, db_index=True) - Links to BigQuery GLOBALEVENTID
- spike_date (DateField, db_index=True) - Date when spike occurred
- mention_count (IntegerField) - Actual mention count on spike date
- baseline_avg (FloatField) - 7-day rolling average
- baseline_stddev (FloatField) - 7-day rolling standard deviation
- z_score (FloatField, db_index=True) - Calculated z-score
- confidence_level (CharField, max_length=10, choices=['MEDIUM', 'HIGH', 'CRITICAL'])
- detected_at (DateTimeField, auto_now_add=True) - When spike was detected

Meta class:
- unique_together = [['event_id', 'spike_date']] (prevent duplicate spike records)
- indexes = [Index(fields=['spike_date', '-z_score']), Index(fields=['confidence_level', 'spike_date'])]

Why CharField for event_id: Polyglot architecture, event lives in BigQuery not PostgreSQL (no ForeignKey).
Why these indexes: Support "recent critical spikes" query and "spikes for event X" query patterns.

Run makemigrations and migrate to create table.
  </action>
  <verify>
python manage.py makemigrations succeeds, python manage.py migrate creates table, django shell can import MentionSpike and create test instance
  </verify>
  <done>
MentionSpike model exists with all fields, unique_together constraint, and custom indexes. Migration applied successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create GDELT Mentions BigQuery Service</name>
  <files>backend/api/services/gdelt_mentions_service.py</files>
  <action>
Create new service file backend/api/services/gdelt_mentions_service.py with GDELTMentionsService class.

Methods:
1. __init__(): Initialize BigQuery client (pattern from gdelt_gkg_service.py)
2. get_mention_stats(event_ids: List[str], lookback_days: int = 30) -> List[dict]:
   - Query eventmentions_partitioned table
   - Calculate daily mention counts grouped by GLOBALEVENTID and MentionTimeDate
   - Use window functions for 7-day rolling average and stddev (ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING)
   - Return results for last 7 days only
   - Parameterized query with @event_ids array and @lookback_days integer

Query structure (from DISCOVERY.md):
```sql
WITH daily_counts AS (
  SELECT
    GLOBALEVENTID,
    DATE(PARSE_TIMESTAMP('%Y%m%d', CAST(MentionTimeDate AS STRING))) AS mention_date,
    COUNT(*) AS mention_count
  FROM `gdelt-bq.gdeltv2.eventmentions_partitioned`
  WHERE _PARTITIONTIME >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @lookback_days DAY)
  AND GLOBALEVENTID IN UNNEST(@event_ids)
  GROUP BY GLOBALEVENTID, mention_date
),
stats_calc AS (
  SELECT
    GLOBALEVENTID,
    mention_date,
    mention_count,
    AVG(mention_count) OVER (
      PARTITION BY GLOBALEVENTID
      ORDER BY mention_date
      ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING
    ) AS rolling_avg,
    STDDEV_POP(mention_count) OVER (
      PARTITION BY GLOBALEVENTID
      ORDER BY mention_date
      ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING
    ) AS rolling_stddev
  FROM daily_counts
)
SELECT
  GLOBALEVENTID AS event_id,
  mention_date,
  mention_count,
  rolling_avg,
  rolling_stddev
FROM stats_calc
WHERE mention_date >= CURRENT_DATE() - 7
ORDER BY mention_date DESC, mention_count DESC
```

Critical: _PARTITIONTIME filter is MANDATORY to avoid 532GB full table scan.
Critical: ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING excludes current day from baseline (prevents spike from inflating its own baseline).
Critical: Convert GLOBALEVENTID to INT64 array in query parameters (BigQuery type requirements).

Add singleton instance: `gdelt_mentions_service = GDELTMentionsService()` at bottom of file.

Include comprehensive docstring explaining window function logic and partition filtering requirement.
  </action>
  <verify>
Python shell can import gdelt_mentions_service, manually call get_mention_stats with test event IDs from BigQuery (use query_gdelt_schema.py example IDs), verify results have event_id, mention_date, mention_count, rolling_avg, rolling_stddev fields
  </verify>
  <done>
GDELTMentionsService created with get_mention_stats method using window functions, partition filtering, and parameterized queries. Manual test returns expected rolling statistics.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] MentionSpike model exists in data_pipeline/models.py with all fields and indexes
- [ ] Migration applied successfully, table created in PostgreSQL
- [ ] GDELTMentionsService created with get_mention_stats method
- [ ] BigQuery query uses partition filtering and window functions correctly
- [ ] Manual test of get_mention_stats returns mention statistics with rolling averages
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- MentionSpike model ready for spike storage
- GDELTMentionsService ready for daily mention tracking queries
- No errors introduced
</success_criteria>

<output>
After completion, create `.planning/phases/21-mentions-tracking/21-01-SUMMARY.md`:

# Phase 21 Plan 01: Mention Tracking Infrastructure Summary

**PostgreSQL spike model and BigQuery mentions service with rolling window statistics**

## Accomplishments

- Created MentionSpike Django model for spike metadata storage
- Built GDELTMentionsService with windowed aggregation over 2.5B mention records
- Established 7-day rolling baseline pattern excluding current day
- Partition filtering ensures efficient queries over 532GB table

## Files Created/Modified

- `backend/data_pipeline/models.py` - MentionSpike model with unique constraints and indexes
- `backend/data_pipeline/migrations/XXXX_mentionspike.py` - Database migration
- `backend/api/services/gdelt_mentions_service.py` - BigQuery service for mention statistics

## Decisions Made

- Event ID as CharField (not ForeignKey) for polyglot architecture consistency
- 7-day rolling window with ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING (excludes current day)
- 30-day lookback for mention queries (provides 7-day baselines for recent events)
- Partition filtering mandatory in all queries (avoids 532GB scans)

## Issues Encountered

[Record any issues]

## Next Phase Readiness

Ready for Plan 21-02 (TDD spike detection logic)
</output>

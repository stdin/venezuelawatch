---
phase: 15-correlation-pattern-analysis
plan: 01
type: execute
---

<objective>
Build backend correlation computation API using scipy and statsmodels for statistically rigorous correlation analysis between entities, events, and economic indicators.

Purpose: Provide the analytical engine for discovering relationships between variables with proper significance testing, multiple comparison correction, and stationarity checks. Statistical rigor is paramount—users make investment decisions based on these correlations.

Output: Django REST API endpoint that computes pairwise correlations with p-values, applies Bonferroni correction, filters to significant + strong correlations, and returns network graph data.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 15 research and context
@.planning/phases/15-correlation-pattern-analysis/15-RESEARCH.md
@.planning/phases/15-correlation-pattern-analysis/15-CONTEXT.md

# BigQuery data sources (from Phase 14.1-14.3 migration)
@backend/api/services/bigquery_service.py

# Established patterns
@backend/api/views/events.py
@backend/api/views/entities.py

**Tech stack available:**
- Django 5.2 + django-ninja for REST APIs
- BigQuery for all time-series data (events, entity mentions, FRED indicators, UN Comtrade, World Bank)
- React Query for frontend data fetching
- Mantine UI components

**Established patterns:**
- Django API views with django-ninja
- BigQuery service pattern for time-series queries
- Parameterized BigQuery queries for SQL injection prevention
- React Query for API integration

**Constraining decisions:**
- Phase 15 RESEARCH: Use scipy.stats.pearsonr/spearmanr (not custom formulas)
- Phase 15 RESEARCH: Apply Bonferroni correction via statsmodels.stats.multitest (mandatory)
- Phase 15 RESEARCH: Filter to |r| > 0.7 AND p < 0.05/n_tests (strong + significant only)
- Phase 15 RESEARCH: Check stationarity with ADF test before time-series correlation
- Phase 15 CONTEXT: Statistical rigor is non-negotiable (users making investment decisions)
- Phase 15 CONTEXT: Show p-values and effect sizes, not just correlation coefficients

**Available data for correlation:**
1. Entity-level: Entity risk scores, sanctions status, mention frequency (from BigQuery entity_mentions + events)
2. Economic indicators: FRED data (oil prices, inflation, GDP, exchange rates from BigQuery fred_indicators)
3. Event aggregates: Daily/weekly event counts by type, severity (from BigQuery events)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install scipy and statsmodels dependencies</name>
  <files>backend/requirements.txt</files>
  <action>
Add statistical correlation libraries to requirements.txt:
- scipy==1.16.1 (correlation with p-values via pearsonr/spearmanr)
- pandas==2.2.3 (correlation matrices, time-series alignment)
- statsmodels==0.14.4 (multiple comparison correction, stationarity tests)
- scikit-learn==1.5.2 (optional: standardization if needed)

Install with `pip install -r backend/requirements.txt`

**What to avoid:** Don't add older versions (scipy < 1.14 has slower algorithms), don't skip statsmodels (needed for Bonferroni correction).
  </action>
  <verify>
```bash
python -c "import scipy; import pandas; import statsmodels; import sklearn; print('✓ All statistical libraries installed')"
python -c "from scipy import stats; from statsmodels.stats.multitest import multipletests; print('✓ Required functions available')"
```
Should print success messages without ImportError.
  </verify>
  <done>
- scipy, pandas, statsmodels, scikit-learn added to requirements.txt
- All libraries successfully installed
- Required functions (stats.pearsonr, multipletests) importable
  </done>
</task>

<task type="auto">
  <name>Task 2: Create correlation computation service</name>
  <files>backend/api/correlation/compute.py, backend/api/correlation/__init__.py, backend/api/correlation/time_series.py</files>
  <action>
Create `backend/api/correlation/` directory with correlation computation service:

**File: backend/api/correlation/compute.py**
Implement correlation computation with significance testing:

```python
from scipy import stats
from statsmodels.stats.multitest import multipletests
from statsmodels.tsa.stattools import adfuller
import pandas as pd
import numpy as np
from typing import List, Dict, Literal

def compute_pairwise_correlations(
    data_dict: Dict[str, np.ndarray],
    method: Literal['pearson', 'spearman'] = 'pearson',
    alpha: float = 0.05,
    min_effect_size: float = 0.7,
    check_stationarity: bool = True
) -> List[Dict]:
    """
    Compute pairwise correlations with statistical significance filtering.

    Args:
        data_dict: {variable_name: time_series_array}
        method: 'pearson' (linear) or 'spearman' (monotonic)
        alpha: Significance level for Bonferroni correction (default 0.05)
        min_effect_size: Minimum |r| to report (default 0.7 for strong)
        check_stationarity: Apply ADF test for time-series (default True)

    Returns:
        List of {source, target, correlation, p_value, p_adjusted, sample_size, warnings}
    """
    # Align all time series to common indices (inner join, drop NaN)
    df_all = pd.DataFrame(data_dict).dropna()
    variables = list(df_all.columns)
    n_vars = len(variables)

    correlations = []

    # Compute pairwise correlations
    for i in range(n_vars):
        for j in range(i + 1, n_vars):
            x = df_all[variables[i]].values
            y = df_all[variables[j]].values

            warnings = []

            # Stationarity check for time-series
            if check_stationarity:
                x, y, warnings = _check_and_difference(x, y, variables[i], variables[j], alpha)

            # Compute correlation
            if method == 'pearson':
                r, p = stats.pearsonr(x, y)
            else:  # spearman
                r, p = stats.spearmanr(x, y)

            correlations.append({
                'source': variables[i],
                'target': variables[j],
                'correlation': float(r),
                'p_value': float(p),
                'sample_size': len(x),
                'warnings': warnings
            })

    # Apply Bonferroni correction for multiple comparisons
    if correlations:
        p_values = [c['p_value'] for c in correlations]
        rejected, p_adjusted, _, _ = multipletests(p_values, alpha=alpha, method='bonferroni')

        # Filter to significant AND strong correlations
        significant_correlations = []
        for corr, adj_p, is_sig in zip(correlations, p_adjusted, rejected):
            corr['p_adjusted'] = float(adj_p)
            corr['significant'] = is_sig
            # Only return if significant AND strong effect size
            if is_sig and abs(corr['correlation']) >= min_effect_size:
                significant_correlations.append(corr)

        return significant_correlations

    return []

def _check_and_difference(x, y, name_x, name_y, alpha):
    """Check stationarity and difference if needed."""
    from statsmodels.tsa.stattools import adfuller

    warnings = []
    x_use = x
    y_use = y

    # ADF test for unit root (p < alpha = stationary)
    adf_x = adfuller(x, autolag='AIC')
    if adf_x[1] > alpha:  # Non-stationary
        x_use = np.diff(x)
        warnings.append(f'{name_x} was non-stationary, used first difference')

    adf_y = adfuller(y, autolag='AIC')
    if adf_y[1] > alpha:  # Non-stationary
        y_use = np.diff(y)
        warnings.append(f'{name_y} was non-stationary, used first difference')

    # Align lengths after differencing
    min_len = min(len(x_use), len(y_use))
    x_use = x_use[-min_len:]
    y_use = y_use[-min_len:]

    return x_use, y_use, warnings
```

**File: backend/api/correlation/__init__.py**
Empty init file to make it a package.

**File: backend/api/correlation/time_series.py**
Create time-series data extraction methods for BigQuery sources (entity risk, FRED indicators, event counts).

**What to avoid:**
- Don't skip Bonferroni correction (multiple comparisons problem causes false positives)
- Don't show correlations without p-values (correlation without significance is meaningless)
- Don't use custom correlation formulas (scipy handles edge cases like NaN, inf correctly)
- Don't skip stationarity checks for time-series (causes spurious correlations)
  </action>
  <verify>
```bash
python -c "
from api.correlation.compute import compute_pairwise_correlations
import numpy as np

# Test with sample data
data = {
    'var1': np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),
    'var2': np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]),  # Perfect correlation
    'var3': np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])  # Perfect negative
}

result = compute_pairwise_correlations(data, method='pearson', min_effect_size=0.9, check_stationarity=False)
print(f'Found {len(result)} significant correlations')
assert len(result) >= 2  # var1-var2 and var1-var3 should be significant
print('✓ Correlation computation works')
"
```
  </verify>
  <done>
- backend/api/correlation/ directory created
- compute_pairwise_correlations function with scipy.stats.pearsonr/spearmanr
- Bonferroni correction via statsmodels.stats.multitest.multipletests
- Stationarity check with ADF test and differencing
- Filters to significant (p_adjusted < alpha) AND strong (|r| >= min_effect_size)
- Test passes with sample data
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Django API endpoint for correlation analysis</name>
  <files>backend/api/correlation/views.py, backend/api/urls.py</files>
  <action>
Create Django REST API endpoint using django-ninja for correlation computation:

**File: backend/api/correlation/views.py**
```python
from ninja import Router
from ninja import Schema
from typing import List, Literal, Optional
from datetime import date
from api.correlation.compute import compute_pairwise_correlations
from api.services.bigquery_service import bigquery_service
import pandas as pd

router = Router()

class CorrelationRequest(Schema):
    variables: List[str]  # e.g., ["entity_123_risk", "oil_price", "sanctions_count"]
    start_date: date
    end_date: date
    method: Literal['pearson', 'spearman'] = 'pearson'
    min_effect_size: float = 0.7
    alpha: float = 0.05

class CorrelationResponse(Schema):
    correlations: List[dict]
    n_tested: int
    n_significant: int
    method: str
    bonferroni_threshold: float

@router.post("/compute/", response=CorrelationResponse)
def compute_correlations(request, payload: CorrelationRequest):
    """
    Compute pairwise correlations between selected variables.

    Statistical rigor:
    - Computes Pearson or Spearman correlation with p-values
    - Applies Bonferroni correction for multiple comparisons
    - Filters to significant (p < alpha/n_tests) AND strong (|r| >= min_effect_size)
    - Checks stationarity for time-series data (ADF test)

    Returns only correlations meeting BOTH significance AND effect size thresholds.
    """
    # Extract time-series data for each variable
    data_dict = {}

    for var in payload.variables:
        if var.startswith('entity_'):
            # Entity risk score time series
            entity_id = var.split('_')[1]
            # Query BigQuery entity_mentions joined with events for risk scores
            # Aggregate by date
            query = f\"\"\"
            SELECT
                DATE(em.mentioned_at) as date,
                AVG(e.risk_score) as risk_score
            FROM {bigquery_service.dataset_id}.entity_mentions em
            JOIN {bigquery_service.dataset_id}.events e ON em.event_id = e.id
            WHERE em.entity_id = @entity_id
              AND em.mentioned_at >= @start_date
              AND em.mentioned_at <= @end_date
            GROUP BY date
            ORDER BY date
            \"\"\"
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("entity_id", "STRING", entity_id),
                    bigquery.ScalarQueryParameter("start_date", "DATE", payload.start_date),
                    bigquery.ScalarQueryParameter("end_date", "DATE", payload.end_date)
                ]
            )
            df = bigquery_service.client.query(query, job_config=job_config).to_dataframe()
            data_dict[var] = df.set_index('date')['risk_score']

        elif var in ['oil_price', 'inflation', 'gdp', 'exchange_rate']:
            # FRED economic indicators
            query = f\"\"\"
            SELECT
                observation_date as date,
                value
            FROM {bigquery_service.dataset_id}.fred_indicators
            WHERE series_id = @series_id
              AND observation_date >= @start_date
              AND observation_date <= @end_date
            ORDER BY date
            \"\"\"
            series_map = {
                'oil_price': 'DCOILWTICO',
                'inflation': 'FPCPITOTLZGVEN',
                'gdp': 'NYGDPMKTPCDVZB',
                'exchange_rate': 'DEXVZUS'
            }
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("series_id", "STRING", series_map[var]),
                    bigquery.ScalarQueryParameter("start_date", "DATE", payload.start_date),
                    bigquery.ScalarQueryParameter("end_date", "DATE", payload.end_date)
                ]
            )
            df = bigquery_service.client.query(query, job_config=job_config).to_dataframe()
            data_dict[var] = df.set_index('date')['value']

        elif var.endswith('_count'):
            # Event count aggregates (e.g., sanctions_count, political_count)
            event_type = var.replace('_count', '')
            query = f\"\"\"
            SELECT
                DATE(mentioned_at) as date,
                COUNT(*) as count
            FROM {bigquery_service.dataset_id}.events
            WHERE type = @event_type
              AND mentioned_at >= @start_date
              AND mentioned_at <= @end_date
            GROUP BY date
            ORDER BY date
            \"\"\"
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("event_type", "STRING", event_type),
                    bigquery.ScalarQueryParameter("start_date", "DATE", payload.start_date),
                    bigquery.ScalarQueryParameter("end_date", "DATE", payload.end_date)
                ]
            )
            df = bigquery_service.client.query(query, job_config=job_config).to_dataframe()
            data_dict[var] = df.set_index('date')['count']

    # Align all time series to common dates (inner join)
    df_all = pd.DataFrame(data_dict).dropna()
    data_arrays = {col: df_all[col].values for col in df_all.columns}

    # Compute correlations
    correlations = compute_pairwise_correlations(
        data_arrays,
        method=payload.method,
        alpha=payload.alpha,
        min_effect_size=payload.min_effect_size,
        check_stationarity=True
    )

    # Calculate total tests for reporting
    n_vars = len(payload.variables)
    n_total_tests = (n_vars * (n_vars - 1)) // 2

    return {
        'correlations': correlations,
        'n_tested': n_total_tests,
        'n_significant': len(correlations),
        'method': payload.method,
        'bonferroni_threshold': payload.alpha / n_total_tests if n_total_tests > 0 else payload.alpha
    }
```

**Update backend/api/urls.py:**
Add correlation router to API:
```python
from api.correlation import views as correlation_views

# In api.add_router section:
api.add_router("/correlation", correlation_views.router, tags=["Correlation"])
```

**What to avoid:**
- Don't compute correlations without Bonferroni correction (causes false positives with many tests)
- Don't show weak correlations |r| < 0.5 (visual noise, misleading)
- Don't use string concatenation for SQL (SQL injection risk—use parameterized queries)
- Don't skip stationarity checks for time-series (causes spurious correlations from shared trends)
  </action>
  <verify>
```bash
# Start Django dev server and test endpoint
curl -X POST http://localhost:8000/api/correlation/compute/ \
  -H "Content-Type: application/json" \
  -d '{
    "variables": ["oil_price", "inflation"],
    "start_date": "2024-01-01",
    "end_date": "2024-12-31",
    "method": "pearson",
    "min_effect_size": 0.5,
    "alpha": 0.05
  }'
```
Should return JSON with correlations array, n_tested, n_significant, bonferroni_threshold.
  </verify>
  <done>
- backend/api/correlation/views.py created with django-ninja endpoint
- POST /api/correlation/compute/ endpoint accepts variable selection, date range, method, thresholds
- Queries BigQuery for entity risk scores, FRED indicators, event counts
- Aligns time series to common dates (inner join, drops NaN)
- Calls compute_pairwise_correlations with significance + effect size filtering
- Returns correlation network data with metadata (n_tested, bonferroni_threshold)
- Uses parameterized BigQuery queries (SQL injection prevention)
- Correlation router added to API urls
  </done>
</task>

</tasks>

<verification>
Before declaring phase plan complete:
- [ ] scipy, pandas, statsmodels installed and importable
- [ ] compute_pairwise_correlations function works with sample data
- [ ] Bonferroni correction applied (p_adjusted < alpha threshold)
- [ ] Strong effect size filter (|r| >= min_effect_size)
- [ ] Stationarity check with ADF test for time-series
- [ ] Django API endpoint returns correlation data
- [ ] BigQuery queries use parameterized queries (no SQL injection)
- [ ] API returns n_tested, n_significant, bonferroni_threshold metadata
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Statistical libraries (scipy, statsmodels) installed
- Correlation computation service with significance testing
- Bonferroni correction for multiple comparisons
- Stationarity checks for time-series data
- Django REST API endpoint operational
- BigQuery integration for entity risk, FRED indicators, event counts
- Parameterized queries for security
- No errors or warnings introduced
</success_criteria>

<output>
After completion, create `.planning/phases/15-correlation-pattern-analysis/15-01-SUMMARY.md`:

# Phase 15 Plan 01: Correlation Backend API Summary

**Statistical correlation engine: scipy/statsmodels backend with Bonferroni correction, stationarity checks, and BigQuery time-series integration.**

## Accomplishments

- Installed scipy 1.16.1 and statsmodels 0.14.4 for statistical correlation
- Created correlation computation service with Pearson/Spearman methods
- Implemented Bonferroni correction for multiple comparisons (statsmodels.stats.multitest)
- Added ADF stationarity test with automatic differencing for time-series
- Built Django API endpoint for correlation analysis
- Integrated BigQuery queries for entity risk scores, FRED indicators, event counts
- Applied significance (p_adjusted < alpha) AND effect size (|r| >= threshold) filtering
- Used parameterized BigQuery queries for SQL injection prevention

## Files Created/Modified

- `backend/requirements.txt` - Added scipy, pandas, statsmodels, scikit-learn
- `backend/api/correlation/compute.py` - Correlation computation with significance testing
- `backend/api/correlation/__init__.py` - Package init
- `backend/api/correlation/time_series.py` - Time-series data extraction
- `backend/api/correlation/views.py` - Django API endpoint
- `backend/api/urls.py` - Added correlation router

## Decisions Made

- Used scipy.stats.pearsonr/spearmanr (not custom formulas) per research guidance
- Applied Bonferroni correction (not FDR) for conservative multiple comparison control
- Default thresholds: alpha=0.05, min_effect_size=0.7 for strong correlations only
- Stationarity check enabled by default for time-series (ADF test with differencing)
- Parameterized BigQuery queries for all data extraction (security)
- Inner join for time-series alignment (drops dates missing from any variable)

## Issues Encountered

None

## Next Step

Ready for 15-02-PLAN.md (Frontend correlation graph visualization with Reagraph)
</output>

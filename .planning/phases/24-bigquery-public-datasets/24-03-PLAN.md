---
phase: 24-bigquery-public-datasets
plan: 03
type: execute
---

<objective>
Integrate BigQuery adapters with entity linking and expose entity-enriched data via API.

Purpose: Complete Phase 24 foundation by connecting adapters → entity resolution → API, enabling multi-source entity-centric intelligence queries.
Output: Working ingestion flow with automatic entity linking, API endpoints exposing linked entity data.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-bigquery-public-datasets/24-RESEARCH.md
@.planning/phases/24-bigquery-public-datasets/24-CONTEXT.md
@.planning/phases/24-bigquery-public-datasets/24-01-SUMMARY.md
@.planning/phases/24-bigquery-public-datasets/24-02-SUMMARY.md
@backend/api/routers/entities.py
@backend/api/services/entity_service.py

**Tech stack available:**
- SplinkEntityResolver from Plan 01
- GoogleTrendsAdapter, WorldBankAdapter from Plan 02
- Django-ninja routers (Phase 1)
- BigQuery Event model with metadata

**Established patterns:**
- Phase 6: Entity extraction from events with fuzzy matching
- Phase 14.3: BigQuery DML UPDATE for metadata enrichment
- Phase 18: Cloud Tasks for async processing
- Phase 22: Adapter publish_events() pattern

**From CONTEXT.md - Essential:**
- Entities automatically linked across sources (PDVSA in SEC = Petróleos in WB = pdvsa in Trends)
- High confidence entity matching using research-driven approach (Splink)
- Data available via API (no UI changes in Phase 24)

**From CONTEXT.MD - Out of scope:**
- Correlation engine (Phase 24.1)
- UI redesign (existing entity pages already show metadata)
- Historical backfill (recent data only)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add entity linking to adapter workflow</name>
  <files>backend/data_pipeline/adapters/base.py</files>
  <action>Extend DataSourceAdapter base class to support entity extraction + linking:

**Add new method _extract_and_link_entities(event: BigQueryEvent) -> List[str]:**
- Extract entity mentions from event title + content using simple pattern matching:
  - Split text into words, filter capitalized sequences (potential entities)
  - OR use existing Phase 6 entity extraction patterns if available
- For each potential entity:
  - Call SplinkEntityResolver.resolve_entity(entity_name, source=self.source_name, entity_type="organization", country_code=event.country)
  - Get (canonical_entity_id, confidence, method)
  - Create EntityAlias record if not exists (source=self.source_name, confidence, resolution_method)
  - Update EntityAlias.last_seen = timezone.now() if exists
- Return list of canonical_entity_ids
- Store in event.metadata["linked_entities"] = [canonical_id1, canonical_id2, ...]

**Update publish_events() to call _extract_and_link_entities:**
- After event validation, before BigQuery insert
- Enrich event.metadata with linked_entities list
- Log entity linking stats: "Linked {count} entities to event {event_id}"

Import SplinkEntityResolver from api.services.splink_resolver.
Import CanonicalEntity, EntityAlias from data_pipeline.models.

This keeps entity linking in base class so all adapters (Google Trends, World Bank, future SEC EDGAR) automatically link entities without duplicating code.</action>
  <verify>grep "_extract_and_link_entities" backend/data_pipeline/adapters/base.py shows new method</verify>
  <done>DataSourceAdapter base class updated with entity extraction + linking, publish_events enriches metadata with linked_entities, imports added</done>
</task>

<task type="auto">
  <name>Task 2: Create API endpoint for entity multi-source data</name>
  <files>backend/api/routers/entities.py</files>
  <action>Add new endpoint to existing entities router:

**GET /api/entities/{entity_id}/sources**

Returns all events across all sources (gdelt, google_trends, world_bank) mentioning this canonical entity.

**Implementation:**
```python
from api.services.bigquery_service import bigquery_service
from data_pipeline.models import CanonicalEntity, EntityAlias

@router.get("/{entity_id}/sources")
def get_entity_sources(entity_id: str):
    # Validate entity exists
    try:
        entity = CanonicalEntity.objects.get(id=entity_id)
    except CanonicalEntity.DoesNotExist:
        return {"error": "Entity not found"}, 404

    # Get all aliases for this entity
    aliases = EntityAlias.objects.filter(canonical_entity=entity)

    # Query BigQuery for events with this entity in metadata.linked_entities
    # Use UNNEST to search JSON array
    query = f\"\"\"
    SELECT event_id, title, source, published_at, metadata, country
    FROM `{project}.{dataset}.events_partitioned`
    WHERE '{entity_id}' IN UNNEST(JSON_EXTRACT_ARRAY(metadata, '$.linked_entities'))
    ORDER BY published_at DESC
    LIMIT 100
    \"\"\"

    events = bigquery_service.query(query)

    # Group by source
    by_source = {}
    for event in events:
        source = event['source']
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(event)

    return {
        "entity": {
            "id": str(entity.id),
            "name": entity.primary_name,
            "type": entity.entity_type,
        },
        "aliases": [{"alias": a.alias, "source": a.source, "confidence": a.confidence} for a in aliases],
        "events_by_source": by_source,
        "total_events": len(events),
    }
```

Follow Phase 6 entity API patterns. Use existing bigquery_service for queries. Add django-ninja schema if needed for type safety.</action>
  <verify>grep "get_entity_sources" backend/api/routers/entities.py shows new endpoint</verify>
  <done>API endpoint created, queries BigQuery for multi-source events by entity, returns grouped by source with aliases</done>
</task>

<task type="auto">
  <name>Task 3: Run migrations and test entity linking end-to-end</name>
  <files>None (testing/verification only)</files>
  <action>Execute migrations and test complete flow:

**1. Run migrations:**
```bash
python manage.py makemigrations
python manage.py migrate
```

**2. Test GoogleTrendsAdapter with entity linking:**
```python
from data_pipeline.adapters.google_trends_adapter import GoogleTrendsAdapter
adapter = GoogleTrendsAdapter()
events = adapter.fetch(lookback_days=1)
if events:
    transformed = adapter.transform(events)
    # Check first event has linked_entities in metadata after publish_events
    print(f"Fetched {len(transformed)} events")
```

**3. Test API endpoint:**
```bash
# First, find a canonical_entity_id from database
python manage.py shell -c "from data_pipeline.models import CanonicalEntity; print(CanonicalEntity.objects.first().id if CanonicalEntity.objects.exists() else 'No entities yet')"

# If entity exists, curl API
curl http://localhost:8000/api/entities/{entity_id}/sources
```

**4. Verify:**
- Migrations apply cleanly
- Adapter fetch + transform works
- Entity linking creates CanonicalEntity + EntityAlias records
- API endpoint returns multi-source events

If no events exist yet (expected for first run), verify adapters import and initialize correctly. Entity linking will work once Cloud Functions trigger ingestion on schedule.</action>
  <verify>python manage.py showmigrations shows new CanonicalEntity migration applied, adapter imports work, API endpoint defined in OpenAPI schema</verify>
  <done>Migrations applied, adapters tested (import + initialize), API endpoint verified in schema, ready for scheduled ingestion</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `python manage.py showmigrations data_pipeline` shows CanonicalEntity migration applied
- [ ] `python -c "from data_pipeline.adapters.google_trends_adapter import GoogleTrendsAdapter; a = GoogleTrendsAdapter()"` succeeds
- [ ] `python -c "from data_pipeline.adapters.world_bank_adapter import WorldBankAdapter; a = WorldBankAdapter()"` succeeds
- [ ] `grep "_extract_and_link_entities" backend/data_pipeline/adapters/base.py` shows entity linking method
- [ ] `grep "get_entity_sources" backend/api/routers/entities.py` shows API endpoint
- [ ] Django server starts without errors: `python manage.py check` passes
</verification>

<success_criteria>

- All migrations applied successfully
- DataSourceAdapter base class enriches events with entity linking
- GoogleTrendsAdapter and WorldBankAdapter inherit entity linking automatically
- API endpoint `/api/entities/{id}/sources` returns multi-source events
- All verification checks pass
- **Phase 24 complete** - foundation ready for Phase 24.1 correlation engine
</success_criteria>

<output>
After completion, create `.planning/phases/24-bigquery-public-datasets/24-03-SUMMARY.md`:

# Phase 24 Plan 03: API Integration & Entity Linking Summary

**Multi-source entity-centric intelligence pipeline operational**

## Accomplishments

- Entity linking integrated into DataSourceAdapter base class (automatic for all adapters)
- SplinkEntityResolver enriches event metadata with linked_entities list
- API endpoint `/api/entities/{id}/sources` exposes multi-source data grouped by source
- CanonicalEntity + EntityAlias migrations applied to database
- End-to-end flow tested: adapter → entity linking → BigQuery → API

## Files Created/Modified

- `backend/data_pipeline/adapters/base.py` - Added _extract_and_link_entities() method
- `backend/api/routers/entities.py` - Added get_entity_sources endpoint
- `backend/data_pipeline/migrations/XXXX_canonical_entities.py` - Applied

## Decisions Made

- Entity linking happens at publish time (not query time) for performance
- Metadata.linked_entities stores canonical IDs (not aliases) for consistent queries
- API groups events by source to show multi-source intelligence coverage
- Simple capitalized-word extraction for entity mentions (Phase 6 extraction can enhance later)

## Issues Encountered

None - SEC EDGAR adapter remains stubbed pending schema discovery (expected from Plan 02)

## Next Step

**Phase 24 complete!**

Foundation ready for Phase 24.1 correlation engine:
- Entity linking operational across Google Trends + World Bank
- Canonical entity registry tracking aliases with confidence scores
- API exposing entity-centric multi-source data
- SEC EDGAR pending schema discovery (can be added when schema known)

Ready for `/gsd:progress` to continue roadmap.
</output>

---
phase: 24-bigquery-public-datasets
plan: 02
type: execute
---

<objective>
Create BigQuery adapters for 3 public datasets (Google Trends, SEC EDGAR, World Bank) following Phase 22 DataSourceAdapter pattern.

Purpose: Ingest multi-source data with native update cadences (Google Trends daily, SEC EDGAR hourly, World Bank quarterly) to feed entity linking pipeline.
Output: Three working adapters publishing events to BigQuery with source-specific metadata.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-bigquery-public-datasets/24-RESEARCH.md
@.planning/phases/24-bigquery-public-datasets/24-CONTEXT.md
@.planning/phases/24-bigquery-public-datasets/24-01-SUMMARY.md
@backend/data_pipeline/adapters/gdelt_adapter.py
@backend/data_pipeline/adapters/base.py
@api/bigquery_models.py

**Tech stack available:**
- google-cloud-bigquery (already installed from Phase 14.1)
- BigQuery public datasets (free tier: 1TB/month queries)
- DataSourceAdapter pattern (Phase 22)
- BigQuery Event model with metadata JSONField

**Established patterns:**
- Phase 22: DataSourceAdapter with fetch/transform/validate/publish
- Phase 20: Query GKG by DocumentIdentifier pattern (for SEC EDGAR full-text extraction)
- Phase 18: Cloud Functions for scheduled ingestion
- Phase 14.3: BigQuery DML UPDATE for intelligence results in metadata

**From RESEARCH.md:**
- Google Trends schema: term (STRING), rank (INT), score (INT), refresh_date (DATE), country_name/region_name
- SEC EDGAR: Quarterly updates, full-text searchable, company filings
- World Bank WDI: 1,400 indicators, 1960-2016+, quarterly updates
- Partition filtering: WHERE refresh_date = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) to minimize data scanned

**From CONTEXT.md - Update cadences:**
- Google Trends: Daily polling
- SEC EDGAR: Hourly checks for new filings
- World Bank: Quarterly ingestion

**From CONTEXT.MD - Out of scope:**
- UI changes (backend only)
- Correlation engine (Phase 24.1)
- Historical backfill (start with recent data)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GoogleTrendsAdapter</name>
  <files>backend/data_pipeline/adapters/google_trends_adapter.py</files>
  <action>Create adapter extending DataSourceAdapter (import from data_pipeline.adapters.base):

**Class: GoogleTrendsAdapter**

**source_name property:** Return "google_trends"

**fetch(lookback_days=1) -> List[Dict]:**
- Query `bigquery-public-data.google_trends.international_top_terms` (international version, not US-only)
- WHERE refresh_date = DATE_SUB(CURRENT_DATE(), INTERVAL lookback_days DAY)
- AND country_name = 'Venezuela' (filter for Venezuela-related searches)
- SELECT term, rank, score, refresh_date, country_name, region_name
- Return list of dicts with query results
- Use partition filter to minimize data scanned per RESEARCH.md best practices

**transform(raw_data: List[Dict]) -> List[BigQueryEvent]:**
- Map to BigQueryEvent fields:
  - event_id: f"gt-{refresh_date}-{term.lower().replace(' ', '-')}" (generate stable ID)
  - title: term value
  - source: "google_trends"
  - url: f"https://trends.google.com/trends/explore?q={term}&geo=VE" (construct Trends URL)
  - published_at: refresh_date as datetime (midnight UTC)
  - content: f"Search interest rank #{rank} with score {score} in Venezuela"
  - metadata: {"rank": rank, "score": score, "country": country_name, "region": region_name, "refresh_date": str(refresh_date)}
  - event_type: "social" (search behavior is social signal)
  - country: "VE" (Venezuela ISO code)
- Return list of BigQueryEvent instances

**validate(event: BigQueryEvent) -> Tuple[bool, Optional[str]]:**
- Check event_id exists and follows pattern
- Check metadata has rank, score fields
- Check country is "VE"
- Return (True, None) if valid, (False, error_message) if invalid

Follow GdeltAdapter patterns: logger for errors, try/except around BigQuery calls, use existing gdelt_bigquery_service.client for queries.</action>
  <verify>python -c "from data_pipeline.adapters.google_trends_adapter import GoogleTrendsAdapter; a = GoogleTrendsAdapter(); print('GoogleTrends OK')" succeeds</verify>
  <done>GoogleTrendsAdapter created, queries public dataset with partition filter, transforms to BigQueryEvent, validates correctly</done>
</task>

<task type="auto">
  <name>Task 2: Create SecEdgarAdapter stub with schema discovery TODO</name>
  <files>backend/data_pipeline/adapters/sec_edgar_adapter.py</files>
  <action>Create adapter extending DataSourceAdapter:

**Class: SecEdgarAdapter**

**source_name property:** Return "sec_edgar"

**fetch(lookback_hours=1) -> List[Dict]:**
- Add TODO comment: "Schema discovery required - exact table names unknown per RESEARCH.md open question"
- Query pattern (when schema known): `bigquery-public-data.sec_edgar.{filings_table}`
- WHERE filter for filings mentioning "Venezuela" in full-text
- AND filed within last lookback_hours
- For now, return [] with logger.warning("SEC EDGAR adapter pending schema discovery")

**transform(raw_data: List[Dict]) -> List[BigQueryEvent]:**
- Skeleton implementation: return []
- TODO: Map filing fields to BigQueryEvent once schema discovered

**validate(event: BigQueryEvent) -> Tuple[bool, Optional[str]]:**
- Skeleton implementation: return (True, None)
- TODO: Validate SEC-specific fields once transform implemented

Add module docstring: "SEC EDGAR adapter - REQUIRES SCHEMA DISCOVERY. See RESEARCH.md open questions. Run `bq ls bigquery-public-data:sec_edgar` to discover tables before implementing."

This is intentional stubbing per RESEARCH.md open question #1 - we don't have exact SEC schema yet. Adapter structure is ready for implementation once schema is discovered.</action>
  <verify>python -c "from data_pipeline.adapters.sec_edgar_adapter import SecEdgarAdapter; a = SecEdgarAdapter(); print('SecEdgar stub OK')" succeeds</verify>
  <done>SecEdgarAdapter stub created with TODO comments, fetch returns empty list with warning, ready for schema discovery</done>
</task>

<task type="auto">
  <name>Task 3: Create WorldBankAdapter</name>
  <files>backend/data_pipeline/adapters/world_bank_adapter.py</files>
  <action>Create adapter extending DataSourceAdapter:

**Class: WorldBankAdapter**

**source_name property:** Return "world_bank"

**fetch(lookback_days=90) -> List[Dict]:**
- Query `bigquery-public-data.world_bank_wdi.*` tables (multiple datasets under world_bank_wdi prefix per RESEARCH.md)
- Start with worldbank_wdi dataset (World Development Indicators)
- Filter for Venezuela (country_code = 'VE' or country_name LIKE '%Venezuela%')
- Key indicators per CONTEXT.md: GDP, GNI, population, inflation, trade balance
- WHERE year >= EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL lookback_days DAY)) (quarterly data, use lookback for last ~3 months)
- SELECT indicator_name, indicator_code, country_name, country_code, year, value
- Return list of dicts

**transform(raw_data: List[Dict]) -> List[BigQueryEvent]:**
- Map to BigQueryEvent:
  - event_id: f"wb-{country_code}-{indicator_code}-{year}" (stable ID from WB codes)
  - title: f"{indicator_name} for {country_name} ({year})"
  - source: "world_bank"
  - url: f"https://data.worldbank.org/indicator/{indicator_code}?locations=VE" (WB indicator page)
  - published_at: datetime(year, 12, 31) for year-end (WB data is annual/quarterly)
  - content: f"{indicator_name}: {value}"
  - metadata: {"indicator_code": indicator_code, "indicator_name": indicator_name, "year": year, "value": value, "country_code": country_code}
  - event_type: "economic" (World Bank = economic indicators)
  - country: "VE"

**validate(event: BigQueryEvent) -> Tuple[bool, Optional[str]]:**
- Check metadata has indicator_code, year, value
- Check event_id follows wb-{code}-{year} pattern
- Check country is "VE"
- Return (True, None) or (False, error)

Follow same patterns: logger, BigQuery client reuse, error handling.</action>
  <verify>python -c "from data_pipeline.adapters.world_bank_adapter import WorldBankAdapter; a = WorldBankAdapter(); print('WorldBank OK')" succeeds</verify>
  <done>WorldBankAdapter created, queries WDI dataset for Venezuela indicators, transforms to economic events, validates correctly</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from data_pipeline.adapters.google_trends_adapter import GoogleTrendsAdapter"` imports
- [ ] `python -c "from data_pipeline.adapters.sec_edgar_adapter import SecEdgarAdapter"` imports
- [ ] `python -c "from data_pipeline.adapters.world_bank_adapter import WorldBankAdapter"` imports
- [ ] All adapters extend DataSourceAdapter
- [ ] GoogleTrendsAdapter and WorldBankAdapter have complete implementations
- [ ] SecEdgarAdapter has intentional stub with TODO comments per RESEARCH.md open question
</verification>

<success_criteria>

- GoogleTrendsAdapter fully implemented with BigQuery public dataset query
- SecEdgarAdapter stub created (pending schema discovery per RESEARCH.md)
- WorldBankAdapter fully implemented for Venezuela indicators
- All adapters follow Phase 22 DataSourceAdapter pattern
- All verification checks pass
- Ready for Plan 03 (API integration + entity linking)
</success_criteria>

<output>
After completion, create `.planning/phases/24-bigquery-public-datasets/24-02-SUMMARY.md`:

# Phase 24 Plan 02: BigQuery Adapters Summary

**Three BigQuery public dataset adapters created following DataSourceAdapter pattern**

## Accomplishments

- GoogleTrendsAdapter: Daily polling for Venezuela search trends (top terms, rising queries)
- SecEdgarAdapter: Stub created pending schema discovery (open question from RESEARCH.md)
- WorldBankAdapter: Quarterly ingestion for Venezuela economic indicators (GDP, GNI, population, etc.)
- All adapters use partition filtering for cost optimization (1TB/month free tier)

## Files Created/Modified

- `backend/data_pipeline/adapters/google_trends_adapter.py` - Google Trends BigQuery adapter
- `backend/data_pipeline/adapters/sec_edgar_adapter.py` - SEC EDGAR stub (pending schema)
- `backend/data_pipeline/adapters/world_bank_adapter.py` - World Bank WDI adapter

## Decisions Made

- Google Trends: Use international_top_terms (not US-only top_terms) for Venezuela data
- World Bank: Start with WDI dataset, focus on 5 key indicators (GDP, GNI, population, inflation, trade)
- SEC EDGAR: Stub implementation until schema discovered (via `bq ls bigquery-public-data:sec_edgar`)
- Event type mapping: Google Trends = "social", World Bank = "economic"

## Issues Encountered

SEC EDGAR schema not documented in RESEARCH.md - stubbed adapter pending manual schema discovery. This is expected per RESEARCH.md open question #1.

## Next Step

Ready for 24-03-PLAN.md (API integration + entity linking with SplinkEntityResolver)
</output>

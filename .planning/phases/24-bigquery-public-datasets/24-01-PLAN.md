---
phase: 24-bigquery-public-datasets
plan: 01
type: execute
---

<objective>
Establish entity resolution foundation with Splink probabilistic matching and canonical entity registry.

Purpose: Enable accurate cross-dataset entity linking (PDVSA in SEC filings = Petróleos de Venezuela in World Bank = "pdvsa" in Google Trends) to support multi-source intelligence correlation in Phase 24.1.
Output: Working Splink linker, CanonicalEntity models with alias tracking, hybrid resolution strategy (exact → Splink → LLM).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-bigquery-public-datasets/24-RESEARCH.md
@.planning/phases/24-bigquery-public-datasets/24-CONTEXT.md
@backend/data_pipeline/adapters/gdelt_adapter.py
@backend/data_pipeline/models.py

**Tech stack available:**
- Splink for probabilistic entity resolution (from RESEARCH.md)
- RapidFuzz (Phase 6) for pre-filtering
- Claude API for LLM disambiguation (Phase 7)
- PostgreSQL for canonical entity registry
- DuckDB as Splink backend

**Established patterns:**
- Polyglot persistence: PostgreSQL for reference data, BigQuery for time-series
- DataSourceAdapter pattern from Phase 22
- Metadata JSON fields for flexible data storage
- CharField for event IDs (not ForeignKey) in polyglot architecture

**Constraining decisions:**
- Phase 14.3: Polyglot architecture — PostgreSQL Event model deprecated, entities remain in PostgreSQL
- Phase 22: Convention-based discovery for adapters ({source}_adapter.py → {Source}Adapter class)
- Phase 6: RapidFuzz Jaro-Winkler 0.85 threshold for entity deduplication
- Phase 7: Claude API integration patterns established

**From RESEARCH.md - Don't hand-roll:**
- Probabilistic entity matching → Use Splink (not custom fuzzy matching)
- Blocking strategies → Use Splink blocking rules (not manual pre-filtering)
- Match probability scoring → Use Splink Fellegi-Sunter model

**From CONTEXT.md - Essential:**
- Entity linking across sources is THE critical challenge
- Research-driven entity matching (not arbitrary choice)
- Definition of done: Entities automatically linked across sources with confidence scores
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Splink and configure DuckDB backend</name>
  <files>requirements.txt</files>
  <action>Add Splink with DuckDB backend: `splink>=4.0.0`, `duckdb>=1.0.0`. Run pip install. Verify import works: python -c "from splink import Linker, SettingsCreator, DuckDBAPI; print('Splink OK')". Do NOT add google-cloud-bigquery (already installed), rapidfuzz (already in Phase 6), or anthropic (already in Phase 7).</action>
  <verify>pip list | grep -E "splink|duckdb" shows versions, import test succeeds</verify>
  <done>Splink 4.x and DuckDB 1.x installed in requirements.txt, import test passes</done>
</task>

<task type="auto">
  <name>Task 2: Create CanonicalEntity and EntityAlias models</name>
  <files>backend/data_pipeline/models.py</files>
  <action>Add two models to data_pipeline/models.py following existing MentionSpike pattern:

**CanonicalEntity model:**
- id: UUIDField (primary_key, default=uuid.uuid4)
- primary_name: CharField(max_length=255)
- entity_type: CharField with choices (person/organization/government/location) matching Phase 6 types
- country_code: CharField(max_length=2, null=True, blank=True) for geographic filtering
- metadata: JSONField(default=dict) for flexible additional data
- created_at: DateTimeField(auto_now_add=True)
- last_verified: DateTimeField(auto_now=True)
- Indexes: [entity_type + country_code], [primary_name]

**EntityAlias model:**
- canonical_entity: ForeignKey(CanonicalEntity, on_delete=CASCADE, related_name='aliases')
- alias: CharField(max_length=255) — the actual name variant
- source: CharField(max_length=50) for tracking origin ('gdelt', 'google_trends', 'sec_edgar', 'world_bank')
- confidence: FloatField for Splink match_probability (0.0-1.0)
- resolution_method: CharField choices (exact/splink/llm)
- first_seen: DateTimeField(auto_now_add=True)
- last_seen: DateTimeField(auto_now=True)
- unique_together: [alias, source]
- Indexes: [alias], [canonical_entity + source], [confidence]

Follow CharField for event_id pattern (not ForeignKey to BigQuery data). Use metadata JSONField for flexibility per Phase 4 patterns.</action>
  <verify>python manage.py makemigrations shows new migration for CanonicalEntity + EntityAlias, python manage.py check passes</verify>
  <done>Models created with correct fields, relationships, indexes; migration generated without errors</done>
</task>

<task type="auto">
  <name>Task 3: Create Splink entity resolution service</name>
  <files>backend/api/services/splink_resolver.py</files>
  <action>Create service following Phase 22 adapter patterns and RESEARCH.md Splink examples:

**Class: SplinkEntityResolver**

**__init__:**
- Configure SettingsCreator with link_type="link_only" (cross-dataset matching, not deduplication)
- Comparisons: JaroWinklerAtThresholds("entity_name", [0.9, 0.85]), ExactMatch("country_code")
- Blocking rules: block_on("substr(entity_name, 1, 3)"), block_on("country_code"), block_on("entity_type")
- Initialize Linker with DuckDBAPI backend
- No training data needed (unsupervised learning per RESEARCH.md)

**resolve_entity(entity_name: str, source: str, entity_type: str, country_code: Optional[str]) -> Tuple[str, float, str]:**
- Tier 1 (exact match): Query EntityAlias for exact alias match (case-insensitive), confidence >= 0.95 → return canonical_id
- Tier 2 (Splink): If no exact match, use linker.inference.predict() with threshold_match_probability=0.7
  - If match_probability >= 0.85: return matched canonical_id, probability, "splink"
- Tier 3 (create new): If no matches or all below threshold, create new CanonicalEntity
- Return (canonical_entity_id, confidence, method)

**_train_linker() (if needed):**
- Call linker.training.estimate_u_using_random_sampling(max_pairs=1e6)
- Call linker.training.estimate_parameters_using_expectation_maximisation() on blocking rules
- Per RESEARCH.md: unsupervised learning, no labels required

Use existing patterns: timezone.now() not datetime.utcnow() (Phase 6 decision), logger for errors.

Do NOT implement LLM tier 3 yet (that's Phase 24.1 correlation logic). For now, create new canonical entity if Splink confidence < 0.85.</action>
  <verify>python -c "from api.services.splink_resolver import SplinkEntityResolver; r = SplinkEntityResolver(); print('Resolver OK')" succeeds</verify>
  <done>SplinkEntityResolver class created, DuckDB linker initializes, resolve_entity method implements hybrid strategy (exact → Splink → new entity), import test passes</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pip list | grep splink` shows splink 4.x installed
- [ ] `python manage.py makemigrations` generates CanonicalEntity + EntityAlias migration
- [ ] `python manage.py check` passes without errors
- [ ] `python -c "from api.services.splink_resolver import SplinkEntityResolver"` imports successfully
- [ ] No TypeScript/frontend changes (backend-only plan)
</verification>

<success_criteria>

- Splink and DuckDB installed in requirements.txt
- CanonicalEntity and EntityAlias models created with correct schema
- Database migration generated for entity registry models
- SplinkEntityResolver service implements hybrid resolution (exact → Splink → new)
- All verification checks pass
- Ready for Plan 02 (BigQuery adapters)
</success_criteria>

<output>
After completion, create `.planning/phases/24-bigquery-public-datasets/24-01-SUMMARY.md`:

# Phase 24 Plan 01: Entity Resolution Foundation Summary

**Entity resolution foundation established with Splink and canonical registry**

## Accomplishments

- Installed Splink 4.x and DuckDB 1.x for probabilistic entity matching
- Created CanonicalEntity and EntityAlias models in PostgreSQL
- Built SplinkEntityResolver with hybrid strategy (exact → Splink → new entity)
- DuckDB backend configured with blocking rules for scalability

## Files Created/Modified

- `requirements.txt` - Added splink>=4.0.0, duckdb>=1.0.0
- `backend/data_pipeline/models.py` - CanonicalEntity + EntityAlias models
- `backend/data_pipeline/migrations/XXXX_canonical_entities.py` - Generated migration
- `backend/api/services/splink_resolver.py` - SplinkEntityResolver service

## Decisions Made

- Tier 1 exact match threshold: 0.95 confidence (high-confidence only)
- Tier 2 Splink threshold: 0.85 match_probability (per RESEARCH.md recommendation)
- Tier 3 LLM disambiguation: Deferred to Phase 24.1 (create new entity for now)
- Blocking rules: First 3 chars + country_code + entity_type (reduces O(n²) to O(n))

## Issues Encountered

None

## Next Step

Ready for 24-02-PLAN.md (BigQuery adapters for Google Trends, SEC EDGAR, World Bank)
</output>

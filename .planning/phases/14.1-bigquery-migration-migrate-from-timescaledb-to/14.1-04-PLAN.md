---
phase: 14.1-bigquery-migration
plan: 04
type: execute
---

<objective>
Backfill BigQuery with historical data from PostgreSQL, validate migration, deprecate TimescaleDB models.

Purpose: Complete data migration to BigQuery, ensure data integrity, prepare for PostgreSQL Event/EntityMention model removal.

Output: BigQuery tables populated with historical data, validation tests passing, PostgreSQL Event/EntityMention models marked deprecated, migration documentation complete.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/TIMESCALEDB-MIGRATION.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-01-PLAN.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-02-PLAN.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-03-PLAN.md

**Migration strategy:** BigQuery infrastructure created (14.1-01), ingestion updated (14.1-02), API views migrated (14.1-03). Final step: backfill historical data and validate.

**Data to migrate:**
- Event model records from PostgreSQL to BigQuery events table
- EntityMention model records from PostgreSQL to BigQuery entity_mentions table
- FRED/UN Comtrade/World Bank data if stored in Event.metadata (unlikely, probably in separate tables)

**Validation requirements:**
- Row count match between PostgreSQL and BigQuery
- Sample data integrity checks (fields match, no corruption)
- Query performance testing (ensure BigQuery queries are fast)
- End-to-end testing (dashboard, entities page, chat all functional)

**Constraining decisions:**
- Phase 1: Event model with JSONField metadata, TimescaleDB hypertables (now deprecated)
- Phase 6: EntityMention with denormalized mentioned_at for trending
- TIMESCALEDB-MIGRATION.md: Timeline Phase 6 (4 hours) for data migration
- Keep PostgreSQL Event/EntityMention models during migration for rollback safety
- Delete after successful validation and production deployment

**Strategy:**
1. Create Django management command for one-time data migration
2. Batch export from PostgreSQL, batch import to BigQuery
3. Validation: row counts, sample checks, query comparison
4. Mark PostgreSQL models as deprecated (comments, not deletion)
5. Document migration in README and deployment guide
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data migration management command</name>
  <files>backend/api/management/commands/migrate_to_bigquery.py</files>
  <action>
Create Django management command for one-time historical data migration:

**File: backend/api/management/commands/migrate_to_bigquery.py**

```python
"""
One-time migration command: Export PostgreSQL Event/EntityMention to BigQuery.

Usage:
    python manage.py migrate_to_bigquery --batch-size 1000 --dry-run
    python manage.py migrate_to_bigquery --batch-size 1000  # Execute migration

Flags:
    --batch-size: Number of records to process per batch (default: 1000)
    --dry-run: Print migration plan without executing
    --table: Migrate specific table (events, entity_mentions, all) (default: all)
"""

from django.core.management.base import BaseCommand
from django.db import connection
from api.models import Event, EntityMention
from api.bigquery_models import Event as BigQueryEvent, EntityMention as BigQueryEntityMention
from api.services.bigquery_service import bigquery_service
from datetime import datetime
import uuid


class Command(BaseCommand):
    help = 'Migrate Event and EntityMention data from PostgreSQL to BigQuery'

    def add_arguments(self, parser):
        parser.add_argument('--batch-size', type=int, default=1000, help='Batch size for processing')
        parser.add_argument('--dry-run', action='store_true', help='Print plan without executing')
        parser.add_argument('--table', type=str, default='all', choices=['events', 'entity_mentions', 'all'])

    def handle(self, *args, **options):
        batch_size = options['batch_size']
        dry_run = options['dry_run']
        table = options['table']

        self.stdout.write(self.style.WARNING(f'\n=== BigQuery Migration {"(DRY RUN)" if dry_run else ""} ===\n'))

        if table in ['events', 'all']:
            self.migrate_events(batch_size, dry_run)

        if table in ['entity_mentions', 'all']:
            self.migrate_entity_mentions(batch_size, dry_run)

        self.stdout.write(self.style.SUCCESS('\n=== Migration Complete ===\n'))

    def migrate_events(self, batch_size, dry_run):
        """Migrate Event records to BigQuery events table."""
        total_count = Event.objects.count()
        self.stdout.write(f'Events to migrate: {total_count}')

        if dry_run:
            self.stdout.write(self.style.WARNING('  [DRY RUN] Would migrate events in batches of {batch_size}'))
            return

        migrated = 0
        batch = []

        for event in Event.objects.order_by('id').iterator(chunk_size=batch_size):
            # Convert Django Event to BigQuery Event
            bq_event = BigQueryEvent(
                id=str(event.id) if event.id else str(uuid.uuid4()),
                title=event.title,
                content=event.content,
                source_url=event.source_url,
                source_name=event.source_name,
                event_type=event.event_type,
                location=event.location,
                risk_score=event.risk_score,
                severity=event.severity,
                mentioned_at=event.mentioned_at,
                created_at=event.created_at,
                metadata=event.metadata or {}
            )
            batch.append(bq_event)

            if len(batch) >= batch_size:
                bigquery_service.insert_events(batch)
                migrated += len(batch)
                self.stdout.write(f'  Migrated {migrated}/{total_count} events...')
                batch = []

        # Insert remaining batch
        if batch:
            bigquery_service.insert_events(batch)
            migrated += len(batch)

        self.stdout.write(self.style.SUCCESS(f'  ✓ Migrated {migrated} events'))

    def migrate_entity_mentions(self, batch_size, dry_run):
        """Migrate EntityMention records to BigQuery entity_mentions table."""
        total_count = EntityMention.objects.count()
        self.stdout.write(f'EntityMentions to migrate: {total_count}')

        if dry_run:
            self.stdout.write(self.style.WARNING(f'  [DRY RUN] Would migrate entity_mentions in batches of {batch_size}'))
            return

        migrated = 0
        batch = []

        for mention in EntityMention.objects.select_related('entity', 'event').order_by('id').iterator(chunk_size=batch_size):
            # Convert Django EntityMention to BigQuery EntityMention
            bq_mention = BigQueryEntityMention(
                id=str(mention.id) if mention.id else str(uuid.uuid4()),
                entity_id=str(mention.entity.id),
                event_id=str(mention.event.id),
                mentioned_at=mention.mentioned_at,
                context=mention.context or ''
            )
            batch.append(bq_mention)

            if len(batch) >= batch_size:
                bigquery_service.insert_entity_mentions(batch)
                migrated += len(batch)
                self.stdout.write(f'  Migrated {migrated}/{total_count} entity_mentions...')
                batch = []

        # Insert remaining batch
        if batch:
            bigquery_service.insert_entity_mentions(batch)
            migrated += len(batch)

        self.stdout.write(self.style.SUCCESS(f'  ✓ Migrated {migrated} entity_mentions'))
```

**Error handling:**
- Wrap bigquery_service.insert_* in try/except to catch BigQuery errors
- Log failed batches to file for retry
- Add --skip-errors flag to continue on errors

**Idempotency:**
- If re-run, skip records that already exist in BigQuery (check by ID)
- Add --force flag to overwrite existing records

**Run:**
```bash
python manage.py migrate_to_bigquery --dry-run  # Preview
python manage.py migrate_to_bigquery  # Execute
```
  </action>
  <verify>python manage.py migrate_to_bigquery --dry-run shows migration plan, no errors</verify>
  <done>Migration command created, dry-run shows correct record counts, ready for execution</done>
</task>

<task type="auto">
  <name>Task 2: Execute data migration and validate</name>
  <files>None (command execution)</files>
  <action>
Execute data migration and validate results:

**Step 1: Run migration command**
```bash
# Dry run first
python manage.py migrate_to_bigquery --dry-run

# Execute migration
python manage.py migrate_to_bigquery --batch-size 1000
```

**Step 2: Validate row counts**
```bash
# PostgreSQL counts
python manage.py shell -c "from api.models import Event, EntityMention; print(f'PG Events: {Event.objects.count()}'); print(f'PG EntityMentions: {EntityMention.objects.count()}')"

# BigQuery counts
bq query --use_legacy_sql=false "SELECT COUNT(*) as count FROM \`venezuelawatch-dev.venezuelawatch_analytics.events\`"
bq query --use_legacy_sql=false "SELECT COUNT(*) as count FROM \`venezuelawatch-dev.venezuelawatch_analytics.entity_mentions\`"

# Counts should match
```

**Step 3: Sample data integrity checks**

Create validation script `backend/scripts/validate_migration.py`:

```python
"""
Validate BigQuery migration data integrity.

Checks:
1. Row counts match between PostgreSQL and BigQuery
2. Sample records have matching field values
3. No null values in required fields
4. Date ranges match
"""

from api.models import Event, EntityMention
from api.services.bigquery_service import bigquery_service
from django.conf import settings
from google.cloud import bigquery
import random

def validate_events():
    """Validate Event migration."""
    print('=== Validating Events ===')

    # Count check
    pg_count = Event.objects.count()
    bq_result = bigquery_service.client.query(
        f"SELECT COUNT(*) as count FROM `{settings.GCP_PROJECT_ID}.{settings.BIGQUERY_DATASET}.events`"
    ).result()
    bq_count = next(bq_result).count

    print(f'PostgreSQL count: {pg_count}')
    print(f'BigQuery count: {bq_count}')
    assert pg_count == bq_count, f'Count mismatch: PG {pg_count} != BQ {bq_count}'

    # Sample field check (10 random records)
    sample_ids = list(Event.objects.values_list('id', flat=True).order_by('?')[:10])
    for event_id in sample_ids:
        pg_event = Event.objects.get(id=event_id)
        bq_result = bigquery_service.client.query(
            f"""
            SELECT * FROM `{settings.GCP_PROJECT_ID}.{settings.BIGQUERY_DATASET}.events`
            WHERE id = '{event_id}' LIMIT 1
            """
        ).result()
        bq_event = next(bq_result, None)

        if not bq_event:
            print(f'  ✗ Event {event_id} missing in BigQuery')
            continue

        # Compare fields
        assert pg_event.title == bq_event.title, f'Title mismatch for {event_id}'
        assert pg_event.source_url == bq_event.source_url, f'URL mismatch for {event_id}'
        assert float(pg_event.risk_score or 0) == float(bq_event.risk_score or 0), f'Risk score mismatch for {event_id}'

    print('  ✓ Sample validation passed')

def validate_entity_mentions():
    """Validate EntityMention migration."""
    print('=== Validating EntityMentions ===')

    # Count check
    pg_count = EntityMention.objects.count()
    bq_result = bigquery_service.client.query(
        f"SELECT COUNT(*) as count FROM `{settings.GCP_PROJECT_ID}.{settings.BIGQUERY_DATASET}.entity_mentions`"
    ).result()
    bq_count = next(bq_result).count

    print(f'PostgreSQL count: {pg_count}')
    print(f'BigQuery count: {bq_count}')
    assert pg_count == bq_count, f'Count mismatch: PG {pg_count} != BQ {bq_count}'

    print('  ✓ Count validation passed')

if __name__ == '__main__':
    import django
    django.setup()
    validate_events()
    validate_entity_mentions()
    print('\n=== All Validation Checks Passed ===')
```

Run: `python backend/scripts/validate_migration.py`

**Step 4: Query performance testing**

Time comparison queries:
```python
# PostgreSQL
import time
start = time.time()
Event.objects.filter(mentioned_at__gte='2024-01-01').count()
print(f'PG: {time.time() - start}s')

# BigQuery
start = time.time()
bigquery_service.get_recent_events(datetime(2024, 1, 1), datetime.now(), limit=1000)
print(f'BQ: {time.time() - start}s')
```

**Step 5: End-to-end testing**
- Load dashboard: Events list should populate
- Load entities page: Trending entities should show
- Open chat: Ask "Show recent events" - tool should return data
- All pages should load without errors

**Success criteria:**
- Row counts match exactly
- Sample data integrity checks pass
- BigQuery queries complete in < 2 seconds
- No frontend errors
  </action>
  <verify>Migration command completes successfully, validation script passes all checks, frontend loads data correctly</verify>
  <done>Data migration complete, validation passed (counts match, sample checks pass, queries fast), frontend functional</done>
</task>

<task type="auto">
  <name>Task 3: Deprecate PostgreSQL Event/EntityMention models</name>
  <files>backend/api/models.py, backend/README.md</files>
  <action>
Mark PostgreSQL Event/EntityMention models as deprecated (don't delete yet):

**backend/api/models.py changes:**

Add deprecation warnings to Event and EntityMention models:

```python
class Event(models.Model):
    """
    DEPRECATED: This model is deprecated as of Phase 14.1 (BigQuery Migration).
    Time-series event data is now stored in BigQuery.

    DO NOT USE for new code. Use api.bigquery_models.Event and api.services.bigquery_service instead.

    This model is kept temporarily for:
    1. Rollback safety during migration
    2. Historical reference
    3. Gradual deprecation

    REMOVAL TIMELINE: After successful production deployment and 30-day stabilization period.
    """
    # ... existing fields ...

    class Meta:
        db_table = 'events'
        # Consider adding: managed = False  # To prevent migrations


class EntityMention(models.Model):
    """
    DEPRECATED: This model is deprecated as of Phase 14.1 (BigQuery Migration).
    Entity mention data is now stored in BigQuery.

    DO NOT USE for new code. Use api.bigquery_models.EntityMention and api.services.bigquery_service instead.

    REMOVAL TIMELINE: After successful production deployment and 30-day stabilization period.
    """
    # ... existing fields ...

    class Meta:
        db_table = 'entity_mentions'
```

**backend/README.md additions:**

Add migration documentation section:

```markdown
## BigQuery Migration (Phase 14.1)

VenezuelaWatch uses polyglot persistence:
- **PostgreSQL (Cloud SQL)**: User accounts, Entity metadata, reference data (via Django ORM)
- **BigQuery**: Event time-series, EntityMention, economic indicators (via BigQueryService)

### Why BigQuery?

TimescaleDB extension is not available on Cloud SQL PostgreSQL. BigQuery provides:
- Native GCP integration (already using for forecasting in Phase 14)
- Excellent time-series query performance with partitioning
- Scales to billions of rows without configuration
- Lower cost than TimescaleDB on dedicated instance

### Querying Time-Series Data

**DO NOT** use Django ORM for time-series data:
```python
# ❌ DEPRECATED (PostgreSQL Event model)
events = Event.objects.filter(mentioned_at__gte=start_date)

# ✅ CORRECT (BigQuery service)
from api.services.bigquery_service import bigquery_service
events = bigquery_service.get_recent_events(start_date, end_date)
```

### Local Development Setup

1. Authenticate with GCP:
   ```bash
   gcloud auth application-default login
   ```

2. Set environment variables in `.env`:
   ```
   GCP_PROJECT_ID=venezuelawatch-dev
   BIGQUERY_DATASET=venezuelawatch_analytics
   ```

3. Verify BigQuery access:
   ```bash
   bq ls venezuelawatch_analytics
   ```

### Data Migration

Historical data was migrated from PostgreSQL to BigQuery using:
```bash
python manage.py migrate_to_bigquery
```

This is a **one-time migration**. New data is written directly to BigQuery by Celery ingestion tasks.

### Deprecated Models

- `api.models.Event` - Use `api.bigquery_models.Event` and `bigquery_service.get_recent_events()`
- `api.models.EntityMention` - Use `api.bigquery_models.EntityMention` and `bigquery_service.get_entity_events()`

These models will be removed after 30-day production stabilization period.
```

**Create migration checklist:**

Create `backend/docs/MIGRATION-CHECKLIST.md`:

```markdown
# BigQuery Migration Checklist

## Pre-Deployment
- [x] BigQuery dataset and tables created
- [x] BigQuery service layer implemented
- [x] Celery ingestion tasks updated
- [x] API views migrated
- [x] Historical data backfilled
- [x] Validation tests passed
- [ ] Production deployment planned

## Production Deployment
- [ ] Run migration command on production database
- [ ] Validate production data migration
- [ ] Monitor error rates for 24 hours
- [ ] Verify BigQuery costs are as expected (~$190/month)

## Post-Deployment (30-day stabilization)
- [ ] No rollback incidents
- [ ] Query performance acceptable (< 2s)
- [ ] Data integrity maintained
- [ ] Cost within budget

## Cleanup (after 30 days)
- [ ] Delete PostgreSQL Event table
- [ ] Delete PostgreSQL EntityMention table
- [ ] Remove deprecated models from codebase
- [ ] Remove migration command
- [ ] Archive migration documentation
```
  </action>
  <verify>Models have deprecation warnings, README documents migration, checklist created</verify>
  <done>PostgreSQL models deprecated with warnings, migration documented in README, checklist created for production deployment</done>
</task>

<task type="manual">
  <name>Task 4: Update deployment configuration for BigQuery</name>
  <files>backend/deploy/cloudbuild.yaml (if exists), backend/docs/DEPLOYMENT.md</files>
  <action>
Document BigQuery deployment requirements:

**If deployment configs exist (cloudbuild.yaml, terraform, etc.):**

1. **Service Account Permissions:**
   - Ensure Cloud Run/Functions service account has BigQuery roles:
     - `roles/bigquery.dataEditor` - For insert operations
     - `roles/bigquery.jobUser` - For query jobs

2. **Environment Variables:**
   - Add to Cloud Run/Functions environment:
     ```yaml
     env:
       - name: GCP_PROJECT_ID
         value: venezuelawatch-prod
       - name: BIGQUERY_DATASET
         value: venezuelawatch_analytics
     ```

3. **Dependencies:**
   - Ensure `google-cloud-bigquery` in requirements.txt
   - Verify version compatibility

**Create DEPLOYMENT.md if doesn't exist:**

```markdown
# Deployment Guide

## Prerequisites

1. **GCP Project Setup:**
   - Cloud SQL PostgreSQL instance (for User/Entity models)
   - BigQuery dataset `venezuelawatch_analytics` (for time-series data)
   - Cloud Storage bucket (for static files)
   - Secret Manager (for API keys)

2. **Service Account Permissions:**
   ```bash
   gcloud projects add-iam-policy-binding PROJECT_ID \
     --member="serviceAccount:SERVICE_ACCOUNT_EMAIL" \
     --role="roles/bigquery.dataEditor"

   gcloud projects add-iam-policy-binding PROJECT_ID \
     --member="serviceAccount:SERVICE_ACCOUNT_EMAIL" \
     --role="roles/bigquery.jobUser"
   ```

3. **BigQuery Dataset Creation:**
   ```bash
   bq mk --dataset --location=US PROJECT_ID:venezuelawatch_analytics
   bq query --use_legacy_sql=false < backend/config/bigquery_schema.sql
   ```

## Deployment Steps

1. Deploy Cloud SQL and run Django migrations:
   ```bash
   python manage.py migrate
   ```

2. Create BigQuery tables:
   ```bash
   bq query --use_legacy_sql=false < backend/config/bigquery_schema.sql
   ```

3. Run data migration (one-time):
   ```bash
   python manage.py migrate_to_bigquery
   ```

4. Deploy backend application (Cloud Run/Functions)

5. Verify deployment:
   ```bash
   curl https://api.venezuelawatch.com/health
   ```

## Monitoring

- **BigQuery Costs:** Monitor in GCP Console → BigQuery → Pricing
  - Expected: ~$190/month (storage + queries)
- **Query Performance:** Check BigQuery → Job History for slow queries
- **Data Freshness:** Monitor Celery task logs for ingestion success rates
```

**Verification approach:**
- Review deployment docs with team
- Test deployment process in staging environment if available
  </action>
  <verify>Deployment docs updated, service account permissions documented, environment variables listed</verify>
  <done>Deployment configuration documented, BigQuery permissions specified, ready for production deployment</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Migration command created and dry-run successful
- [ ] Data migration executed: row counts match between PostgreSQL and BigQuery
- [ ] Validation script passes all checks (counts, sample data, date ranges)
- [ ] Query performance acceptable (< 2s for typical queries)
- [ ] End-to-end testing: dashboard, entities, chat all functional
- [ ] PostgreSQL models marked as deprecated with clear warnings
- [ ] Migration documented in README.md
- [ ] Deployment checklist created
- [ ] Deployment docs updated with BigQuery requirements
- [ ] No errors in Django check or BigQuery queries
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Historical data migrated to BigQuery successfully
- Validation confirms data integrity
- PostgreSQL models deprecated (not deleted)
- Migration fully documented
- Deployment guide updated
- Ready for production deployment
- Phase 14.1 complete
</success_criteria>

<output>
After completion, create `.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-04-SUMMARY.md`:

# Phase 14.1 Plan 04: Data Migration & Validation Summary

**Migration complete: Historical data backfilled, validation passed, PostgreSQL models deprecated, ready for production.**

## Accomplishments

- Created Django management command for one-time data migration
- Migrated X events and Y entity_mentions from PostgreSQL to BigQuery (fill in actual numbers)
- Validation passed: row counts match, sample data integrity confirmed
- Query performance acceptable: < 2s for typical queries
- PostgreSQL Event/EntityMention models marked deprecated
- Migration fully documented in README.md
- Deployment checklist and guide created

## Files Created/Modified

- `backend/api/management/commands/migrate_to_bigquery.py` - Data migration command
- `backend/scripts/validate_migration.py` - Validation script
- `backend/api/models.py` - Added deprecation warnings to Event/EntityMention
- `backend/README.md` - BigQuery migration documentation
- `backend/docs/MIGRATION-CHECKLIST.md` - Production deployment checklist
- `backend/docs/DEPLOYMENT.md` - Deployment guide with BigQuery setup

## Decisions Made

- Keep PostgreSQL Event/EntityMention models for 30-day stabilization period (rollback safety)
- Batch size 1000 for migration (balance between performance and memory)
- Validation checks: row counts, sample data, query performance, end-to-end testing
- Deprecation warnings in models (not deletion) for gradual migration
- Service account needs `roles/bigquery.dataEditor` and `roles/bigquery.jobUser`

## Migration Statistics

- Events migrated: [COUNT]
- EntityMentions migrated: [COUNT]
- Migration duration: [TIME]
- Validation checks: All passed ✓

## Issues Encountered

None (or document any issues and resolutions)

## Next Step

**Phase 14.1 Complete!** Ready for production deployment or continue to Phase 15 (Correlation & Pattern Analysis).

Production deployment checklist in `backend/docs/MIGRATION-CHECKLIST.md`.
</output>

---
phase: 14.1-bigquery-migration
plan: 03
type: execute
---

<objective>
Update Django API views to query BigQuery instead of PostgreSQL for time-series data.

Purpose: Complete polyglot persistence migration - API endpoints serve BigQuery data, Django ORM remains for User/Entity reference data.

Output: Phase 4/6/7 API endpoints querying BigQuery, trending/risk/event endpoints using BigQueryService, PostgreSQL Event/EntityMention models deprecated but not deleted (rollback safety).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/TIMESCALEDB-MIGRATION.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-01-PLAN.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-02-PLAN.md

**Migration strategy:** BigQuery service layer created (14.1-01), ingestion tasks updated (14.1-02). Now update API views.

**Affected API endpoints (backend/api/views.py):**
- `/api/events/` - GET events with filters (event_type, severity, risk_score, time range)
- `/api/events/{id}/` - GET single event detail
- `/api/entities/trending/` - GET trending entities (Phase 6 Plan 03)
- `/api/entities/{id}/profile/` - GET entity profile with events/mentions (Phase 6 Plan 03)
- `/api/risk/trends/` - GET risk trends over time (Phase 4 Plan 04)
- `/api/chat/tools/` - Tool functions used by AI chat (Phase 7)

**Constraining decisions:**
- Phase 1: django-ninja Router pattern for API organization
- Phase 4: Risk score 0-100 scale, SEV1-5 severity levels
- Phase 6: Exponential time-decay trending (7-day half-life), metric toggle (mentions/risk/sanctions)
- Phase 6: Redis Sorted Sets for trending (keep for now, evaluate in data migration plan)
- Phase 7: SSE streaming chat with tool calling (search_events, get_entity_profile, get_trending_entities, analyze_risk_trends)
- 14.1-01: BigQueryService query methods (get_recent_events, get_entity_trending, get_risk_trends)

**Strategy:**
1. Update views to use BigQueryService instead of Django ORM queries
2. Keep Entity PostgreSQL queries (reference data, not time-series)
3. Update trending logic to query BigQuery entity_mentions (or keep Redis cache)
4. Add query methods to BigQueryService as needed for view requirements
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update events list and detail endpoints</name>
  <files>backend/api/views.py, backend/api/services/bigquery_service.py</files>
  <action>
Update event endpoints (Phase 1/4) to query BigQuery:

**backend/api/views.py changes:**

1. **Import BigQueryService:**
   ```python
   from api.services.bigquery_service import bigquery_service
   ```

2. **Update `/api/events/` GET endpoint:**
   - Old: `Event.objects.filter(...)` Django ORM query
   - New: `bigquery_service.get_recent_events(start_date, end_date, event_type, limit)`
   - Filter parameters: event_type, severity, min_risk_score, max_risk_score, start_date, end_date, limit
   - **Challenge:** 14.1-01 BigQueryService.get_recent_events() only has event_type filter
   - **Solution:** Add severity and risk_score filtering to BigQueryService method

3. **Update BigQueryService.get_recent_events():**
   ```python
   def get_recent_events(
       self,
       start_date: datetime,
       end_date: datetime,
       event_type: Optional[str] = None,
       severity: Optional[str] = None,
       min_risk_score: Optional[float] = None,
       max_risk_score: Optional[float] = None,
       limit: int = 100
   ) -> List[dict]:
       query = f"""
           SELECT *
           FROM `{self.project_id}.{self.dataset_id}.events`
           WHERE mentioned_at BETWEEN @start_date AND @end_date
       """

       params = [
           bigquery.ScalarQueryParameter('start_date', 'TIMESTAMP', start_date),
           bigquery.ScalarQueryParameter('end_date', 'TIMESTAMP', end_date),
       ]

       if event_type:
           query += " AND event_type = @event_type"
           params.append(bigquery.ScalarQueryParameter('event_type', 'STRING', event_type))

       if severity:
           query += " AND severity = @severity"
           params.append(bigquery.ScalarQueryParameter('severity', 'STRING', severity))

       if min_risk_score is not None:
           query += " AND risk_score >= @min_risk_score"
           params.append(bigquery.ScalarQueryParameter('min_risk_score', 'FLOAT64', min_risk_score))

       if max_risk_score is not None:
           query += " AND risk_score <= @max_risk_score"
           params.append(bigquery.ScalarQueryParameter('max_risk_score', 'FLOAT64', max_risk_score))

       query += " ORDER BY mentioned_at DESC LIMIT @limit"
       params.append(bigquery.ScalarQueryParameter('limit', 'INT64', limit))

       job_config = bigquery.QueryJobConfig(query_parameters=params)
       results = self.client.query(query, job_config=job_config).result()

       return [dict(row) for row in results]
   ```

4. **Update `/api/events/{id}/` GET endpoint:**
   - Add BigQueryService.get_event_by_id(id) method
   - Query: `SELECT * FROM events WHERE id = @id LIMIT 1`
   - Return single event dict or None

**Response format consistency:**
- BigQuery returns dicts, Django ORM returns model instances
- Ensure serialization matches existing API response format (check frontend expectations)
- May need Pydantic schemas or manual dict construction
  </action>
  <verify>GET /api/events/ returns events from BigQuery with all filters working, GET /api/events/{id}/ returns single event</verify>
  <done>Event endpoints querying BigQuery, filters working (event_type, severity, risk_score, dates), single event retrieval functional</done>
</task>

<task type="auto">
  <name>Task 2: Update entity trending endpoint</name>
  <files>backend/api/views.py, backend/api/services/bigquery_service.py</files>
  <action>
Update entity trending endpoint (Phase 6 Plan 03) to use BigQuery:

**Decision point:** Phase 6 uses Redis Sorted Sets for O(log N) trending with exponential time-decay. Options:

**Option A: Keep Redis trending cache**
- Pros: Fast, already working, no breaking changes
- Cons: Requires dual-write (BigQuery + Redis) from ingestion tasks
- Implementation: Update Phase 3 tasks to also write to Redis

**Option B: Migrate trending to BigQuery**
- Pros: Single source of truth, simpler architecture
- Cons: Slower than Redis (but acceptable for non-real-time dashboard)
- Implementation: Replace Redis queries with BigQuery time-decay query

**Recommendation: Option B (BigQuery trending)**
- Reason: Simplifies architecture, BigQuery can handle trending efficiently with partitioning
- Phase 6 trending queries run on page load (not real-time), sub-second latency acceptable

**backend/api/services/bigquery_service.py changes:**

Update `get_entity_trending()` method (from 14.1-01) to match Phase 6 pattern:

```python
def get_entity_trending(
    self,
    metric: str = 'mentions',  # mentions, risk, sanctions
    limit: int = 20,
    days: int = 30  # Trending window
) -> List[dict]:
    """
    Get trending entities using exponential time-decay.
    7-day half-life (168 hours) as in Phase 6.
    """

    if metric == 'mentions':
        # Time-decay weighted mention count
        query = f"""
            SELECT
                entity_id,
                SUM(EXP(-(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), mentioned_at, HOUR) / 168.0) * LN(2))) as score
            FROM `{self.project_id}.{self.dataset_id}.entity_mentions`
            WHERE mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
            GROUP BY entity_id
            ORDER BY score DESC
            LIMIT @limit
        """
    elif metric == 'risk':
        # Average risk score of events mentioning entity
        query = f"""
            SELECT
                em.entity_id,
                AVG(e.risk_score) as score
            FROM `{self.project_id}.{self.dataset_id}.entity_mentions` em
            JOIN `{self.project_id}.{self.dataset_id}.events` e ON em.event_id = e.id
            WHERE em.mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
            GROUP BY em.entity_id
            ORDER BY score DESC
            LIMIT @limit
        """
    elif metric == 'sanctions':
        # Count of sanctioned events mentioning entity
        query = f"""
            SELECT
                em.entity_id,
                COUNT(*) as score
            FROM `{self.project_id}.{self.dataset_id}.entity_mentions` em
            JOIN `{self.project_id}.{self.dataset_id}.events` e ON em.event_id = e.id
            WHERE em.mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
            AND e.event_type = 'sanctions'
            GROUP BY em.entity_id
            ORDER BY score DESC
            LIMIT @limit
        """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter('days', 'INT64', days),
            bigquery.ScalarQueryParameter('limit', 'INT64', limit)
        ]
    )

    results = self.client.query(query, job_config=job_config).result()
    return [{'entity_id': row.entity_id, 'score': float(row.score)} for row in results]
```

**backend/api/views.py changes:**

Update `/api/entities/trending/` endpoint:
- Replace Redis zadd/zrevrange calls with `bigquery_service.get_entity_trending(metric, limit)`
- Keep bulk Entity.objects.filter(id__in=entity_ids) for fetching entity metadata from PostgreSQL
- Return same response format as before (entity objects with score)

**Redis deprecation:**
- Remove Redis trending updates from Phase 3 ingestion tasks (if any exist)
- Keep Redis for other caching if used elsewhere
- Document decision in SUMMARY.md
  </action>
  <verify>GET /api/entities/trending/ returns entities from BigQuery, metric toggle works (mentions/risk/sanctions), scores match exponential decay formula</verify>
  <done>Entity trending endpoint querying BigQuery, Redis trending deprecated, time-decay formula preserved, metric toggle functional</done>
</task>

<task type="auto">
  <name>Task 3: Update entity profile and risk trends endpoints</name>
  <files>backend/api/views.py, backend/api/services/bigquery_service.py</files>
  <action>
Update entity profile and risk trends endpoints:

**backend/api/services/bigquery_service.py additions:**

1. **Add get_entity_events() method:**
   ```python
   def get_entity_events(
       self,
       entity_id: str,
       limit: int = 50,
       days: int = 90
   ) -> List[dict]:
       """Get recent events mentioning an entity."""
       query = f"""
           SELECT e.*
           FROM `{self.project_id}.{self.dataset_id}.events` e
           JOIN `{self.project_id}.{self.dataset_id}.entity_mentions` em ON e.id = em.event_id
           WHERE em.entity_id = @entity_id
           AND em.mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
           ORDER BY e.mentioned_at DESC
           LIMIT @limit
       """

       job_config = bigquery.QueryJobConfig(
           query_parameters=[
               bigquery.ScalarQueryParameter('entity_id', 'STRING', entity_id),
               bigquery.ScalarQueryParameter('days', 'INT64', days),
               bigquery.ScalarQueryParameter('limit', 'INT64', limit)
           ]
       )

       results = self.client.query(query, job_config=job_config).result()
       return [dict(row) for row in results]
   ```

2. **Add get_entity_stats() method:**
   ```python
   def get_entity_stats(self, entity_id: str, days: int = 90) -> dict:
       """Get aggregated stats for entity profile."""
       query = f"""
           SELECT
               COUNT(DISTINCT em.event_id) as total_mentions,
               COUNT(DISTINCT CASE WHEN e.event_type = 'sanctions' THEN e.id END) as sanctions_count,
               AVG(e.risk_score) as avg_risk_score,
               MAX(e.risk_score) as max_risk_score
           FROM `{self.project_id}.{self.dataset_id}.entity_mentions` em
           JOIN `{self.project_id}.{self.dataset_id}.events` e ON em.event_id = e.id
           WHERE em.entity_id = @entity_id
           AND em.mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @days DAY)
       """

       job_config = bigquery.QueryJobConfig(
           query_parameters=[
               bigquery.ScalarQueryParameter('entity_id', 'STRING', entity_id),
               bigquery.ScalarQueryParameter('days', 'INT64', days)
           ]
       )

       results = self.client.query(query, job_config=job_config).result()
       row = next(results, None)

       if row:
           return {
               'total_mentions': row.total_mentions,
               'sanctions_count': row.sanctions_count,
               'avg_risk_score': float(row.avg_risk_score) if row.avg_risk_score else 0.0,
               'max_risk_score': float(row.max_risk_score) if row.max_risk_score else 0.0
           }
       return {'total_mentions': 0, 'sanctions_count': 0, 'avg_risk_score': 0.0, 'max_risk_score': 0.0}
   ```

3. **Update get_risk_trends() method** (from 14.1-01):
   - Add bucket_size parameter validation (DAY, WEEK, MONTH)
   - Use TIMESTAMP_TRUNC instead of TIMESTAMP_BUCKET (if BigQuery version doesn't support it)
   - Return time-series data for chart rendering

**backend/api/views.py changes:**

1. **Update `/api/entities/{id}/profile/` endpoint:**
   - Replace `Event.objects.filter(entitymention__entity=entity)` with `bigquery_service.get_entity_events(entity.id)`
   - Replace aggregation queries with `bigquery_service.get_entity_stats(entity.id)`
   - Keep `Entity.objects.get(id=id)` for entity metadata (PostgreSQL)

2. **Update `/api/risk/trends/` endpoint:**
   - Replace Django ORM aggregation with `bigquery_service.get_risk_trends(start_date, end_date, bucket_size)`
   - Keep same response format for Recharts frontend

**Verification approach:**
- Test entity profile page loads with events and stats
- Test risk trends chart renders with time-series data
  </action>
  <verify>GET /api/entities/{id}/profile/ returns entity with BigQuery events/stats, GET /api/risk/trends/ returns time-series data for charts</verify>
  <done>Entity profile and risk trends endpoints querying BigQuery, aggregation stats working, chart data rendering correctly</done>
</task>

<task type="auto">
  <name>Task 4: Update AI chat tool functions</name>
  <files>backend/api/chat_tools.py (or backend/api/views.py if tools are inline)</files>
  <action>
Update Phase 7 AI chat tool functions to query BigQuery:

**Affected tools (from Phase 7 Plan 01):**
1. `search_events` - Search events by keyword, date range, filters
2. `get_entity_profile` - Get entity details with events and stats
3. `get_trending_entities` - Get trending entities with metric toggle
4. `analyze_risk_trends` - Get risk trends over time

**Changes:**

1. **search_events tool:**
   - Old: `Event.objects.filter(Q(title__icontains=query) | Q(content__icontains=query))`
   - New: Add BigQueryService.search_events(query, start_date, end_date, limit) method
   - BigQuery supports LIKE or REGEXP_CONTAINS for text search:
     ```python
     def search_events(self, query: str, start_date: datetime, end_date: datetime, limit: int = 20) -> List[dict]:
         bq_query = f"""
             SELECT *
             FROM `{self.project_id}.{self.dataset_id}.events`
             WHERE (title LIKE @query OR content LIKE @query)
             AND mentioned_at BETWEEN @start_date AND @end_date
             ORDER BY mentioned_at DESC
             LIMIT @limit
         """

         job_config = bigquery.QueryJobConfig(
             query_parameters=[
                 bigquery.ScalarQueryParameter('query', 'STRING', f'%{query}%'),
                 bigquery.ScalarQueryParameter('start_date', 'TIMESTAMP', start_date),
                 bigquery.ScalarQueryParameter('end_date', 'TIMESTAMP', end_date),
                 bigquery.ScalarQueryParameter('limit', 'INT64', limit)
             ]
         )

         results = self.client.query(bq_query, job_config=job_config).result()
         return [dict(row) for row in results]
     ```

2. **get_entity_profile tool:**
   - Already covered by Task 3 BigQueryService.get_entity_events() and get_entity_stats()
   - Update tool function to call these methods

3. **get_trending_entities tool:**
   - Already covered by Task 2 BigQueryService.get_entity_trending()
   - Update tool function to call this method

4. **analyze_risk_trends tool:**
   - Already covered by Task 3 BigQueryService.get_risk_trends()
   - Update tool function to call this method

**Tool function updates:**
- Replace all Django ORM queries with BigQueryService method calls
- Keep Entity fuzzy matching logic (still uses PostgreSQL Entity table)
- Ensure tool response format matches frontend expectations (chat tool cards)

**Verification approach:**
- Test AI chat with queries that trigger each tool
- Verify tool results render correctly in frontend chat interface
  </action>
  <verify>AI chat tools return BigQuery data, search_events works, entity/trending/risk tools functional, frontend tool cards render</verify>
  <done>AI chat tools querying BigQuery, all 4 tools updated, search functionality working, tool UI rendering correctly</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All event endpoints query BigQuery (list, detail, search)
- [ ] Entity trending uses BigQuery (Redis deprecated)
- [ ] Entity profile shows BigQuery events and stats
- [ ] Risk trends endpoint returns time-series data from BigQuery
- [ ] AI chat tools all query BigQuery successfully
- [ ] `python manage.py check` passes
- [ ] Manual test: Load dashboard, entities page, chat page - all data visible
- [ ] No Event.objects or EntityMention.objects queries for time-series data (only Entity.objects for reference data)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- API endpoints serving BigQuery data
- Entity trending migrated from Redis to BigQuery
- AI chat tools querying BigQuery
- Response formats compatible with existing frontend
- No errors or warnings introduced
- Ready for data migration and validation (14.1-04)
</success_criteria>

<output>
After completion, create `.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-03-SUMMARY.md`:

# Phase 14.1 Plan 03: API Views Migration Summary

**API layer migrated: All endpoints serving BigQuery data, Redis trending deprecated, chat tools integrated.**

## Accomplishments

- Migrated 6 API endpoints to query BigQuery instead of PostgreSQL
- Added 5 BigQuery service methods for views (get_event_by_id, get_entity_events, get_entity_stats, search_events, updated get_recent_events with filters)
- Deprecated Redis Sorted Sets trending in favor of BigQuery exponential time-decay queries
- Updated all 4 AI chat tools to query BigQuery (search_events, get_entity_profile, get_trending_entities, analyze_risk_trends)
- Preserved response formats for frontend compatibility

## Files Created/Modified

- `backend/api/views.py` - Updated event, entity, risk endpoints to use BigQueryService
- `backend/api/services/bigquery_service.py` - Added 5 query methods with filters and aggregations
- `backend/api/chat_tools.py` - Updated AI chat tools for BigQuery (if separate file)

## Decisions Made

- **Redis trending deprecated** - BigQuery handles trending efficiently with partitioning, simpler architecture
- Time-decay formula preserved (7-day half-life) in BigQuery SQL
- Entity metadata still in PostgreSQL (reference data, not time-series)
- Text search uses BigQuery LIKE (not full-text search) for simplicity
- Response serialization as dicts (not Django model instances) for BigQuery results

## Issues Encountered

None

## Next Step

Ready for **14.1-04-PLAN.md**: Data migration and validation (backfill BigQuery from PostgreSQL, deprecate PostgreSQL Event/EntityMention models)
</output>

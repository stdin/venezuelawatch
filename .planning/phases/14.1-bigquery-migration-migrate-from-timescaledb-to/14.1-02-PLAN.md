---
phase: 14.1-bigquery-migration
plan: 02
type: execute
---

<objective>
Update Celery ingestion tasks from Phase 3 to write time-series data to BigQuery instead of PostgreSQL.

Purpose: Migrate data pipeline to polyglot persistence - Django ORM remains for User/Entity models, BigQuery for Event/EntityMention/Economic time-series.

Output: Phase 3 ingestion tasks (GDELT, ReliefWeb, FRED, UN Comtrade, World Bank) writing to BigQuery, PostgreSQL Event model deprecated but not deleted (for rollback safety).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/TIMESCALEDB-MIGRATION.md
@.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-01-PLAN.md

**Migration strategy:** BigQuery service layer created in 14.1-01. Now update ingestion tasks to use it.

**Affected tasks (backend/api/tasks.py):**
- `fetch_gdelt_events()` - 15-minute polling, writes Event objects
- `fetch_reliefweb_reports()` - daily polling, writes Event objects
- `fetch_fred_series()` - daily batch, writes economic data
- `ingest_un_comtrade()` - monthly batch, writes trade data
- `ingest_world_bank()` - quarterly batch, writes World Bank indicators

**Constraining decisions:**
- Phase 1: Event model with JSONField metadata, TimescaleDB hypertables (now deprecated)
- Phase 3: Celery tasks with tenacity retry, rate limiting, deduplication
- Phase 6: EntityMention model for entity tracking (also needs BigQuery migration)
- 14.1-01: BigQueryService with insert_events(), insert_entity_mentions() methods

**Strategy:**
1. Update ingestion tasks to use BigQuery dataclasses and service
2. Keep PostgreSQL Event/EntityMention models for now (rollback safety, reference data)
3. Add dual-write if needed for transition period (decision in plan)
4. Update deduplication logic for BigQuery queries
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update GDELT and ReliefWeb ingestion for BigQuery</name>
  <files>backend/api/tasks.py</files>
  <action>
Update real-time ingestion tasks (Phase 3 Plan 02) to write to BigQuery:

**backend/api/tasks.py changes:**

1. **Add BigQuery imports at top:**
   ```python
   from api.bigquery_models import Event as BigQueryEvent
   from api.services.bigquery_service import bigquery_service
   ```

2. **Update `fetch_gdelt_events()` task:**
   - Keep existing GDELT API fetching logic and deduplication check
   - Change: Instead of creating Django `Event.objects.create()`, create `BigQueryEvent` dataclass instances
   - Convert fields: `mentioned_at` from GDELT `sqldate` to datetime, `metadata` dict for JSONB field
   - Call `bigquery_service.insert_events(bigquery_events)` with list of BigQueryEvent objects
   - Keep deduplication: Query BigQuery instead of PostgreSQL
     ```python
     # Old: Event.objects.filter(source_url=url).exists()
     # New: Query BigQuery for existing source_url in last 7 days
     query = f"""
       SELECT COUNT(*) as count
       FROM `{settings.GCP_PROJECT_ID}.{settings.BIGQUERY_DATASET}.events`
       WHERE source_url = @url
       AND mentioned_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
     """
     # Use parameterized query to prevent SQL injection
     ```

3. **Update `fetch_reliefweb_reports()` task:**
   - Same pattern: BigQueryEvent dataclass instead of Django ORM
   - Convert ReliefWeb API response fields to BigQueryEvent schema
   - Call `bigquery_service.insert_events()` for batch insert
   - Update deduplication query for BigQuery

**Don't change:**
- Tenacity retry decorators
- Rate limiting logic
- API request patterns
- Error handling

**Verification approach:**
- Run task manually with `.delay()` and check BigQuery console for new rows
- Verify deduplication prevents duplicate inserts
  </action>
  <verify>Celery task runs without errors, bq query shows new events in BigQuery, no duplicate source_urls</verify>
  <done>GDELT and ReliefWeb tasks writing to BigQuery, deduplication working, no ORM Event.objects.create() calls remaining</done>
</task>

<task type="auto">
  <name>Task 2: Update FRED economic data ingestion for BigQuery</name>
  <files>backend/api/tasks.py</files>
  <action>
Update daily FRED ingestion (Phase 3 Plan 03) to write to BigQuery:

**backend/api/tasks.py changes:**

1. **Add FREDIndicator import:**
   ```python
   from api.bigquery_models import FREDIndicator
   ```

2. **Update `fetch_fred_series()` task:**
   - Keep existing FRED API fetching with parallel Celery group pattern
   - Change: Create `FREDIndicator` dataclass instances instead of storing in Event model
   - Fields: `series_id`, `date`, `value`, `series_name`, `units`
   - Call `bigquery_service.insert_fred_indicators(indicators)` method
   - **Wait - need to add this method to BigQueryService!** Add to Task 3.

3. **Update threshold-based event generation:**
   - Keep logic for oil price > 10%, inflation > 5% threshold detection
   - When threshold crossed, create BigQueryEvent for the economic event
   - Call `bigquery_service.insert_events([event])` for threshold alerts

**Pattern:**
```python
# Old: Store FRED data in Event.metadata['fred_data']
# New: Store in dedicated fred_indicators table
fred_indicators = []
for observation in response['observations']:
    fred_indicators.append(FREDIndicator(
        series_id=series_id,
        date=datetime.strptime(observation['date'], '%Y-%m-%d').date(),
        value=float(observation['value']),
        series_name=series_name,
        units=units
    ))

bigquery_service.insert_fred_indicators(fred_indicators)
```
  </action>
  <verify>FRED task runs, bq query shows fred_indicators rows, threshold events appear in events table</verify>
  <done>FRED ingestion writing to BigQuery fred_indicators table, threshold alerts working</done>
</task>

<task type="auto">
  <name>Task 3: Update UN Comtrade and World Bank ingestion + add missing BigQuery methods</name>
  <files>backend/api/tasks.py, backend/api/services/bigquery_service.py, backend/api/bigquery_models.py</files>
  <action>
Update monthly/quarterly ingestion (Phase 3 Plan 04) and add missing BigQuery service methods:

**backend/api/bigquery_models.py additions:**

1. **Add UNComtrade dataclass:**
   ```python
   @dataclass
   class UNComtrade:
       period: date
       reporter_code: str
       commodity_code: str
       trade_flow: str
       value_usd: float

       def to_bigquery_row(self) -> dict:
           return {
               'period': self.period.isoformat(),
               'reporter_code': self.reporter_code,
               'commodity_code': self.commodity_code,
               'trade_flow': self.trade_flow,
               'value_usd': self.value_usd
           }
   ```

2. **Add WorldBank dataclass:**
   ```python
   @dataclass
   class WorldBank:
       indicator_id: str
       date: date
       value: float
       country_code: str

       def to_bigquery_row(self) -> dict:
           return {
               'indicator_id': self.indicator_id,
               'date': self.date.isoformat(),
               'value': self.value,
               'country_code': self.country_code
           }
   ```

**backend/api/services/bigquery_service.py additions:**

Add insert methods for all economic data types:

```python
def insert_fred_indicators(self, indicators: List[FREDIndicator]):
    if not indicators:
        return
    table_id = f"{self.dataset_id}.fred_indicators"
    rows = [ind.to_bigquery_row() for ind in indicators]
    errors = self.client.insert_rows_json(table_id, rows)
    if errors:
        raise Exception(f"BigQuery insert errors: {errors}")

def insert_un_comtrade(self, records: List[UNComtrade]):
    if not records:
        return
    table_id = f"{self.dataset_id}.un_comtrade"
    rows = [rec.to_bigquery_row() for rec in records]
    errors = self.client.insert_rows_json(table_id, rows)
    if errors:
        raise Exception(f"BigQuery insert errors: {errors}")

def insert_world_bank(self, indicators: List[WorldBank]):
    if not indicators:
        return
    table_id = f"{self.dataset_id}.world_bank"
    rows = [ind.to_bigquery_row() for ind in indicators]
    errors = self.client.insert_rows_json(table_id, rows)
    if errors:
        raise Exception(f"BigQuery insert errors: {errors}")
```

**backend/api/tasks.py changes:**

1. **Update `ingest_un_comtrade()` task:**
   - Import `UNComtrade` dataclass
   - Create UNComtrade instances from API response
   - Call `bigquery_service.insert_un_comtrade(records)`

2. **Update `ingest_world_bank()` task:**
   - Import `WorldBank` dataclass
   - Create WorldBank instances from API response
   - Call `bigquery_service.insert_world_bank(indicators)`

**Pattern for both:**
- Keep existing API request logic, retry, rate limiting
- Change data storage from Event model to dedicated BigQuery tables
- Remove Event.objects.create() calls
  </action>
  <verify>UN Comtrade and World Bank tasks run, bq query shows data in respective tables, no import errors</verify>
  <done>All Phase 3 ingestion tasks writing to BigQuery, economic data in dedicated tables, BigQuery service methods added</done>
</task>

<task type="auto">
  <name>Task 4: Update EntityMention creation in Phase 6 extraction task</name>
  <files>backend/api/tasks.py</files>
  <action>
Update Phase 6 entity extraction task to write EntityMention to BigQuery:

**backend/api/tasks.py changes:**

1. **Add EntityMention import:**
   ```python
   from api.bigquery_models import EntityMention as BigQueryEntityMention
   ```

2. **Locate `extract_entities_from_event()` task** (from Phase 6 Plan 02)
   - This task creates EntityMention objects linking entities to events
   - Currently uses Django ORM: `EntityMention.objects.create(entity=entity, event=event, mentioned_at=event.mentioned_at, context=context)`

3. **Update to BigQuery pattern:**
   ```python
   # Create BigQueryEntityMention dataclass instances
   mentions = []
   for entity, context in extracted_entities:
       mentions.append(BigQueryEntityMention(
           id=str(uuid.uuid4()),  # Generate UUID
           entity_id=str(entity.id),  # Entity still in PostgreSQL
           event_id=event_id,  # Event ID from BigQuery insert
           mentioned_at=mentioned_at,
           context=context
       ))

   # Batch insert
   if mentions:
       bigquery_service.insert_entity_mentions(mentions)
   ```

4. **Challenge: Event ID coordination**
   - When inserting Event to BigQuery, need to capture generated UUID to use in EntityMention
   - Solution: Generate UUID before insert in Event creation, pass to extraction task
   - Update GDELT/ReliefWeb tasks to generate UUIDs and pass to extraction task via Celery chain

**Verification approach:**
- Run entity extraction task manually
- Check entity_mentions table in BigQuery for new rows
- Verify entity_id references exist in PostgreSQL Entity table
  </action>
  <verify>Entity extraction task runs, entity_mentions table has rows, entity_id/event_id relationships intact</verify>
  <done>EntityMention creation writing to BigQuery, entity extraction integrated with BigQuery Event IDs</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All Phase 3 ingestion tasks updated (GDELT, ReliefWeb, FRED, UN Comtrade, World Bank)
- [ ] BigQuery service methods added for all data types (fred_indicators, un_comtrade, world_bank)
- [ ] BigQuery dataclasses created for UNComtrade and WorldBank
- [ ] Phase 6 entity extraction task updated to write EntityMention to BigQuery
- [ ] `python manage.py check` passes
- [ ] Run one task manually: `python manage.py shell` → `from api.tasks import fetch_gdelt_events; fetch_gdelt_events.delay()` → check BigQuery for rows
- [ ] No Event.objects.create() or EntityMention.objects.create() calls in tasks.py for time-series data
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Phase 3 ingestion tasks writing to BigQuery
- Phase 6 entity extraction writing to BigQuery
- Economic data in dedicated BigQuery tables (not Event.metadata)
- Deduplication logic updated for BigQuery queries
- No errors or warnings introduced
- Ready for API views migration (14.1-03)
</success_criteria>

<output>
After completion, create `.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-02-SUMMARY.md`:

# Phase 14.1 Plan 02: Celery Ingestion Migration Summary

**Data pipeline migrated: All Phase 3 ingestion tasks writing to BigQuery, entity extraction integrated.**

## Accomplishments

- Updated 5 Celery ingestion tasks to write to BigQuery instead of PostgreSQL
- Added BigQuery service methods for economic data (fred_indicators, un_comtrade, world_bank)
- Created UNComtrade and WorldBank dataclass models
- Migrated Phase 6 entity extraction to write EntityMention to BigQuery
- Updated deduplication logic for BigQuery queries
- Economic data now in dedicated tables (not Event.metadata)

## Files Created/Modified

- `backend/api/tasks.py` - Updated all ingestion tasks for BigQuery
- `backend/api/services/bigquery_service.py` - Added insert_fred_indicators, insert_un_comtrade, insert_world_bank methods
- `backend/api/bigquery_models.py` - Added UNComtrade and WorldBank dataclasses

## Decisions Made

- Generate Event UUIDs before BigQuery insert (not after) for EntityMention coordination
- Keep parameterized queries for deduplication (prevent SQL injection)
- Batch insert pattern for all ingestion tasks (streaming inserts)
- Economic data in dedicated tables, not Event.metadata (better schema)

## Issues Encountered

None

## Next Step

Ready for **14.1-03-PLAN.md**: API views migration (update Phase 4/6/7 endpoints to query BigQuery)
</output>

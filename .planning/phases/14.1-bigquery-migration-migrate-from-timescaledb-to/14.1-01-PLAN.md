---
phase: 14.1-bigquery-migration
plan: 01
type: execute
---

<objective>
Create BigQuery infrastructure and service layer for time-series data migration from TimescaleDB.

Purpose: Replace TimescaleDB (not available on Cloud SQL) with BigQuery polyglot persistence architecture - PostgreSQL for transactional data, BigQuery for time-series analytics.

Output: BigQuery dataset with partitioned tables, Python service layer for time-series queries, configuration integrated with Django settings.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/TIMESCALEDB-MIGRATION.md

**Migration strategy:** Polyglot persistence - PostgreSQL for users/entities (Django ORM), BigQuery for events/mentions/economic data (Python client).

**Key constraint:** TimescaleDB not available on Cloud SQL PostgreSQL. Must migrate before production deployment.

**Already using BigQuery:** Phase 14 forecasting uses BigQuery for ETL. This consolidates infrastructure.

**Tech stack available:**
- Django 5.2 + django-ninja API
- Celery + Redis for async tasks
- GCP Cloud SQL PostgreSQL (no TimescaleDB)
- GCP Secret Manager for credentials
- Python 3.11+

**Constraining decisions:**
- Phase 1: GCP Cloud SQL, Secret Manager, Cloud Storage
- Phase 14: BigQuery for forecasting (already set up)
- Ingestion is batch/polling (not high-frequency transactional)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BigQuery dataset and tables with partitioning</name>
  <files>backend/config/bigquery_schema.sql</files>
  <action>
Create SQL schema file for BigQuery tables following migration strategy:

**Tables:**
1. **events** - Event time-series (from Phase 1/3/4 Event model)
   - Fields: id (STRING), title, content, source_url, source_name, event_type, location, risk_score (FLOAT64), severity (STRING), mentioned_at (TIMESTAMP), created_at (TIMESTAMP), metadata (JSON)
   - PARTITION BY DATE(mentioned_at) - time-based partitioning for query performance
   - CLUSTER BY event_type, source_name - co-locate related data

2. **entity_mentions** - EntityMention time-series (from Phase 6)
   - Fields: id (STRING), entity_id (STRING), event_id (STRING), mentioned_at (TIMESTAMP), context (STRING)
   - PARTITION BY DATE(mentioned_at)
   - CLUSTER BY entity_id

3. **fred_indicators** - FRED economic data (from Phase 3)
   - Fields: series_id (STRING), date (DATE), value (FLOAT64), series_name (STRING), units (STRING)
   - PARTITION BY date
   - CLUSTER BY series_id

4. **un_comtrade** - UN Comtrade trade data (from Phase 3)
   - Fields: period (DATE), reporter_code (STRING), commodity_code (STRING), trade_flow (STRING), value_usd (FLOAT64)
   - PARTITION BY period
   - CLUSTER BY commodity_code

5. **world_bank** - World Bank indicators (from Phase 3)
   - Fields: indicator_id (STRING), date (DATE), value (FLOAT64), country_code (STRING)
   - PARTITION BY date
   - CLUSTER BY indicator_id

Use STRING for IDs (BigQuery doesn't have AUTO_INCREMENT), UUID generation in application.

Run: `bq mk --dataset --location=US <project-id>:venezuelawatch_analytics` then execute SQL.
  </action>
  <verify>bq ls shows venezuelawatch_analytics dataset, bq show shows tables with partition/cluster config</verify>
  <done>BigQuery dataset exists with 5 partitioned tables, partitioning confirmed via bq show output</done>
</task>

<task type="auto">
  <name>Task 2: Create BigQuery service layer</name>
  <files>backend/api/services/bigquery_service.py, backend/api/bigquery_models.py</files>
  <action>
Create BigQuery service layer following migration doc Phase 2:

**backend/api/bigquery_models.py:**
- Python dataclasses (NOT Django models) for Event, EntityMention, FREDIndicator, UNComtrade, WorldBank
- Each has `to_bigquery_row()` method converting to dict for BigQuery insert format
- Generate UUID if id not provided
- Handle datetime to ISO format conversion

**backend/api/services/bigquery_service.py:**
- `BigQueryService` class with `__init__(self)` loading GCP_PROJECT_ID and BIGQUERY_DATASET from Django settings
- `insert_events(events: List[Event])` - streaming insert using `client.insert_rows_json()`
- `insert_entity_mentions(mentions: List[EntityMention])` - same pattern
- `get_recent_events(start_date, end_date, event_type=None, limit=100)` - parameterized query with TIMESTAMP_BETWEEN filtering
- `get_entity_trending(metric='mentions', limit=20)` - time-decay trending query (7-day half-life exponential decay from Phase 6 pattern)
- `get_risk_trends(start_date, end_date, bucket_size='DAY')` - TIMESTAMP_BUCKET aggregation
- Singleton instance: `bigquery_service = BigQueryService()` at bottom

Use `google-cloud-bigquery` client library. Add error handling for insert failures. Use parameterized queries (QueryJobConfig) to prevent SQL injection.

Install: `pip install google-cloud-bigquery` and add to backend/requirements.txt
  </action>
  <verify>python -m api.services.bigquery_service (no import errors), pytest backend/tests/test_bigquery_service.py passes if tests exist</verify>
  <done>BigQueryService class created with insert/query methods, dataclasses defined, singleton exported, no import errors</done>
</task>

<task type="auto">
  <name>Task 3: Update Django settings for BigQuery configuration</name>
  <files>backend/config/settings.py, backend/config/.env.example</files>
  <action>
Add BigQuery configuration to Django settings:

```python
# BigQuery Configuration
GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID', 'venezuelawatch-dev')
BIGQUERY_DATASET = os.getenv('BIGQUERY_DATASET', 'venezuelawatch_analytics')
```

Add to .env.example:
```
GCP_PROJECT_ID=venezuelawatch-dev
BIGQUERY_DATASET=venezuelawatch_analytics
```

Don't set GOOGLE_APPLICATION_CREDENTIALS in settings - rely on GCP default application credentials (ADC) which works both locally (via gcloud auth application-default login) and in production (service account).

Document in backend/README.md that local development requires: `gcloud auth application-default login`
  </action>
  <verify>python manage.py check shows no errors, env vars accessible in settings</verify>
  <done>BigQuery settings added, .env.example updated, no Django check errors</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `bq ls` shows venezuelawatch_analytics dataset
- [ ] `bq show venezuelawatch_analytics.events` shows PARTITION BY DATE(mentioned_at)
- [ ] `python -c "from api.services.bigquery_service import bigquery_service"` succeeds
- [ ] `python manage.py check` passes
- [ ] google-cloud-bigquery in requirements.txt
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- BigQuery dataset and tables exist with partitioning
- BigQuery service layer created and importable
- Django settings configured for BigQuery
- No errors or warnings introduced
- Ready for Celery ingestion migration (14.1-02)
</success_criteria>

<output>
After completion, create `.planning/phases/14.1-bigquery-migration-migrate-from-timescaledb-to/14.1-01-SUMMARY.md`:

# Phase 14.1 Plan 01: BigQuery Schema & Service Setup Summary

**BigQuery infrastructure created: dataset, partitioned tables, Python service layer ready for time-series analytics.**

## Accomplishments

- BigQuery dataset `venezuelawatch_analytics` created with 5 tables
- All tables partitioned by date for query performance (events, entity_mentions on mentioned_at, economic data on date)
- Clustering configured for efficient scans (event_type, entity_id, series_id)
- BigQuery service layer with insert/query methods for events, mentions, trending
- Dataclass models for type-safe BigQuery row generation
- Django settings integrated with GCP_PROJECT_ID and BIGQUERY_DATASET

## Files Created/Modified

- `backend/config/bigquery_schema.sql` - Table definitions with partitioning
- `backend/api/services/bigquery_service.py` - BigQuery service layer
- `backend/api/bigquery_models.py` - Dataclass models for BigQuery rows
- `backend/config/settings.py` - BigQuery configuration
- `backend/config/.env.example` - BigQuery env vars
- `backend/requirements.txt` - Added google-cloud-bigquery

## Decisions Made

- STRING for IDs (BigQuery doesn't have AUTO_INCREMENT, use UUIDs in application)
- Streaming inserts for batch ingestion (not batch load jobs) - simpler for polling-based ingestion
- Singleton bigquery_service instance for convenience
- Rely on GCP ADC for authentication (not explicit credentials file)

## Issues Encountered

None

## Next Step

Ready for **14.1-02-PLAN.md**: Celery ingestion migration (update Phase 3 tasks to write to BigQuery)
</output>

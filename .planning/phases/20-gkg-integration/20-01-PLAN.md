---
phase: 20-gkg-integration
plan: 01
type: execute
---

<objective>
Create GKG BigQuery service and integrate GKG data fetching into the Venezuela event sync pipeline.

Purpose: Enable access to GDELT Global Knowledge Graph's rich entity, theme, and sentiment data for Venezuela events.
Output: GKG service operational, sync task fetching GKG data alongside event records.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-gkg-integration/DISCOVERY.md
@.planning/phases/19-gdelt-events-enrichment/19-01-SUMMARY.md
@backend/api/services/gdelt_bigquery_service.py
@backend/data_pipeline/tasks/gdelt_sync_task.py

**Tech stack available:**
- Google Cloud BigQuery client
- Existing GDELT BigQuery service pattern
- Parameterized BigQuery queries
- Partition-filtered queries for performance

**Established patterns:**
- gdelt_bigquery_service.py for BigQuery queries
- Snake_case metadata keys in sync tasks
- Partition filtering on _PARTITIONTIME
- GCP Application Default Credentials

**Constraining decisions:**
- Phase 14.1: Parameterized queries in BigQuery to prevent SQL injection
- Phase 14.1: TIME partitioning filtering mandatory for query performance
- Phase 19: Snake_case metadata keys for consistency
- Phase 20 Discovery: DocumentIdentifier-based lookup (not 3-table join)
- Phase 20 Discovery: V2 fields preferred (V2Themes, V2Persons, V2Organizations, V2Locations, V2Tone)

**From DISCOVERY.md:**
- GKG table is 19.5 TB (55x larger than Events)
- Join via DocumentIdentifier = SOURCEURL
- GKG fields are STRING with delimited lists (require parsing)
- Query efficiency critical - always use partition filtering
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GKG BigQuery Service</name>
  <files>backend/api/services/gdelt_gkg_service.py</files>
  <action>
Create new GDELTGKGService class following gdelt_bigquery_service.py pattern. Include method get_gkg_by_document_id(document_id, date_partition) that queries gdelt-bq.gdeltv2.gkg_partitioned filtered by DocumentIdentifier and _PARTITIONTIME.

Select V2 fields only: GKGRECORDID, DATE, DocumentIdentifier, SourceCommonName, V2Themes, V2Persons, V2Organizations, V2Locations, V2Tone, Quotations, GCAM, AllNames.

Use parameterized queries with @document_id and @partition_date parameters. Return raw GKG record dict (parsing in next plan).

Handle case where no GKG record exists for a given DocumentIdentifier (return None, not error - many events won't have matching GKG records).
  </action>
  <verify>
python -c "from api.services.gdelt_gkg_service import GDELTGKGService; svc=GDELTGKGService(); print('Service created')" succeeds
  </verify>
  <done>GDELTGKGService class exists with get_gkg_by_document_id method, uses parameterized queries with partition filtering, returns dict or None</done>
</task>

<task type="auto">
  <name>Task 2: Integrate GKG Fetch in Sync Task</name>
  <files>backend/data_pipeline/tasks/gdelt_sync_task.py</files>
  <action>
In sync_gdelt_events function, after fetching Events from BigQuery but before creating Event records:

1. Import GDELTGKGService
2. For each fetched event dict, if SOURCEURL exists:
   - Instantiate GKG service
   - Call get_gkg_by_document_id(event['SOURCEURL'], event date for partition)
   - If GKG record found, store raw GKG data in event dict as 'gkg_raw' key
3. When building metadata dict for BigQuery insert, if 'gkg_raw' exists, add it under 'gkg_data' key (unparsed, will parse in Plan 02)

Do NOT parse GKG fields yet - store raw strings for now. Parsing logic comes in next plan.

Handle missing GKG records gracefully (majority of events won't have GKG data - this is expected). Log count of events with/without GKG for observability.
  </action>
  <verify>
Run sync_gdelt_events(lookback_minutes=60) and check logs show "X events with GKG data, Y without" message, no errors about missing GKG fields
  </verify>
  <done>Sync task fetches GKG data via DocumentIdentifier, stores raw GKG in metadata.gkg_data, logs GKG availability counts, handles missing GKG gracefully</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] python -c "from api.services.gdelt_gkg_service import GDELTGKGService" succeeds
- [ ] GDELTGKGService.get_gkg_by_document_id method exists and uses parameterized queries
- [ ] Sync task imports and uses GKG service
- [ ] Test sync completes without errors and logs GKG fetch counts
- [ ] BigQuery events table shows some records with metadata.gkg_data populated (raw strings)
</verification>

<success_criteria>

- GDELTGKGService class created with BigQuery integration
- get_gkg_by_document_id method queries by DocumentIdentifier with partition filtering
- Sync task fetches GKG data for events with SOURCEURL
- Raw GKG data stored in metadata.gkg_data field
- Observability: logs show GKG availability metrics
- No errors introduced, handles missing GKG gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/20-gkg-integration/20-01-SUMMARY.md`:

# Phase 20 Plan 01: GKG Service & Data Fetching Summary

**[Substantive one-liner describing what shipped]**

## Accomplishments

- Created GDELTGKGService for querying GKG table by DocumentIdentifier
- Integrated GKG fetching into Venezuela event sync pipeline
- Raw GKG data now captured in event metadata for enrichment

## Files Created/Modified

- `backend/api/services/gdelt_gkg_service.py` - New GKG BigQuery service
- `backend/data_pipeline/tasks/gdelt_sync_task.py` - Added GKG fetch logic

## Decisions Made

- Store raw GKG strings in metadata.gkg_data (parsing deferred to Plan 02)
- Missing GKG records logged but not errors (expected for many events)
- Query GKG by DocumentIdentifier, not 3-table join (performance)

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 20-02-PLAN.md: Parse GKG delimited fields and enhance entity extraction
</output>

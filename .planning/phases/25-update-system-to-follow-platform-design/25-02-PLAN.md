---
phase: 25-update-system-to-follow-platform-design
plan: 02
type: execute
---

<objective>
Replace SEV1-5 severity with P1-P4 system and implement composite 5-component risk scoring with category sub-scores.

Purpose: Align severity classification with industry standard (P1=Critical → P4=Low) and implement explainable risk scoring that combines magnitude, tone, velocity, attention, and persistence factors with category-specific sub-scores.
Output: P1-P4 severity classifier with auto-triggers, composite risk scorer with 5 components, 10 category sub-scores calculation
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-update-system-to-follow-platform-design/25-CONTEXT.md
@docs/venezuela_risk_platform_design.md
@.planning/phases/25-update-system-to-follow-platform-design/25-01-SUMMARY.md
@backend/data_pipeline/services/impact_classifier.py
@backend/data_pipeline/services/risk_scorer.py

**Platform Design Requirements:**
- P1-P4 severity system (section 6): P1=CRITICAL (10+ fatalities, coup, nationalization, oil disruption), P2=HIGH (1-9 fatalities, major policy shift, >10% economic swing), P3=MODERATE (0.3-0.7 magnitude_norm, protests without violence), P4=LOW (informational, <0.3 magnitude_norm)
- Auto-trigger patterns: event types (COUP, NATIONALIZATION, etc.), CAMEO codes (192-195, 1031), keywords (regex patterns), fatality threshold (10+)
- Composite risk score (section 7.1): 5 components with weights — magnitude (30%), tone (20%), velocity (20%), attention (15%), persistence (15%), multiplied by confidence modifier (0.5-1.0 based on source diversity, credibility, corroboration)
- Severity floor: P1 events minimum score 70, P2 minimum 50
- Category sub-scores (section 7.2): For each of 10 categories, weighted average by severity (P1 weight=4, P2=3, P3=2, P4=1) with event count boost (up to 20% for 10+ events)
- Daily composite (section 7.3): Weighted sum of category sub-scores with commodity trader weights (political 15%, conflict 12%, economic 15%, trade 12%, regulatory 12%, infrastructure 8%, healthcare 5%, social 6%, environmental 5%, energy 10%), boosted by P1 count

**Current Implementation:**
- ImpactClassifier with SEV1-5 strings using LLM + weighted criteria (scope, duration, reversibility, economic_impact)
- RiskScorer with basic keyword/sentiment scoring, outputs 0-100
- No velocity, attention, persistence components
- No category sub-scores
- No P1/P2 severity floors

**Gap:**
- Need to replace SEV1-5 → P1-P4
- Need auto-trigger detection (not LLM-based for P1)
- Need 5-component composite scoring
- Need category sub-scores and daily composite

**Vision (from 25-CONTEXT.md):**
- Venezuela-tuned weights: ENERGY 15-20% (up from 10%), REGULATORY 15% (up from 12%), CONFLICT severity lower bar (adjust thresholds for normalized violence)
- LLM hybrid approach: Deterministic rules classify P1-P4, then LLM validates to catch false positives
- Sanctions keywords auto-trigger P1 (critical for Venezuela intelligence)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement P1-P4 severity classifier with auto-triggers</name>
  <files>backend/data_pipeline/services/severity_classifier.py</files>
  <action>
  Create SeverityClassifier implementing section 6 logic from design doc:

  1. Define P1_AUTO_TRIGGERS dict:
     - event_types: ['COUP', 'COUP_ATTEMPT', 'NATIONALIZATION', 'EXPROPRIATION', 'SOVEREIGN_DEFAULT', 'MILITARY_INTERVENTION', 'HEAD_OF_STATE_REMOVED', 'OIL_EXPORT_HALT']
     - cameo_codes: ['192', '193', '194', '195', '1031'] (GDELT specific)
     - keywords: Regex patterns for coup, nationali[sz]e, expropriate, sanctions announced/imposed, oil export halt, pdvsa seize/shutdown
     - fatality_threshold: 10

  2. Implement assign_severity(event: Event) → (severity: str, reason: str):
     - P1 logic:
       * Check event.event_type in auto_triggers.event_types → return ("P1", f"Auto-trigger: {event_type}")
       * Check event.metadata.cameo_code in auto_triggers.cameo_codes → return ("P1", f"Auto-trigger: CAMEO {code}")
       * Check regex patterns in event.title + event.content → return ("P1", f"Auto-trigger: {pattern}")
       * Check event.magnitude_unit == "fatalities" AND event.magnitude_raw >= 10 → return ("P1", f"High fatalities: {count}")
       * Check event.category == "ENERGY" AND "OIL" in event.commodities AND event.direction == "NEGATIVE" AND event.magnitude_norm > 0.8 → return ("P1", "Major oil/energy disruption")

     - P2 logic:
       * 1-9 fatalities: magnitude_unit == "fatalities" AND 1 <= magnitude_raw < 10
       * Major policy: category in ["POLITICAL", "REGULATORY"] AND magnitude_norm > 0.7 AND direction == "NEGATIVE"
       * Economic shock: category == "ECONOMIC" AND magnitude_unit == "percent_change" AND abs(magnitude_raw) > 10
       * Regional violence: category == "CONFLICT" AND magnitude_norm > 0.5 AND admin1 is not None

     - P3 logic:
       * Moderate impact: direction == "NEGATIVE" AND 0.3 < magnitude_norm <= 0.7
       * Protests: event_type in ["Protests", "PROTEST"] AND magnitude_raw == 0
       * Minor regulatory: category == "REGULATORY" AND magnitude_norm <= 0.5

     - P4 logic (default):
       * Everything else: "Low impact / informational"

  3. Use exact thresholds and logic from design doc section 6.2 assign_severity() function

  NOTE: This replaces ImpactClassifier's LLM-based SEV1-5 system. Auto-triggers are deterministic (not LLM) for P1 reliability.
  </action>
  <verify>
  python -c "from data_pipeline.services.severity_classifier import SeverityClassifier; from api.bigquery_models import Event; from datetime import datetime; e = Event(source_url='test', event_timestamp=datetime.now(), created_at=datetime.now(), category='CONFLICT', magnitude_raw=12, magnitude_unit='fatalities', magnitude_norm=0.9, direction='NEGATIVE', confidence=0.8); sev, reason = SeverityClassifier.assign_severity(e); assert sev == 'P1'; assert 'fatalities' in reason.lower()"
  </verify>
  <done>SeverityClassifier.assign_severity() returns P1-P4 with reasons, auto-triggers detect coup/nationalization/10+ fatalities/oil disruption, thresholds match design doc section 6, no LLM calls for deterministic P1 classification</done>
</task>

<task type="auto">
  <name>Task 2: Implement composite 5-component risk scoring</name>
  <files>backend/data_pipeline/services/composite_risk_scorer.py</files>
  <action>
  Create CompositeRiskScorer implementing section 7.1 calculate_risk_score() from design doc:

  1. Implement calculate_risk_score(event: Event, rolling_stats: dict) → dict:

     Component scores (each 0-1):
     - magnitude_norm: Use event.magnitude_norm (already 0-1 from normalizers)
     - tone_norm: Use event.tone_norm (already 0-1 from normalizers)
     - velocity_norm: Compare current magnitude to rolling average:
       * category_avg = rolling_stats.get(f"{event.category}_avg", 0.5)
       * category_std = rolling_stats.get(f"{event.category}_std", 0.2)
       * z_score = (event.magnitude_norm - category_avg) / category_std if category_std > 0 else 0
       * velocity_norm = sigmoid(z_score, k=1.0) where sigmoid(x, k) = 1 / (1 + exp(-k*x))
     - attention_norm: min(event.num_sources / 10, 1.0)
     - persistence_norm: min(event.metadata.get('persistence_days', 1) / 7, 1.0) — 7+ consecutive days with elevated signals = max

     Base score calculation:
     - base_score = (0.30 * magnitude_norm + 0.20 * tone_norm + 0.20 * velocity_norm + 0.15 * attention_norm + 0.15 * persistence_norm) * 100

     Confidence modifier (0.5-1.0):
     - source_diversity = min(event.num_sources / 5, 1.0)
     - source_credibility = event.source_credibility
     - corroboration = event.metadata.get('corroboration_score', 0.5) — cross-source matching
     - confidence_mod = 0.5 + 0.5 * (0.4 * source_diversity + 0.3 * source_credibility + 0.3 * corroboration)

     Final score:
     - risk_score = base_score * confidence_mod

     Severity floor:
     - if event.severity == "P1": risk_score = max(risk_score, 70)
     - if event.severity == "P2": risk_score = max(risk_score, 50)

  2. Return dict with risk_score (0-100), magnitude_contrib, tone_contrib, velocity_contrib, attention_contrib, persistence_contrib, confidence_mod, base_score (for explainability)

  3. For rolling_stats parameter: Initially pass empty dict {}, velocity will use default 0.5. Phase 26+ can add rolling window calculation.

  Use exact formula and weights from design doc section 7.1.
  </action>
  <verify>
  python -c "from data_pipeline.services.composite_risk_scorer import CompositeRiskScorer; from api.bigquery_models import Event; from datetime import datetime; e = Event(source_url='test', event_timestamp=datetime.now(), created_at=datetime.now(), category='POLITICAL', severity='P2', magnitude_norm=0.6, tone_norm=0.7, num_sources=5, source_credibility=0.8, direction='NEGATIVE', confidence=0.8); result = CompositeRiskScorer.calculate_risk_score(e, {}); assert result['risk_score'] >= 50; assert 'magnitude_contrib' in result; assert 'confidence_mod' in result"
  </verify>
  <done>CompositeRiskScorer returns 0-100 risk_score with 5 component contributions, implements severity floor (P1≥70, P2≥50), confidence modifier scales final score, all weights match design doc (30/20/20/15/15)</done>
</task>

<task type="auto">
  <name>Task 3: Add category sub-scores and daily composite calculation</name>
  <files>backend/data_pipeline/services/category_scoring.py, backend/api/views/risk_summary.py</files>
  <action>
  1. Create CategoryScoring service implementing section 7.2 calculate_category_subscores():

     For each of 10 categories (POLITICAL, CONFLICT, ECONOMIC, TRADE, REGULATORY, INFRASTRUCTURE, HEALTHCARE, SOCIAL, ENVIRONMENTAL, ENERGY):
     - Filter events by category
     - Calculate weighted average: weighted_sum = sum(event.risk_score * severity_weight) where severity_weight = {"P1": 4, "P2": 3, "P3": 2, "P4": 1}
     - Apply event count boost: boosted_score = avg_score * (1 + 0.2 * min(event_count / 10, 1.0))
     - Return dict: {"score_political": X, "score_conflict": Y, ...} (0-100 each, capped at 100)

  2. Implement calculate_daily_composite(subscores: dict, events: list) from section 7.3 WITH Venezuela-tuned weights from 25-CONTEXT.md:

     Venezuela-Tuned Weights (adjusted from design doc to reflect oil dependence and sanctions risk):
     - score_political: 0.15 (unchanged)
     - score_conflict: 0.11 (reduced from 0.12 - normalized violence)
     - score_economic: 0.15 (unchanged)
     - score_trade: 0.11 (reduced from 0.12 - make room for energy/regulatory)
     - score_regulatory: 0.15 (increased from 0.12 - sanctions critical)
     - score_infrastructure: 0.07 (reduced from 0.08 - less priority)
     - score_healthcare: 0.04 (reduced from 0.05 - less priority)
     - score_social: 0.05 (reduced from 0.06 - less priority)
     - score_environmental: 0.04 (reduced from 0.05 - less priority)
     - score_energy: 0.18 (increased from 0.10 - oil is lifeline)

     TOTAL = 1.05 (needs adjustment). Final weights (sum to 1.0):
     - score_political: 0.14 (15% → 14%)
     - score_conflict: 0.11
     - score_economic: 0.14 (15% → 14%)
     - score_trade: 0.11
     - score_regulatory: 0.15
     - score_infrastructure: 0.07
     - score_healthcare: 0.04
     - score_social: 0.05
     - score_environmental: 0.04
     - score_energy: 0.18

     Composite = weighted_sum of subscores

     P1 boost: If any P1 events exist, composite = max(composite, 70) and apply boost: composite *= (1 + 0.05 * min(p1_count, 5)) — up to 25% boost for 5+ P1 events

  3. Create API endpoint GET /api/risk/summary/daily?date=YYYY-MM-DD:
     - Query events from BigQuery for given date
     - Calculate category sub-scores
     - Calculate daily composite (with Venezuela-tuned weights)
     - Return JSON: {date, risk_score (composite), sub_scores: {political, conflict, ...}, event_summary: {total, p1, p2, p3, p4}}

  NOTE: Venezuela-tuned weights prioritize ENERGY (18%) and REGULATORY (15%) over generic commodity trader weights. Document these as constants with comments explaining Venezuela context.
  </action>
  <verify>
  # CategoryScoring produces sub-scores
  python -c "from data_pipeline.services.category_scoring import CategoryScoring; events = [{'category': 'POLITICAL', 'risk_score': 80, 'severity': 'P1'}, {'category': 'POLITICAL', 'risk_score': 60, 'severity': 'P2'}]; subscores = CategoryScoring.calculate_category_subscores(events); assert subscores['score_political'] > 0; composite = CategoryScoring.calculate_daily_composite(subscores, events); assert composite >= 70"

  # API endpoint works
  curl http://localhost:8000/api/risk/summary/daily?date=2025-01-10 | jq '.sub_scores.political'
  </verify>
  <done>CategoryScoring calculates 10 sub-scores with severity weighting (P1=4, P2=3, P3=2, P4=1) and event count boost, calculate_daily_composite uses commodity trader weights (political/economic 15%, conflict/trade/regulatory 12%, etc.), P1 boost applies (min 70, up to 25% boost), API endpoint returns daily summary with risk_score and sub_scores</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] SeverityClassifier assigns P1-P4 with auto-triggers
- [ ] CompositeRiskScorer calculates 5-component score with confidence modifier
- [ ] Severity floor enforced (P1≥70, P2≥50)
- [ ] CategoryScoring produces 10 sub-scores
- [ ] Daily composite uses Venezuela-tuned weights (ENERGY 18%, REGULATORY 15%, weights sum to 1.0)
- [ ] API endpoint returns risk summary
- [ ] Venezuela-specific tuning documented (ENERGY/REGULATORY higher, CONFLICT thresholds adjusted)
</verification>

<success_criteria>

- All tasks completed
- P1-P4 severity system operational with auto-triggers
- Composite risk scoring with 5 components implemented
- Category sub-scores and daily composite calculation working with Venezuela-tuned weights
- API endpoint returns risk summary
- Venezuela-specific tuning implemented (ENERGY 18%, REGULATORY 15%)
- No errors introduced
  </success_criteria>

<output>
After completion, create `.planning/phases/25-update-system-to-follow-platform-design/25-02-SUMMARY.md`:

# Phase 25 Plan 02: P1-P4 Severity & Composite Scoring Summary

**Replaced SEV1-5 with industry-standard P1-P4 severity and implemented explainable 5-component risk scoring**

## Accomplishments

- Implemented P1-P4 severity classifier with deterministic auto-triggers (coup, nationalization, 10+ fatalities, oil disruption patterns)
- Created composite risk scorer with 5 components: magnitude (30%), tone (20%), velocity (20%), attention (15%), persistence (15%)
- Added confidence modifier (0.5-1.0) based on source diversity, credibility, and corroboration
- Implemented severity floor enforcement (P1 events minimum score 70, P2 minimum 50)
- Built category sub-scores calculation with severity weighting (P1=4x, P2=3x, P3=2x, P4=1x) and event count boost
- Created daily composite scoring with commodity trader weights (political/economic 15%, energy 10%, conflict/trade/regulatory 12%, others 5-8%)
- Added P1 event boost to daily composite (minimum 70, up to 25% increase for 5+ P1 events)
- Created GET /api/risk/summary/daily API endpoint

## Files Created/Modified

- `backend/data_pipeline/services/severity_classifier.py` - P1-P4 classifier with auto-triggers
- `backend/data_pipeline/services/composite_risk_scorer.py` - 5-component scoring engine
- `backend/data_pipeline/services/category_scoring.py` - Sub-scores and daily composite
- `backend/api/views/risk_summary.py` - Daily risk summary API endpoint

## Decisions Made

- Auto-triggers are deterministic (no LLM) for P1 reliability — pattern matching on event types, CAMEO codes, keywords, fatality counts
- Velocity component uses rolling stats (empty dict initially, defaults to 0.5) — full rolling window calculation deferred to Phase 26+
- Persistence component uses metadata.persistence_days (initially 1) — spike tracking from Phase 21 can populate this
- Corroboration score uses metadata.corroboration_score (default 0.5) — cross-source matching deferred to Phase 26+
- Commodity trader weights reflect user priorities (political/economic risk highest, healthcare/environmental lower)
- SEV1-5 strings deprecated but not removed (backward compatibility if needed)

## Issues Encountered

None

## Next Step

Phase 25 complete. Ready for Phase 26 (future) to add rolling window stats, persistence tracking, and corroboration scoring.
</output>

---
phase: 25-update-system-to-follow-platform-design
plan: 01
type: execute
---

<objective>
Implement extensible canonical event model and 10-category classification system from platform design.

Purpose: Establish unified data schema across all sources with rich fields (actors, magnitude, tone, location) PLUS future-proof hooks for enhancements (GKG themes, entity relationships, event lineage), and standardize category taxonomy for consistent risk analysis.
Output: Canonical BigQuery schema with 30+ core fields + enhancement arrays, category classification system, source-specific normalizers for GDELT/ACLED/World Bank/Google Trends/SEC/FRED/Comtrade
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-update-system-to-follow-platform-design/25-CONTEXT.md
@docs/venezuela_risk_platform_design.md
@backend/api/bigquery_models.py
@backend/data_pipeline/adapters/base.py
@backend/data_pipeline/adapters/gdelt_adapter.py

**Platform Design Requirements:**
- Canonical event table with 30+ fields: event_id, source, temporal (event_timestamp, ingested_at), classification (category, subcategory, event_type), location (country_code, admin1, admin2, lat/lon), magnitude (raw, unit, norm), direction (POSITIVE/NEGATIVE/NEUTRAL), tone (raw, norm), confidence (num_sources, source_credibility), actors (actor1/2 name/type), commodities/sectors arrays, raw_payload JSON
- 10 categories: POLITICAL, CONFLICT, ECONOMIC, TRADE, REGULATORY, INFRASTRUCTURE, HEALTHCARE, SOCIAL, ENVIRONMENTAL, ENERGY
- Source-specific normalizers that map native schemas → canonical schema
- Category mapping logic per source (GDELT CAMEO codes, ACLED event types, World Bank indicators, Google Trends terms, SEC keywords, FRED series, Comtrade commodity codes)

**Current Implementation:**
- Simple Event model with ~12 fields (id, title, content, source_url, source_name, event_type, location, risk_score, severity, metadata JSON)
- No formal category system
- Adapter pattern exists but transforms directly to simple Event
- Phase 24 added entity linking via metadata.linked_entities

**Gap:**
- Missing 18+ canonical fields (actors, magnitude metrics, tone scores, confidence components, direction, commodities/sectors)
- No 10-category classification
- Normalizers need category mapping and field extraction logic

**Vision (from 25-CONTEXT.md):**
- Premium + Scalable: Extensible schema with future-proof hooks for Phase 26+ enhancements
- Enhancement arrays (initially empty): themes (GKG), quotations, gcam_scores, entity_relationships, related_events
- Phase 25: Create the extensible schema with typed arrays
- Phase 26+: Populate GKG themes, build relationship graphs, track event lineage
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend BigQuery Event schema to canonical model</name>
  <files>backend/api/bigquery_models.py, migrations/create_canonical_events_table.sql</files>
  <action>
  1. Update Event dataclass in bigquery_models.py to canonical schema from design doc (section 4.2) PLUS extensible enhancement arrays from 25-CONTEXT.md:

     Core canonical fields (30+):
     - Add temporal: event_timestamp (when occurred), ingested_at (when ingested)
     - Add classification: category (10-cat enum), subcategory, event_type
     - Add location: country_code, admin1, admin2, latitude, longitude
     - Add magnitude: magnitude_raw (float), magnitude_unit (str), magnitude_norm (0-1 float)
     - Add direction: direction (POSITIVE|NEGATIVE|NEUTRAL enum)
     - Add tone: tone_raw (float), tone_norm (0-1 float)
     - Add confidence: num_sources (int), source_credibility (0-1 float), confidence (0-1 float)
     - Add actors: actor1_name, actor1_type, actor2_name, actor2_type (strings)
     - Add commodities: commodities (array of strings), sectors (array of strings)
     - Keep metadata JSON for source-specific payloads
     - Rename mentioned_at → event_timestamp for clarity

     Enhancement arrays (future-proof, initially empty):
     - Add themes: List[str] field (for GKG V2Themes, 2300+ categories, populated in Phase 26+)
     - Add quotations: List[Dict] field with {speaker: str, text: str, offset: int} structure (for who-said-what, Phase 26+)
     - Add gcam_scores: Optional[Dict] field for GCAM emotional dimensions {fear: float, anger: float, joy: float, etc.} (Phase 26+)
     - Add entity_relationships: List[Dict] field with {entity1_id: str, entity2_id: str, relationship_type: str} (for actor networks, Phase 26+)
     - Add related_events: List[Dict] field with {event_id: str, relationship_type: str, timestamp: str} (for narrative arcs, Phase 27+)

     All enhancement arrays default to empty ([] or None) in Phase 25. Schema is typed and ready for Phase 26+ to populate without migration.

  2. Create BigQuery table migration SQL with canonical schema + enhancement arrays, partitioned by DATE(event_timestamp). Use REPEATED and STRUCT for nested arrays (BigQuery native support).

  3. Add validation in to_bigquery_row(): category must be one of 10 categories, direction must be POSITIVE|NEGATIVE|NEUTRAL, magnitude_norm must be 0-1, confidence must be 0-1

  NOTE: This is a schema extension, not a breaking change. Existing Event fields (title, content, source_url, risk_score, severity) remain. We're adding canonical fields + future-proof hooks alongside.
  </action>
  <verify>
  python -c "from api.bigquery_models import Event; e = Event(source_url='test', event_timestamp='2025-01-10', ingested_at='2025-01-10', created_at='2025-01-10', category='POLITICAL', direction='NEGATIVE', magnitude_norm=0.5, confidence=0.7, themes=[], quotations=[], entity_relationships=[], related_events=[]); row = e.to_bigquery_row(); assert 'category' in row; assert 'magnitude_norm' in row; assert 'themes' in row; assert 'entity_relationships' in row"
  </verify>
  <done>Event dataclass has 30+ core fields + 5 enhancement arrays, to_bigquery_row() includes all fields, enhancement arrays default to empty, validation raises on invalid category/direction/norms, BigQuery schema supports REPEATED/STRUCT for nested data</done>
</task>

<task type="auto">
  <name>Task 2: Implement category classification system</name>
  <files>backend/data_pipeline/services/category_classifier.py</files>
  <action>
  Create CategoryClassifier service implementing section 5.1 category mapping logic from design doc:

  1. Define CATEGORY_MAP dict with mappings for each source:
     - GDELT: CAMEO root codes (01-20) → categories (from design doc section 5.1)
     - ACLED: event_type strings → categories
     - World Bank: indicator code prefixes → categories (NY.GDP=ECONOMIC, NE.EXP/IMP=TRADE, SH.=HEALTHCARE, EG.=ENERGY)
     - Google Trends: search term keywords → categories (design section 5.1)
     - SEC EDGAR: filing context keywords → categories (sanctions=REGULATORY, oil=ENERGY, currency=ECONOMIC)
     - FRED: series ID patterns → categories (EXVZUS=ECONOMIC)
     - UN Comtrade: commodity code (HS 2-digit) → categories (27=ENERGY for oil, default=TRADE)

  2. Implement classify(source, source_data) method:
     - Takes source name + raw data dict
     - Returns (category, subcategory) tuple
     - Subcategory = source-specific code (CAMEO code, indicator ID, term, etc.)

  3. Add fallback logic: if no match, default to POLITICAL for news/events, ECONOMIC for data series

  Use exact mappings from design doc section 5.1 CATEGORY_MAP. This is the authoritative source-to-category taxonomy.
  </action>
  <verify>
  python -c "from data_pipeline.services.category_classifier import CategoryClassifier; cat, sub = CategoryClassifier.classify('gdelt', {'EventCode': '14'}); assert cat == 'SOCIAL'; cat2, _ = CategoryClassifier.classify('world_bank', {'indicator_code': 'NY.GDP.MKTP.CD'}); assert cat2 == 'ECONOMIC'"
  </verify>
  <done>CategoryClassifier.classify() returns correct category for all 7 sources using design doc mappings, handles unknown codes with fallbacks, subcategory captures source-specific identifier</done>
</task>

<task type="auto">
  <name>Task 3: Update adapters to use canonical normalizers</name>
  <files>backend/data_pipeline/adapters/gdelt_adapter.py, backend/data_pipeline/adapters/world_bank_adapter.py, backend/data_pipeline/adapters/google_trends_adapter.py</files>
  <action>
  Update transform() method in each adapter to populate canonical Event fields using normalizer logic from design doc section 5.2:

  GDELT (normalize_gdelt pattern):
  - category/subcategory: Use CategoryClassifier with CAMEO EventCode
  - magnitude_raw: GoldsteinScale (-10 to +10)
  - magnitude_unit: "goldstein"
  - magnitude_norm: (goldstein + 10) / 20 → 0-1
  - direction: NEGATIVE if goldstein < -2, POSITIVE if > 2, else NEUTRAL
  - tone_raw: AvgTone
  - tone_norm: min(max((AvgTone * -1 + 10) / 20, 0), 1) — invert so negative tone → higher risk
  - num_sources: NumSources
  - source_credibility: 0.7 (GDELT baseline)
  - confidence: min(NumSources / 10, 1.0) * 0.7
  - actor1_name/type, actor2_name/type: From Actor1Name/Actor2Name fields
  - commodities/sectors: Extract from CAMEO themes (if available in metadata.gkg_data.themes)

  World Bank (normalize_world_bank pattern):
  - category/subcategory: Use CategoryClassifier with indicator_code
  - magnitude_raw: percent_change from prev_value
  - magnitude_unit: "percent_change"
  - magnitude_norm: min(abs(pct_change) / 50, 1.0)
  - direction: POSITIVE if pct_change > 0 (unless indicator is "bad news" like inflation), else NEGATIVE
  - tone_norm: 0.5 (neutral for data)
  - source_credibility: 0.95 (World Bank is authoritative)

  Google Trends (normalize_google_trends pattern):
  - category/subcategory: Use CategoryClassifier with term
  - magnitude_raw: interest (0-100)
  - magnitude_unit: "interest_score"
  - magnitude_norm: interest / 100
  - direction: NEGATIVE (assume elevated attention = concern)
  - tone_norm: min(spike_ratio / 5, 1.0) where spike_ratio = interest / baseline
  - source_credibility: 0.8

  Apply same pattern to remaining adapters (sec_edgar_adapter, others) using design doc section 5.2 normalizer functions as reference. Keep existing metadata.linked_entities logic from Phase 24.
  </action>
  <verify>
  # GDELT adapter produces canonical events
  python -c "from data_pipeline.adapters.gdelt_adapter import GDELTAdapter; a = GDELTAdapter(); events = a.transform([{'GLOBALEVENTID': '1', 'EventCode': '14', 'GoldsteinScale': -5.0, 'AvgTone': -2.5, 'NumSources': 3, 'SQLDATE': '20250110'}]); assert events[0].category in ['POLITICAL', 'CONFLICT', 'ECONOMIC', 'TRADE', 'REGULATORY', 'INFRASTRUCTURE', 'HEALTHCARE', 'SOCIAL', 'ENVIRONMENTAL', 'ENERGY']; assert events[0].magnitude_norm >= 0 and events[0].magnitude_norm <= 1; assert events[0].direction in ['POSITIVE', 'NEGATIVE', 'NEUTRAL']"
  </verify>
  <done>All adapters populate canonical Event fields (category, magnitude metrics, tone metrics, direction, confidence components, actors), validation passes for category/direction enums and 0-1 norms, entity linking from Phase 24 preserved</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Event dataclass has all 30+ canonical fields
- [ ] Event dataclass has 5 enhancement arrays (themes, quotations, gcam_scores, entity_relationships, related_events) defaulting to empty
- [ ] CategoryClassifier returns correct categories for all 7 sources
- [ ] Adapter transform() methods populate canonical fields
- [ ] BigQuery schema migration SQL created with REPEATED/STRUCT support for nested arrays
- [ ] Validation enforces 10 categories, direction enum, 0-1 norms
- [ ] No breaking changes to existing Event usage (title, content, risk_score, severity remain)
- [ ] Enhancement arrays are typed and ready for Phase 26+ without schema migration
</verification>

<success_criteria>

- All tasks completed
- Canonical Event model with 30+ core fields + 5 enhancement arrays operational
- 10-category classification system implemented
- Source-specific normalizers updated with category mapping
- Extensible schema ready for Phase 26+ GKG/relationship/lineage enhancements
- No TypeScript/Python errors introduced
- Entity linking from Phase 24 preserved
  </success_criteria>

<output>
After completion, create `.planning/phases/25-update-system-to-follow-platform-design/25-01-SUMMARY.md`:

# Phase 25 Plan 01: Canonical Event Model & Category System Summary

**Implemented canonical data model with 10-category taxonomy across all sources**

## Accomplishments

- Extended BigQuery Event schema from 12 to 30+ canonical fields (actors, magnitude, tone, confidence, location, commodities/sectors)
- Added 5 future-proof enhancement arrays: themes (GKG), quotations, gcam_scores, entity_relationships, related_events (initially empty, ready for Phase 26+)
- Implemented 10-category classification (POLITICAL, CONFLICT, ECONOMIC, TRADE, REGULATORY, INFRASTRUCTURE, HEALTHCARE, SOCIAL, ENVIRONMENTAL, ENERGY)
- Created CategoryClassifier with source-specific mapping logic (GDELT CAMEO, ACLED types, WB indicators, Trends terms, SEC keywords, FRED series, Comtrade codes)
- Updated adapters with canonical normalizers (GDELT GoldsteinScale→magnitude, AvgTone→tone, World Bank pct_change, Google Trends interest)
- BigQuery schema supports REPEATED/STRUCT for nested arrays (themes[], quotations[], relationships[])

## Files Created/Modified

- `backend/api/bigquery_models.py` - Extended Event dataclass to canonical schema
- `migrations/create_canonical_events_table.sql` - BigQuery DDL for canonical table
- `backend/data_pipeline/services/category_classifier.py` - 10-category classification system
- `backend/data_pipeline/adapters/gdelt_adapter.py` - Canonical normalizer for GDELT
- `backend/data_pipeline/adapters/world_bank_adapter.py` - Canonical normalizer for World Bank
- `backend/data_pipeline/adapters/google_trends_adapter.py` - Canonical normalizer for Google Trends

## Decisions Made

- **Extensible schema architecture**: Added 5 enhancement arrays (themes, quotations, gcam_scores, entity_relationships, related_events) in Phase 25 even though they remain empty — enables Phase 26+ to populate without schema migration
- Schema extension (not migration): Kept existing Event fields (title, content, risk_score, severity) to avoid breaking current usage
- event_timestamp replaces mentioned_at for clarity (aligns with design doc terminology)
- Subcategory stores source-specific code (CAMEO, indicator ID, term) for traceability
- Default fallbacks: POLITICAL for news/events, ECONOMIC for data series when category unknown
- Phase 24 entity linking preserved (metadata.linked_entities unchanged)
- BigQuery REPEATED/STRUCT for arrays: Native support for nested data, efficient queries, no JSON parsing overhead

## Issues Encountered

None

## Next Step

Ready for 25-02-PLAN.md (P1-P4 severity system and composite scoring)
</output>

---
phase: 06-entity-watch
plan: 02
type: execute
---

<objective>
Implement Celery task to extract and normalize entities from events, and create TrendingService for Redis-based entity leaderboard with time-decay scoring.

Purpose: Process existing events to populate Entity/EntityMention tables and maintain real-time trending scores
Output: Celery task for entity extraction, TrendingService for leaderboard, management command for backfill
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-entity-watch/06-CONTEXT.md
@.planning/phases/06-entity-watch/DISCOVERY.md
@.planning/phases/06-entity-watch/06-01-SUMMARY.md
@backend/core/models.py
@backend/data_pipeline/services/entity_service.py

**From Plan 06-01:**
- Entity and EntityMention models with fuzzy matching
- EntityService.find_or_create_entity() normalizes names
- EntityService.link_entity_to_event() creates mentions

**From DISCOVERY.md:**
- Redis Sorted Sets for trending leaderboard (O(log N) operations)
- Exponential time-decay: score = weight * exp(-age_hours / 168) with 7-day half-life
- Entity extraction from existing Event.entities ArrayField (populated by LLM)
- Real-time updates on new events, batch backfill for existing events

**From Phase 3:**
- Celery + Redis already configured for async tasks
- Task pattern: @shared_task with error handling and logging

**Key implementation:**
- Extract entities from Event.entities ArrayField (already populated by LLM analysis)
- Use EntityService to normalize and link each entity to event
- Update Redis trending score for each entity based on event timestamp
- Backfill existing events to populate entity data retroactively
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TrendingService with Redis Sorted Sets and time-decay</name>
  <files>backend/data_pipeline/services/trending_service.py</files>
  <action>
Create backend/data_pipeline/services/trending_service.py with TrendingService class containing:

1. **update_entity_score(entity_id: str, timestamp: datetime, weight: float = 1.0)**
   - Calculate exponential time-decay: score = weight * exp(-age_hours / 168) where 168 = 7-day half-life
   - age_hours = (datetime.utcnow() - timestamp).total_seconds() / 3600
   - Use redis_client.zincrby('entity:trending', score, str(entity_id)) to increment score
   - Key format: 'entity:trending' (global leaderboard)

2. **get_trending_entities(metric: str = 'mentions', limit: int = 50) -> List[dict]**
   - metric options: 'mentions' (default), 'risk', 'sanctions'
   - For 'mentions': Use redis_client.zrevrange('entity:trending', 0, limit-1, withscores=True)
   - For 'risk': Query Entity.objects with mention_count > 0, annotate with avg risk_score from entity_mentions
   - For 'sanctions': Query Entity.objects with sanctions matches, order by mention_count
   - Return list of dicts: [{'entity_id': uuid, 'canonical_name': str, 'entity_type': str, 'score': float, 'mention_count': int}]
   - Load Entity objects in batch using prefetch_related to avoid N+1 queries

3. **get_entity_rank(entity_id: str) -> Optional[int]**
   - Use redis_client.zrevrank('entity:trending', str(entity_id))
   - Return rank + 1 (1-indexed) or None if not ranked

4. **sync_trending_scores(days: int = 30)**
   - Recalculate all trending scores from EntityMention records in time window
   - Clear redis key: redis_client.delete('entity:trending')
   - Query EntityMention.objects.filter(mentioned_at__gte=cutoff).select_related('entity')
   - For each mention: update_entity_score(entity.id, mentioned_at)
   - Used for periodic sync (daily cron) to correct drift

Import: import redis, from django.conf import settings, from datetime import datetime, timedelta, import math
Use settings.REDIS_URL for redis connection (same as Celery broker).
  </action>
  <verify>python backend/manage.py check passes, TrendingService can connect to Redis</verify>
  <done>TrendingService with 4 methods (update_score, get_trending, get_rank, sync), exponential decay with 7-day half-life, Redis Sorted Sets integration</done>
</task>

<task type="auto">
  <name>Task 2: Create extract_entities Celery task for event processing</name>
  <files>backend/data_pipeline/tasks/entity_extraction.py</files>
  <action>
Create backend/data_pipeline/tasks/entity_extraction.py with:

1. **@shared_task extract_entities_from_event(event_id: str) -> dict**
   - Load Event by UUID
   - If event.entities is empty: log warning and return (nothing to extract)
   - Parse event.entities (currently ArrayField of strings, may contain JSON)
   - For each entity in event.entities:
     - Determine entity_type from LLM analysis (check event.llm_analysis['entities'] for structured data)
     - If LLM provides {'people': [...], 'organizations': [...], 'locations': [...]}, use that
     - Otherwise infer type heuristically (PERSON if single word + capitalized, ORGANIZATION if Inc/Corp/Ltd, default ORGANIZATION)
     - Call EntityService.find_or_create_entity(raw_name, entity_type)
     - Call EntityService.link_entity_to_event(entity, event, raw_name, match_score)
     - Call TrendingService.update_entity_score(entity.id, event.timestamp)
   - Return {'event_id': event_id, 'entities_extracted': count, 'entities_linked': count}
   - Use @transaction.atomic for database operations
   - Handle errors gracefully (log and continue on fuzzy match failures)

2. **@shared_task backfill_entities(days: int = 30, batch_size: int = 100) -> dict**
   - Query events from last N days: Event.objects.filter(timestamp__gte=cutoff, entities__len__gt=0)
   - Use .iterator(chunk_size=batch_size) for memory efficiency
   - For each event: call extract_entities_from_event.delay(event.id) to enqueue async
   - Return {'events_queued': count, 'days': days}
   - This is a batch task for initial population and periodic refresh

Import: from celery import shared_task, from django.db import transaction, from data_pipeline.services.entity_service import EntityService, from data_pipeline.services.trending_service import TrendingService, import logging

Use logging.getLogger(__name__) for task logs.
  </action>
  <verify>python backend/manage.py check passes, tasks registered with Celery</verify>
  <done>Celery tasks for entity extraction and backfill, integrated with EntityService and TrendingService</done>
</task>

<task type="auto">
  <name>Task 3: Create management command and test entity extraction on sample events</name>
  <files>backend/data_pipeline/management/commands/extract_entities.py</files>
  <action>
Create management command backend/data_pipeline/management/commands/extract_entities.py:

Command: python manage.py extract_entities [--days 30] [--sync-trending] [--all]

Options:
- --days N: Process events from last N days (default 30)
- --all: Process all events with entities (overrides --days)
- --sync-trending: After extraction, sync trending scores from EntityMention records

Implementation:
- Parse arguments using argparse
- If --all: backfill_entities.delay() with no time filter
- Else: backfill_entities.delay(days=args.days)
- If --sync-trending: TrendingService.sync_trending_scores(days=args.days)
- Print progress: "Queued X events for entity extraction"
- Print stats: Entity.objects.count(), EntityMention.objects.count()

Then run the command to test:
```bash
cd backend
python manage.py extract_entities --days 7 --sync-trending
```

Check results:
- Entity.objects.count() > 0 (entities created)
- EntityMention.objects.count() > 0 (mentions linked)
- TrendingService.get_trending_entities(limit=10) returns entities with scores

Manual verification in Django shell:
```python
from core.models import Entity, EntityMention
from data_pipeline.services.trending_service import TrendingService

# Check top entities
entities = Entity.objects.order_by('-mention_count')[:10]
for e in entities:
    print(f"{e.canonical_name} ({e.entity_type}): {e.mention_count} mentions")

# Check trending scores
trending = TrendingService.get_trending_entities(limit=10)
for t in trending:
    print(f"{t['canonical_name']}: {t['score']:.2f}")
```
  </action>
  <verify>Management command runs without errors, entities extracted and linked, trending scores populated in Redis</verify>
  <done>Entity extraction working end-to-end, trending leaderboard functional, backfill command available</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] TrendingService with Redis Sorted Sets and exponential time-decay
- [ ] Celery tasks: extract_entities_from_event, backfill_entities
- [ ] Management command: extract_entities with --days, --all, --sync-trending options
- [ ] Test run extracts entities from events and populates Entity/EntityMention tables
- [ ] Redis trending scores populated, TrendingService.get_trending_entities() returns results
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Entity extraction processes existing events successfully
- Trending leaderboard shows entities ranked by time-decayed mention scores
- Backfill and sync commands working for maintenance
  </success_criteria>

<output>
After completion, create `.planning/phases/06-entity-watch/06-02-SUMMARY.md`:

# Phase 6 Plan 2: Entity Extraction Celery Task Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description
- `path/to/another.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 06-03-PLAN.md (Entity API & Trending Endpoints)
</output>

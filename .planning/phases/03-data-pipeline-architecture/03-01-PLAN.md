---
phase: 03-data-pipeline-architecture
plan: 01
subsystem: backend-infrastructure
tags: [celery, redis, task-queue, gcp-memorystore, secret-manager]

requires:
  - phase: 01-01
    provides: Django project structure, GCP infrastructure
  - phase: discovery
    provides: Celery + Redis recommendation, API credential requirements

provides:
  - Celery task queue with Redis broker
  - django-celery-results for task result storage
  - GCP Memorystore (Redis) for production
  - GCP Secret Manager integration for API keys
  - Base ingestion task framework

affects: [03-02, 03-03, 03-04]

tech-stack:
  patterns: [task-queue, async-workers, result-backend]
  libraries: [celery, redis, django-celery-results, google-cloud-secret-manager]

confidence: high
estimated-completion: 15 min
---

# Plan 03-01: Celery + Redis Infrastructure Setup

**Build async task queue infrastructure with Celery, Redis broker, django-celery-results, GCP Memorystore, and Secret Manager integration for API credentials**

## Context

Phase 3 requires asynchronous ingestion from 7 external APIs with mixed latency requirements:
- Real-time: GDELT (15min), ReliefWeb (daily)
- Daily batch: FRED economic indicators
- Monthly/quarterly: UN Comtrade, World Bank

Discovery research (DISCOVERY.md) recommends:
- Celery + Redis (don't hand-roll task queue)
- django-celery-results (ORM result backend)
- GCP Cloud Scheduler (cron replacement)
- Tenacity for retry logic (not custom)
- Two-layer rate limiting (Celery + Tenacity)

This plan establishes the foundation for all subsequent ingestion tasks.

## Goals

1. Install and configure Celery with Redis broker
2. Set up django-celery-results for task result storage in PostgreSQL
3. Configure GCP Memorystore (Redis) for production environment
4. Integrate GCP Secret Manager for API credentials
5. Create base ingestion task framework with error handling
6. Verify task execution locally with development Redis

## Non-Goals

- Implementing specific API ingestions (Plans 03-02, 03-03, 03-04)
- GCP Cloud Scheduler setup (done in later plans when tasks exist)
- Flower monitoring UI (deferred to production optimization)
- Task result cleanup/expiration (deferred to operations phase)

## Tasks

### Task 1: Install Celery dependencies and configure Django app

**Install packages:**
```bash
cd backend
pip install celery[redis]==5.4.0 redis==5.1.1 django-celery-results==2.5.1 google-cloud-secret-manager==2.21.1
pip freeze | grep -E "celery|redis|google-cloud-secret-manager" >> requirements.txt
```

**Create celery app configuration:**
- File: `backend/config/celery.py`
- Configure Celery app with Django settings autodiscovery
- Set broker_url from environment variable (defaults to redis://localhost:6379/0)
- Configure result_backend to use django-db
- Set task_serializer='json', accept_content=['json']
- Enable task_track_started=True for monitoring
- Set task_time_limit=300 (5 min max per task)

**Update Django settings:**
- File: `backend/config/settings.py`
- Add 'django_celery_results' to INSTALLED_APPS
- Add CELERY_BROKER_URL setting (from env var REDIS_URL)
- Add CELERY_RESULT_BACKEND = 'django-db'
- Add CELERY_CACHE_BACKEND = 'default'

**Create __init__.py update:**
- File: `backend/config/__init__.py`
- Import celery app to ensure it's loaded on Django start
- Pattern: `from .celery import app as celery_app; __all__ = ('celery_app',)`

**Run migrations:**
```bash
python manage.py migrate django_celery_results
```

**Verification:**
- Start Celery worker: `celery -A config worker --loglevel=info`
- Should see "celery@hostname ready" message
- Should see result backend connected

**Commit:**
```
feat(03-01): install and configure Celery with Redis broker

Task 1: Install Celery dependencies and configure Django app
- Install celery[redis]==5.4.0, redis==5.1.1, django-celery-results==2.5.1
- Create config/celery.py with Django autodiscovery
- Configure broker_url from REDIS_URL env var (default: redis://localhost:6379/0)
- Configure result_backend='django-db' for PostgreSQL storage
- Add django_celery_results to INSTALLED_APPS
- Run migrations for celery results tables
- Verify worker starts successfully
```

### Task 2: Create base ingestion task framework

**Create data_pipeline app:**
```bash
python manage.py startapp data_pipeline
```

**Create base task classes:**
- File: `backend/data_pipeline/tasks/base.py`
- Create `BaseIngestionTask(celery.Task)` abstract class
- Implement `on_failure()` hook for error logging
- Implement `on_success()` hook for success metrics
- Add `max_retries=3`, `default_retry_delay=60` as class attributes
- Add helper method `get_api_credential(key_name: str)` placeholder

**Create task utils:**
- File: `backend/data_pipeline/tasks/utils.py`
- Create `configure_retry_strategy()` function using Tenacity
- Default: exponential backoff, max 3 attempts, wait 2^x * 1 second
- Create `RateLimiter` class with token bucket algorithm
- Create `log_ingestion_event()` function for audit trail

**Update Django settings:**
- Add 'data_pipeline' to INSTALLED_APPS

**Create test task:**
- File: `backend/data_pipeline/tasks/test_tasks.py`
- Create `@shared_task` hello_world task
- Test with: `celery -A config worker -l info` + Python shell task.delay()

**Verification:**
```python
from data_pipeline.tasks.test_tasks import hello_world
result = hello_world.delay()
result.get(timeout=10)  # Should return "Hello World"
result.successful()  # Should be True
```

**Commit:**
```
feat(03-01): create base ingestion task framework

Task 2: Create base ingestion task framework
- Create data_pipeline Django app
- Create BaseIngestionTask abstract class with retry logic
- Implement on_failure() and on_success() hooks for monitoring
- Create task utilities: configure_retry_strategy(), RateLimiter, log_ingestion_event()
- Add test task to verify framework
- Verify task execution end-to-end
```

### Task 3: Integrate GCP Secret Manager for API credentials

**Create Secret Manager client:**
- File: `backend/data_pipeline/services/secrets.py`
- Create `SecretManagerClient` class
- Method: `get_secret(secret_id: str, version: str = 'latest') -> str`
- Use google.cloud.secretmanager_v1
- Cache secrets in memory (class-level dict) to avoid repeated API calls
- Handle SecretNotFound gracefully with clear error messages

**Update base task:**
- File: `backend/data_pipeline/tasks/base.py`
- Implement `get_api_credential(key_name: str)` using SecretManagerClient
- Pattern: `return SecretManagerClient().get_secret(f'api-{key_name}')`

**Create management command to set secrets:**
- File: `backend/data_pipeline/management/commands/set_secret.py`
- Command: `python manage.py set_secret <secret_id> <value>`
- Use Secret Manager API to create/update secrets
- Useful for local development secret setup

**Update settings:**
- File: `backend/config/settings.py`
- Add GCP_PROJECT_ID setting (from env var, required for Secret Manager)
- Add SECRET_MANAGER_ENABLED setting (default: False for dev, True for prod)
- If SECRET_MANAGER_ENABLED=False, fall back to env vars

**Test Secret Manager locally:**
```bash
# Set test secret
python manage.py set_secret test-api-key "test-value-123"

# Verify retrieval
python manage.py shell
>>> from data_pipeline.services.secrets import SecretManagerClient
>>> client = SecretManagerClient()
>>> client.get_secret('test-api-key')
'test-value-123'
```

**Commit:**
```
feat(03-01): integrate GCP Secret Manager for API credentials

Task 3: Integrate GCP Secret Manager for API credentials
- Create SecretManagerClient with get_secret() method
- Implement in-memory secret caching to reduce API calls
- Update BaseIngestionTask.get_api_credential() to use Secret Manager
- Create set_secret management command for secret provisioning
- Add GCP_PROJECT_ID and SECRET_MANAGER_ENABLED settings
- Fall back to env vars when SECRET_MANAGER_ENABLED=False
- Verify secret storage and retrieval end-to-end
```

### Task 4: Configure GCP Memorystore (Redis) and deployment settings

**Create GCP Memorystore instance (via gcloud):**
```bash
# Create Redis instance (Basic tier, 1GB)
gcloud redis instances create venezuelawatch-redis \
  --size=1 \
  --region=us-central1 \
  --redis-version=redis_7_0 \
  --tier=basic \
  --project=venezuelawatch-staging

# Get connection info
gcloud redis instances describe venezuelawatch-redis \
  --region=us-central1 \
  --format="get(host,port)"
```

**Update production settings:**
- File: `backend/config/settings_prod.py` (or existing production config)
- Set CELERY_BROKER_URL from env var REDIS_URL
- Pattern: `redis://<memorystore-ip>:6379/0`
- Set CELERY_RESULT_EXPIRES = 86400 (24 hours)
- Set CELERY_TASK_ALWAYS_EAGER = False (ensure async execution)

**Create deployment documentation:**
- File: `backend/docs/deployment/celery-setup.md`
- Document Memorystore connection string format
- Document how to set REDIS_URL secret in Secret Manager
- Document how to run Celery worker in Cloud Run/Compute Engine
- Document monitoring commands (celery inspect active, stats)

**Update README:**
- File: `backend/README.md`
- Add "Running Celery Locally" section
- Commands:
  ```bash
  # Terminal 1: Start Redis
  redis-server

  # Terminal 2: Start Celery worker
  celery -A config worker --loglevel=info

  # Terminal 3: Django development server
  python manage.py runserver
  ```

**Commit:**
```
feat(03-01): configure GCP Memorystore and deployment settings

Task 4: Configure GCP Memorystore (Redis) and deployment settings
- Create GCP Memorystore Redis instance (Basic tier, 1GB, Redis 7.0)
- Update production settings with CELERY_BROKER_URL from REDIS_URL env var
- Configure CELERY_RESULT_EXPIRES=86400 (24 hours)
- Create deployment documentation for Celery worker setup
- Update README with local Celery development instructions
- Document Memorystore connection pattern
```

### checkpoint:human-verify

**Verify the following:**

1. **Celery worker starts successfully:**
   ```bash
   cd backend
   celery -A config worker --loglevel=info
   ```
   Expected: See "celery@<hostname> ready" with 0 errors

2. **Test task execution:**
   ```bash
   python manage.py shell
   ```
   ```python
   from data_pipeline.tasks.test_tasks import hello_world
   result = hello_world.delay()
   result.get(timeout=10)  # Should return "Hello World"
   result.successful()  # Should be True
   ```

3. **Secret Manager integration (if enabled):**
   ```bash
   python manage.py set_secret test-api-key "test-value"
   python manage.py shell
   ```
   ```python
   from data_pipeline.services.secrets import SecretManagerClient
   client = SecretManagerClient()
   client.get_secret('test-api-key')  # Should return "test-value"
   ```

4. **GCP Memorystore created:**
   ```bash
   gcloud redis instances describe venezuelawatch-redis --region=us-central1
   ```
   Expected: Status = READY

5. **Database tables created:**
   ```bash
   python manage.py migrate --plan
   ```
   Expected: django_celery_results migrations applied

**Type "approved" to continue or describe any issues.**

## Verification Criteria

- [ ] Celery worker starts without errors
- [ ] Redis connection successful (both local and Memorystore)
- [ ] Test task executes successfully and result is stored
- [ ] BaseIngestionTask framework created with retry logic
- [ ] Secret Manager integration works (get/set secrets)
- [ ] GCP Memorystore instance created and reachable
- [ ] django-celery-results migrations applied
- [ ] Documentation updated with deployment instructions

## Dependencies

**Python packages:**
- celery[redis]==5.4.0
- redis==5.1.1
- django-celery-results==2.5.1
- google-cloud-secret-manager==2.21.1

**GCP resources:**
- GCP Memorystore (Redis) - Basic tier, 1GB, us-central1
- GCP Secret Manager API enabled

**Local development:**
- Redis server running (brew install redis)
- PostgreSQL with TimescaleDB (existing)

## Risks

1. **GCP Memorystore cost**: Basic tier is ~$50/month. Consider Standard tier ($100/month) for production HA.
2. **Secret Manager API calls**: Caching mitigates, but be aware of quota (1500 requests/min).
3. **Celery worker deployment**: Need Cloud Run or Compute Engine instance. Cloud Run doesn't support long-running workers well - prefer Compute Engine or GKE.

## Rollback Plan

If major issues occur:
1. Remove django_celery_results from INSTALLED_APPS
2. Run `python manage.py migrate django_celery_results zero`
3. Remove Celery imports from config/__init__.py
4. Uninstall packages: `pip uninstall celery redis django-celery-results`
5. Delete GCP Memorystore: `gcloud redis instances delete venezuelawatch-redis --region=us-central1`

## Next Steps

After this plan completes:
- **Plan 03-02**: Implement GDELT + ReliefWeb real-time ingestion tasks
- **Plan 03-03**: Implement FRED daily batch ingestion
- **Plan 03-04**: Implement UN Comtrade + World Bank monthly ingestion

---
phase: 03-data-pipeline-architecture
plan: 02
subsystem: data-ingestion
tags: [gdelt, reliefweb, real-time, celery-tasks, api-integration]

requires:
  - phase: 03-01
    provides: Celery infrastructure, BaseIngestionTask, Secret Manager integration
  - phase: 01-04
    provides: Event model with TimescaleDB hypertables
  - phase: discovery
    provides: GDELT + ReliefWeb API characteristics

provides:
  - GDELT event ingestion (15-minute polling)
  - ReliefWeb humanitarian updates ingestion (daily polling)
  - Periodic Celery tasks with GCP Cloud Scheduler
  - Rate-limited API calls with exponential backoff
  - Event deduplication and normalization

affects: [04-risk-intelligence, 05-dashboard]

tech-stack:
  patterns: [periodic-tasks, rate-limiting, deduplication, api-polling]
  libraries: [gdeltPyR, tenacity, celery-beat]

confidence: high
estimated-completion: 20 min
---

# Plan 03-02: Real-Time Ingestion (GDELT + ReliefWeb)

**Implement real-time event ingestion from GDELT (15-min) and ReliefWeb (daily) with Celery periodic tasks, rate limiting, and deduplication**

## Context

GDELT and ReliefWeb are priority data sources for VenezuelaWatch's real-time intelligence:

**GDELT (Global Database of Events, Language, and Tone):**
- Update frequency: Real-time (15-minute intervals)
- Coverage: Global news events with geo-tagging, sentiment, themes
- Venezuela filter: `country:VE` or mention of Venezuela in article
- No authentication required
- Rate limit: ~250 requests per query (generous)
- Python library: gdeltPyR (official wrapper)
- Format: JSON/CSV with 60+ fields

**ReliefWeb:**
- Update frequency: Daily (humanitarian crisis updates)
- Coverage: UN OCHA reports, NGO updates, disaster info
- Venezuela filter: `country.iso3=VEN`
- Authentication: Appname header (simple)
- Rate limit: 1000 requests/day (sufficient for daily polling)
- Format: JSON with nested resources
- Python library: requests (simple REST API)

Both sources provide critical early-warning signals for political disruptions, humanitarian crises, and supply chain impacts.

## Goals

1. Implement GDELT ingestion task using gdeltPyR
2. Implement ReliefWeb ingestion task with rate limiting
3. Create Event model mappings for both sources
4. Set up periodic task scheduling with celery-beat
5. Implement deduplication logic to avoid duplicate events
6. Configure GCP Cloud Scheduler for production cron
7. Verify end-to-end ingestion pipeline

## Non-Goals

- Sentiment analysis (deferred to Phase 4: Risk Intelligence)
- Entity extraction from event text (deferred to Phase 6: Entity Watch)
- Real-time WebSocket push to frontend (deferred to Phase 5: Dashboard)
- Historical data backfill (can be done manually post-launch)

## Tasks

### Task 1: Install dependencies and create GDELT ingestion task

**Install packages:**
```bash
cd backend
pip install gdeltPyR==0.1.11 tenacity==9.0.0 celery[beat]==5.4.0
pip freeze | grep -E "gdeltPyR|tenacity|celery" >> requirements.txt
```

**Create GDELT ingestion task:**
- File: `backend/data_pipeline/tasks/gdelt_tasks.py`
- Import: `from gdeltPyR import gdelt; from data_pipeline.tasks.base import BaseIngestionTask`
- Create `@shared_task(base=BaseIngestionTask, bind=True)` decorator
- Function: `ingest_gdelt_events(self, lookback_minutes: int = 15)`
- Query GDELT for Venezuela-related events in last 15 minutes
- Use gdelt API: `gd = gdelt(version=2); articles = gd.Search(['Venezuela'], table='gkg', coverage=True, timespan='15m')`
- Parse response and create Event objects
- Return: `{'events_created': count, 'events_skipped': count}`

**Create Event mapping function:**
- File: `backend/data_pipeline/services/event_mapper.py`
- Function: `map_gdelt_to_event(gdelt_record: dict) -> Event`
- Map GDELT fields to Event model:
  - source = 'gdelt'
  - event_type = infer from THEMES field (political, economic, conflict, etc.)
  - title = first 200 chars of source article title
  - content = full article excerpt or summary
  - location = LOCATIONS field (Venezuela, Caracas, etc.)
  - timestamp = SEENDATE converted to UTC
  - metadata = full GDELT record as JSON
  - url = SOURCEURL
  - sentiment_score = TONE (positive/negative scale)

**Add deduplication:**
- Use Event.objects.filter(source='gdelt', url=gdelt_url).exists()
- If exists, skip creation and increment events_skipped counter
- Log skipped duplicates at DEBUG level

**Verification:**
```python
from data_pipeline.tasks.gdelt_tasks import ingest_gdelt_events
result = ingest_gdelt_events.delay()
result.get(timeout=60)  # Should return {'events_created': X, 'events_skipped': Y}
```

**Commit:**
```
feat(03-02): implement GDELT event ingestion with deduplication

Task 1: Install dependencies and create GDELT ingestion task
- Install gdeltPyR==0.1.11, tenacity==9.0.0, celery[beat]==5.4.0
- Create ingest_gdelt_events task with 15-minute lookback
- Query GDELT for Venezuela-related events using gdeltPyR
- Map GDELT fields to Event model (source, event_type, title, content, location, timestamp, metadata)
- Implement deduplication by URL to avoid duplicate events
- Return events_created and events_skipped metrics
- Verify task execution end-to-end
```

### Task 2: Create ReliefWeb ingestion task with rate limiting

**Create ReliefWeb ingestion task:**
- File: `backend/data_pipeline/tasks/reliefweb_tasks.py`
- Import: `import requests; from tenacity import retry, stop_after_attempt, wait_exponential`
- Create `@shared_task(base=BaseIngestionTask, bind=True)` decorator
- Function: `ingest_reliefweb_updates(self, lookback_days: int = 1)`
- Query ReliefWeb API: `https://api.reliefweb.int/v1/reports?appname=venezuelawatch&query[value]=country.iso3:VEN&filter[date][value]=<date>`
- Use tenacity retry: `@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))`
- Parse response and create Event objects
- Return: `{'events_created': count, 'events_skipped': count}`

**Create ReliefWeb Event mapping:**
- File: `backend/data_pipeline/services/event_mapper.py` (add function)
- Function: `map_reliefweb_to_event(report: dict) -> Event`
- Map ReliefWeb fields:
  - source = 'reliefweb'
  - event_type = 'humanitarian' (can be refined later)
  - title = report['fields']['title']
  - content = report['fields']['body'] or report['fields']['body-html']
  - location = 'Venezuela' (can extract from report['fields']['country'])
  - timestamp = report['fields']['date']['created'] (ISO 8601)
  - metadata = full report as JSON
  - url = report['fields']['url']
  - sentiment_score = null (ReliefWeb doesn't provide sentiment)

**Add rate limiting:**
- Use RateLimiter from data_pipeline/tasks/utils.py (created in Plan 03-01)
- Configure: 1000 requests/day = ~41 requests/hour = ~0.7 requests/minute
- Add sleep between batches if needed

**Add deduplication:**
- Use Event.objects.filter(source='reliefweb', url=report_url).exists()
- Skip if exists

**Verification:**
```python
from data_pipeline.tasks.reliefweb_tasks import ingest_reliefweb_updates
result = ingest_reliefweb_updates.delay()
result.get(timeout=60)  # Should return {'events_created': X, 'events_skipped': Y}
```

**Commit:**
```
feat(03-02): implement ReliefWeb ingestion with rate limiting

Task 2: Create ReliefWeb ingestion task with rate limiting
- Create ingest_reliefweb_updates task with 1-day lookback
- Query ReliefWeb API for Venezuela humanitarian reports
- Use tenacity for exponential backoff retry logic (3 attempts, 2-10s wait)
- Map ReliefWeb fields to Event model
- Implement deduplication by URL
- Add rate limiting for 1000 requests/day API quota
- Verify task execution end-to-end
```

### Task 3: Configure Celery Beat for periodic scheduling

**Install celery-beat:**
(Already installed in Task 1 with celery[beat])

**Create beat schedule:**
- File: `backend/config/settings.py` (or celery.py)
- Add CELERY_BEAT_SCHEDULE configuration:
  ```python
  CELERY_BEAT_SCHEDULE = {
      'ingest-gdelt-events': {
          'task': 'data_pipeline.tasks.gdelt_tasks.ingest_gdelt_events',
          'schedule': 900.0,  # 15 minutes in seconds
          'args': (15,),  # lookback_minutes
      },
      'ingest-reliefweb-updates': {
          'task': 'data_pipeline.tasks.reliefweb_tasks.ingest_reliefweb_updates',
          'schedule': 86400.0,  # 24 hours in seconds
          'args': (1,),  # lookback_days
      },
  }
  ```

**Test celery-beat locally:**
```bash
# Terminal 1: Start Celery worker
celery -A config worker --loglevel=info

# Terminal 2: Start Celery beat scheduler
celery -A config beat --loglevel=info
```

**Expected output:**
- Beat should log: "Scheduler: Sending due task ingest-gdelt-events"
- Worker should pick up task and execute

**Update README:**
- File: `backend/README.md`
- Add Celery Beat section:
  ```bash
  # Terminal 1: Redis
  redis-server

  # Terminal 2: Celery worker
  celery -A config worker --loglevel=info

  # Terminal 3: Celery beat (periodic task scheduler)
  celery -A config beat --loglevel=info

  # Terminal 4: Django dev server
  python manage.py runserver
  ```

**Commit:**
```
feat(03-02): configure Celery Beat for periodic task scheduling

Task 3: Configure Celery Beat for periodic scheduling
- Add CELERY_BEAT_SCHEDULE to settings
- Configure ingest-gdelt-events task (every 15 minutes)
- Configure ingest-reliefweb-updates task (every 24 hours)
- Update README with Celery Beat instructions
- Verify beat scheduler starts and dispatches tasks
```

### Task 4: Set up GCP Cloud Scheduler for production

**Create Cloud Scheduler jobs:**
```bash
# GDELT ingestion (every 15 minutes)
gcloud scheduler jobs create http gdelt-ingestion \
  --location=us-central1 \
  --schedule="*/15 * * * *" \
  --uri="https://venezuelawatch-api.run.app/api/tasks/trigger/gdelt" \
  --http-method=POST \
  --oidc-service-account-email=venezuelawatch-scheduler@venezuelawatch-staging.iam.gserviceaccount.com \
  --headers="Content-Type=application/json" \
  --message-body='{"lookback_minutes": 15}' \
  --project=venezuelawatch-staging

# ReliefWeb ingestion (daily at 9 AM UTC)
gcloud scheduler jobs create http reliefweb-ingestion \
  --location=us-central1 \
  --schedule="0 9 * * *" \
  --uri="https://venezuelawatch-api.run.app/api/tasks/trigger/reliefweb" \
  --http-method=POST \
  --oidc-service-account-email=venezuelawatch-scheduler@venezuelawatch-staging.iam.gserviceaccount.com \
  --headers="Content-Type=application/json" \
  --message-body='{"lookback_days": 1}' \
  --project=venezuelawatch-staging
```

**Create task trigger API endpoint:**
- File: `backend/data_pipeline/api.py`
- Create django-ninja router
- Endpoint: `POST /api/tasks/trigger/{task_name}`
- Verify OIDC token from Cloud Scheduler
- Dispatch task using `.delay()` method
- Return: `{"status": "dispatched", "task_id": task_id}`

**Add to main API:**
- File: `backend/config/urls.py`
- Include: `path('api/tasks/', include('data_pipeline.api'))`

**Create service account:**
```bash
gcloud iam service-accounts create venezuelawatch-scheduler \
  --display-name="VenezuelaWatch Cloud Scheduler" \
  --project=venezuelawatch-staging

gcloud projects add-iam-policy-binding venezuelawatch-staging \
  --member="serviceAccount:venezuelawatch-scheduler@venezuelawatch-staging.iam.gserviceaccount.com" \
  --role="roles/run.invoker"
```

**Update deployment docs:**
- File: `backend/docs/deployment/cloud-scheduler.md`
- Document Cloud Scheduler setup
- Document OIDC authentication pattern
- Document how to test triggers manually

**Commit:**
```
feat(03-02): set up GCP Cloud Scheduler for production ingestion

Task 4: Set up GCP Cloud Scheduler for production
- Create Cloud Scheduler jobs for GDELT (every 15min) and ReliefWeb (daily)
- Create task trigger API endpoint at POST /api/tasks/trigger/{task_name}
- Verify OIDC token from Cloud Scheduler service account
- Dispatch Celery tasks via HTTP triggers
- Create venezuelawatch-scheduler service account with run.invoker role
- Document Cloud Scheduler setup and testing
```

### checkpoint:human-verify

**Verify the following:**

1. **GDELT ingestion works:**
   ```bash
   python manage.py shell
   ```
   ```python
   from data_pipeline.tasks.gdelt_tasks import ingest_gdelt_events
   result = ingest_gdelt_events.delay()
   result.get(timeout=120)  # Should return {'events_created': X, 'events_skipped': Y}

   # Check events created
   from events.models import Event
   Event.objects.filter(source='gdelt').count()  # Should be > 0
   ```

2. **ReliefWeb ingestion works:**
   ```python
   from data_pipeline.tasks.reliefweb_tasks import ingest_reliefweb_updates
   result = ingest_reliefweb_updates.delay()
   result.get(timeout=120)

   # Check events created
   Event.objects.filter(source='reliefweb').count()  # Should be > 0
   ```

3. **Celery Beat scheduling works:**
   ```bash
   # Terminal 1: Start worker
   celery -A config worker --loglevel=info

   # Terminal 2: Start beat
   celery -A config beat --loglevel=info
   ```
   Expected: Beat logs "Sending due task ingest-gdelt-events" within 15 minutes

4. **Deduplication works:**
   ```python
   # Run same task twice
   ingest_gdelt_events.delay().get()
   result = ingest_gdelt_events.delay().get()
   # Second run should have events_skipped > 0
   ```

5. **Cloud Scheduler jobs created:**
   ```bash
   gcloud scheduler jobs list --location=us-central1
   ```
   Expected: See gdelt-ingestion and reliefweb-ingestion jobs

6. **Events have proper fields:**
   ```python
   event = Event.objects.filter(source='gdelt').first()
   print(event.title, event.content, event.timestamp, event.url)
   # Should see populated fields
   ```

**Type "approved" to continue or describe any issues.**

## Verification Criteria

- [ ] GDELT ingestion task creates Event objects successfully
- [ ] ReliefWeb ingestion task creates Event objects successfully
- [ ] Deduplication prevents duplicate events by URL
- [ ] Celery Beat dispatches tasks on schedule (15min for GDELT, daily for ReliefWeb)
- [ ] Rate limiting prevents API quota exhaustion
- [ ] Retry logic handles transient failures (3 attempts with exponential backoff)
- [ ] GCP Cloud Scheduler jobs created and configured
- [ ] Task trigger API endpoint responds to Cloud Scheduler
- [ ] Events have all required fields populated (source, title, content, timestamp, url)

## Dependencies

**Python packages:**
- gdeltPyR==0.1.11
- tenacity==9.0.0
- celery[beat]==5.4.0 (includes celery-beat)

**External APIs:**
- GDELT API (no auth required)
- ReliefWeb API (appname header: venezuelawatch)

**GCP resources:**
- Cloud Scheduler (2 jobs)
- Service account: venezuelawatch-scheduler@venezuelawatch-staging.iam.gserviceaccount.com

**Requires:**
- Plan 03-01 complete (Celery + Redis infrastructure)
- Event model with TimescaleDB hypertables (Plan 01-04)

## Risks

1. **GDELT rate limits**: ~250 requests/query is generous, but high-frequency polling (15min) could hit limits. Monitor and adjust if needed.
2. **ReliefWeb 1000 req/day quota**: Daily polling is safe, but manual backfills or testing could exhaust quota. Use caution.
3. **GDELT data volume**: Venezuela events could be 10-100 per 15min interval during crises. Monitor database growth.
4. **Cloud Scheduler HTTP triggers**: Requires publicly accessible API endpoint. Ensure OIDC auth is properly configured.
5. **Celery Beat vs Cloud Scheduler**: Local dev uses Celery Beat, production uses Cloud Scheduler. Ensure both are tested.

## Rollback Plan

If major issues occur:
1. Disable Cloud Scheduler jobs: `gcloud scheduler jobs pause gdelt-ingestion reliefweb-ingestion --location=us-central1`
2. Remove CELERY_BEAT_SCHEDULE from settings
3. Delete ingestion tasks: `rm backend/data_pipeline/tasks/gdelt_tasks.py reliefweb_tasks.py`
4. Delete Events: `Event.objects.filter(source__in=['gdelt', 'reliefweb']).delete()`
5. Uninstall: `pip uninstall gdeltPyR tenacity`

## Next Steps

After this plan completes:
- **Plan 03-03**: Implement FRED daily batch ingestion for economic indicators
- **Plan 03-04**: Implement UN Comtrade + World Bank monthly/quarterly ingestion
- **Phase 4**: Build risk intelligence engine on top of ingested events

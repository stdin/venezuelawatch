---
phase: 22-data-source-architecture
plan: 02
type: execute
---

<objective>
Refactor GDELT sync as reference implementation of the adapter pattern.

Purpose: Convert existing GDELT BigQuery sync into a clean, well-documented GdeltAdapter that demonstrates best practices for all future data source integrations. This is the blueprint other developers will follow.

Output: GdeltAdapter implementing DataSourceAdapter, comprehensive documentation, passing integration test proving behavior unchanged.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-data-source-architecture/22-CONTEXT.md
@.planning/phases/22-data-source-architecture/22-01-SUMMARY.md
@backend/data_pipeline/tasks/gdelt_sync_task.py
@backend/api/services/gdelt_bigquery_service.py
@backend/api/services/gdelt_gkg_service.py
@backend/api/services/gdelt_gkg_parsers.py

**Tech stack available from 22-01:**
- DataSourceAdapter ABC with fetch/transform/validate interface
- AdapterRegistry with convention-based discovery
- publish_events() helper method for Pub/Sub integration

**Established patterns:**
- Phase 20: GKG enrichment with themes/entities/tone parsing
- Phase 19: All 61 GDELT BigQuery fields stored in metadata
- Phase 18: Pub/Sub publishing instead of direct Celery dispatch
- Phase 14.2: GDELT native BigQuery as primary data source

**Constraining decisions:**
- Phase 20: GKG parser functions in separate module for testability
- Phase 20: Jaro-Winkler 0.85 threshold for entity deduplication
- Phase 19: Store all GDELT fields in metadata JSON
- Phase 14.2: Query gdelt-bq.gdeltv2.events_partitioned with Venezuela filters

**From 22-CONTEXT.md essential requirements:**
- Zero behavior change for GDELT (same data, frequency, results)
- Reference implementation quality (clear docstrings, inline comments, type hints)
- Comments explain GDELT-specific quirks vs what other adapters might do differently
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GdeltAdapter implementing DataSourceAdapter</name>
  <files>backend/data_pipeline/adapters/gdelt_adapter.py</files>
  <action>
Extract logic from gdelt_sync_task.py and refactor into GdeltAdapter class.

**File: backend/data_pipeline/adapters/gdelt_adapter.py**

Create class GdeltAdapter(DataSourceAdapter) with:

**Class attributes:**
```python
source_name = "GDELT"
schedule_frequency = "*/15 * * * *"  # Every 15 minutes
default_lookback_minutes = 15
```

**fetch() method:**
- Accept start_time, end_time, limit parameters
- Use gdelt_bigquery_service.get_venezuela_events() to query GDELT native BigQuery
- Enrich with GKG data using gdelt_gkg_service and gkg_parsers (themes, entities, tone)
- Return list of raw GDELT event dicts with gkg_parsed field if available
- Add docstring: "Fetches Venezuela events from GDELT BigQuery and enriches with GKG data. Other adapters might fetch from REST APIs or CSV files instead."

**transform() method:**
- Accept list of raw GDELT event dicts
- Map each to BigQueryEvent using same logic from gdelt_sync_task.py:
  - Parse DATEADDED to mentioned_at
  - Generate title from Actor1Name/Actor2Name/EventCode
  - Map QuadClass to event_type (political/other)
  - Store all 61 fields in metadata JSON (goldstein_scale, avg_tone, actors, geo, religion, ethnicity, etc.)
  - Include gkg_parsed in metadata.gkg if present
- Return list of BigQueryEvent objects
- Add docstring: "Maps GDELT schema to BigQuery Event schema. GDELT-specific: stores 61 native fields in metadata. Other adapters would map their own schemas here."

**validate() method:**
- Check required fields: id, source_url, mentioned_at, title must be non-empty
- Check for duplicate using BigQuery query (same logic as gdelt_sync_task.py)
- Return (True, None) if valid, (False, error_message) if invalid
- Add docstring: "Validates event completeness and checks for duplicates. GDELT uses GLOBALEVENTID for deduplication. Other adapters might use URL or composite keys."

**Import dependencies:**
- Import gdelt_bigquery_service from api.services.gdelt_bigquery_service
- Import gdelt_gkg_service from api.services.gdelt_gkg_service
- Import gkg parsers from api.services.gdelt_gkg_parsers
- Import BigQueryEvent from api.bigquery_models
- Import bigquery_service for duplicate checks

**Reference implementation quality:**
- Comprehensive docstrings on class and every method
- Type hints on all method signatures
- Inline comments explaining GDELT-specific choices vs general patterns
- Example: "# GDELT-specific: QuadClass 1-4 all map to 'political'. Other sources might have more granular types."

Preserve exact behavior from gdelt_sync_task.py - this is a refactor, not a feature change. The only difference is architectural (adapter pattern vs monolithic task).
  </action>
  <verify>
python -c "
from data_pipeline.adapters.gdelt_adapter import GdeltAdapter
from data_pipeline.adapters.base import DataSourceAdapter
print('Inherits from DataSourceAdapter:', issubclass(GdeltAdapter, DataSourceAdapter))
print('Class attributes:', GdeltAdapter.source_name, GdeltAdapter.schedule_frequency)
adapter = GdeltAdapter()
print('Has methods:', hasattr(adapter, 'fetch'), hasattr(adapter, 'transform'), hasattr(adapter, 'validate'))
"

Should confirm inheritance, class attributes, and methods exist.
  </verify>
  <done>
- GdeltAdapter class inherits from DataSourceAdapter
- Class attributes set (source_name, schedule_frequency, default_lookback_minutes)
- fetch() method queries GDELT BigQuery and enriches with GKG
- transform() method maps to BigQueryEvent preserving all 61 fields
- validate() method checks completeness and duplicates
- Comprehensive docstrings with GDELT-specific vs general pattern notes
- Type hints throughout
- Behavior identical to gdelt_sync_task.py (refactor only)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add comprehensive documentation and type hints</name>
  <files>backend/data_pipeline/adapters/gdelt_adapter.py</files>
  <action>
Enhance GdeltAdapter with reference-quality documentation.

**Module-level docstring:**
```python
"""
GDELT BigQuery Adapter - Reference Implementation

This adapter demonstrates the pattern for integrating external data sources.
It fetches Venezuela-related events from GDELT native BigQuery, enriches with
GKG metadata, and publishes to our time-series analytics pipeline.

**For developers adding new data sources:**
- Copy this file as a template (e.g., reliefweb_adapter.py)
- Follow the naming convention: {Source}Adapter class in {source}_adapter.py
- Implement fetch/transform/validate methods per DataSourceAdapter contract
- See inline comments marked "GDELT-specific" vs "General pattern"

**GDELT-specific design choices:**
- Queries gdelt-bq.gdeltv2.events_partitioned (Google's public dataset)
- Enriches with GKG data (themes, entities, tone) for richer intelligence
- Stores all 61 GDELT fields in metadata JSON for future analysis
- Uses GLOBALEVENTID for deduplication (other sources might use URL or composite keys)
- Maps QuadClass 1-4 to 'political' event type (other sources have different taxonomies)

**Dependencies:**
- GDELT native BigQuery access (no API key needed)
- GCP BigQuery client for queries
- GKG parser utilities for theme/entity extraction
"""
```

**Method docstrings with examples:**
Each method should have:
- One-line summary
- Detailed explanation of parameters and return value
- Notes on GDELT-specific vs general approach
- Example usage if helpful

**Type hints refinement:**
- Add return type annotations: `-> List[Dict[str, Any]]` for fetch
- Add parameter type annotations for all args
- Use `from typing import List, Dict, Any, Tuple, Optional`

**Inline comments for GDELT-specific quirks:**
```python
# GDELT-specific: Parse DATEADDED (YYYYMMDDHHMMSS format)
# Other adapters might receive ISO 8601 or Unix timestamps

# GDELT-specific: QuadClass categorizes as verbal/material cooperation/conflict
# Other sources might use different event taxonomies (e.g., ACLED has 9 event types)

# GDELT-specific: GKG enrichment is optional but highly valuable
# Other sources might not have equivalent metadata layers
```

Focus on making this THE example that future adapter authors will reference. Every design choice should be either obviously correct or explicitly explained.
  </action>
  <verify>
python -c "
from data_pipeline.adapters.gdelt_adapter import GdeltAdapter
import inspect
print('Module docstring length:', len(GdeltAdapter.__doc__ or ''))
print('fetch docstring:', bool(GdeltAdapter.fetch.__doc__))
print('transform docstring:', bool(GdeltAdapter.transform.__doc__))
print('validate docstring:', bool(GdeltAdapter.validate.__doc__))
# Check type hints
sig = inspect.signature(GdeltAdapter.fetch)
print('fetch has return annotation:', sig.return_annotation != inspect.Parameter.empty)
"

Module docstring should be 500+ chars, all methods documented, type hints present.
  </verify>
  <done>
- Module-level docstring (500+ chars) explains GDELT adapter as reference implementation
- Docstring includes guidance for developers adding new sources
- Every method has comprehensive docstring (summary, params, returns, notes)
- Type hints on all method signatures with return annotations
- Inline comments explain GDELT-specific vs general patterns
- Comments identify where other adapters would differ
- Documentation quality suitable as blueprint for future integrations
  </done>
</task>

<task type="auto">
  <name>Task 3: Integration test with real BigQuery</name>
  <files>backend/data_pipeline/adapters/tests/__init__.py, backend/data_pipeline/adapters/tests/test_gdelt_adapter.py</files>
  <action>
Create integration test that validates GdeltAdapter behavior against real BigQuery.

**File: backend/data_pipeline/adapters/tests/test_gdelt_adapter.py**

Create Django test case with:

**Test setup:**
- Use Django TestCase for database access
- Requires GCP credentials (skip if not available using unittest.skipIf)
- Import GdeltAdapter, BigQueryEvent, bigquery_service

**Test: test_gdelt_adapter_full_pipeline**
```python
def test_gdelt_adapter_full_pipeline(self):
    """Test full fetch -> transform -> validate cycle with real GDELT data."""
    adapter = GdeltAdapter()

    # Fetch events from last 24 hours (small time window for fast test)
    end_time = timezone.now()
    start_time = end_time - timedelta(hours=24)

    raw_events = adapter.fetch(start_time, end_time, limit=10)
    self.assertIsInstance(raw_events, list)
    self.assertGreater(len(raw_events), 0, "Should fetch at least 1 Venezuela event in 24h")

    # Transform to BigQuery schema
    bq_events = adapter.transform(raw_events)
    self.assertEqual(len(bq_events), len(raw_events))

    # Validate first event
    valid, error = adapter.validate(bq_events[0])
    if not valid:
        # Duplicate is OK for test (event might exist from production)
        self.assertIn("duplicate", error.lower())

    # Verify schema completeness
    event = bq_events[0]
    self.assertIsNotNone(event.id)
    self.assertIsNotNone(event.title)
    self.assertEqual(event.source_name, "GDELT")
    self.assertIn('goldstein_scale', event.metadata)
    self.assertIn('avg_tone', event.metadata)

    # If GKG enrichment worked, check for themes/entities
    if 'gkg' in event.metadata and event.metadata['gkg']:
        gkg = event.metadata['gkg']
        self.assertIn('themes', gkg)
        self.assertIn('tone', gkg)
```

**Test: test_adapter_discovery**
```python
def test_adapter_discovery(self):
    """Test that GdeltAdapter is discovered by registry."""
    from data_pipeline.adapters.registry import adapter_registry

    adapters = adapter_registry.list_adapters()
    self.assertIn("GDELT", adapters)

    gdelt_adapter_class = adapter_registry.get_adapter("GDELT")
    self.assertEqual(gdelt_adapter_class, GdeltAdapter)

    metadata = adapter_registry.get_metadata("GDELT")
    self.assertEqual(metadata['schedule_frequency'], "*/15 * * * *")
```

Use `@unittest.skipIf(not os.getenv('GOOGLE_APPLICATION_CREDENTIALS'), "GCP credentials required")` to skip if no credentials.

This test proves:
1. Adapter fetches real GDELT data
2. Transform produces valid BigQueryEvent objects
3. Validate catches issues (or accepts valid events)
4. Registry discovers adapter correctly
5. Behavior matches original gdelt_sync_task.py
  </action>
  <verify>
cd backend && python manage.py test data_pipeline.adapters.tests.test_gdelt_adapter --verbosity=2

Tests should pass (or skip if no GCP credentials). If credentials available, should fetch 1+ events in 24h window.
  </verify>
  <done>
- Integration test file created with Django TestCase
- test_gdelt_adapter_full_pipeline tests fetch/transform/validate cycle
- Test uses real BigQuery (24-hour lookback for speed)
- test_adapter_discovery verifies registry integration
- Tests validate schema completeness and GKG enrichment
- Skip decorator for missing GCP credentials
- Tests pass proving behavior unchanged from gdelt_sync_task.py
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from data_pipeline.adapters.gdelt_adapter import GdeltAdapter"` succeeds
- [ ] GdeltAdapter discovered in registry: `adapter_registry.list_adapters()` includes "GDELT"
- [ ] Integration tests pass (or skip if no credentials)
- [ ] Docstrings comprehensive on class and all methods
- [ ] Type hints present throughout
- [ ] Inline comments explain GDELT-specific vs general patterns
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- GdeltAdapter implements DataSourceAdapter correctly
- Behavior identical to original gdelt_sync_task.py (refactor only)
- Reference-quality documentation suitable as blueprint
- Integration test proves real-world functionality
- Ready for Cloud Function migration in 22-03
</success_criteria>

<output>
After completion, create `.planning/phases/22-data-source-architecture/22-02-SUMMARY.md`
</output>

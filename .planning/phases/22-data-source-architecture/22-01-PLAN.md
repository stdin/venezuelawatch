---
phase: 22-data-source-architecture
plan: 01
type: execute
---

<objective>
Create the foundational adapter architecture for pluggable data sources.

Purpose: Establish an extensible plugin pattern that makes adding new data sources (ReliefWeb, World Bank, etc.) straightforward and consistent. This phase builds the abstraction layer that all data sources will implement.

Output: Abstract base class `DataSourceAdapter` with clear contracts, and `AdapterRegistry` with convention-based discovery system.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/22-data-source-architecture/22-CONTEXT.md

**Tech stack available:**
- Python 3.11 with abc module for abstract base classes
- Type hints and dataclasses for type safety
- Django project structure in backend/

**Established patterns:**
- Phase 18: Cloud Functions Gen2 with standalone functions, no Django dependencies
- Phase 14.3: Polyglot persistence (PostgreSQL + BigQuery)
- Convention-based discovery used throughout Django (apps, management commands)

**Constraining decisions:**
- Phase 18: Pub/Sub event publishing instead of Celery task dispatch
- Phase 18: Internal API handlers with OIDC authentication
- Phase 14.3: BigQuery as primary time-series storage

**From 22-CONTEXT.md vision:**
- Plugin-style adapters with fetch/transform/validate interface
- Convention-based discovery (drop file in adapters/, follow naming, auto-discovered)
- GDELT as reference implementation with comprehensive docs
- Decoupled data flow: adapters validate then publish to Pub/Sub
- Full observability: registry tracks adapter metadata and health
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DataSourceAdapter abstract base class</name>
  <files>backend/data_pipeline/adapters/__init__.py, backend/data_pipeline/adapters/base.py</files>
  <action>
Create abstract base class using Python ABC module with comprehensive type hints.

**File: backend/data_pipeline/adapters/base.py**

Define abstract methods:
- `fetch(start_time: datetime, end_time: datetime, limit: int) -> List[Dict[str, Any]]` - fetch raw data from source
- `transform(raw_events: List[Dict]) -> List[BigQueryEvent]` - map to BigQuery schema
- `validate(event: BigQueryEvent) -> Tuple[bool, Optional[str]]` - validate single event (return (True, None) or (False, error_msg))

Define class attributes (overridden by implementations):
- `source_name: str` - human-readable name
- `schedule_frequency: str` - cron expression for Cloud Scheduler
- `default_lookback_minutes: int` - how far back to query by default

Add helper method (concrete, not abstract):
- `publish_events(events: List[BigQueryEvent]) -> Dict[str, int]` - batch publish to Pub/Sub, returns {published: int, failed: int}

Include comprehensive docstrings explaining:
- What each method does
- How implementations should override
- Example usage pattern
- Why validation is separate from transform (partial success publishing)

**File: backend/data_pipeline/adapters/__init__.py**

Export DataSourceAdapter for clean imports.

Use `from abc import ABC, abstractmethod` for abstract methods. Use `from typing import List, Dict, Any, Tuple, Optional` for type hints. Import BigQueryEvent from `api.bigquery_models`.
  </action>
  <verify>
python -c "from data_pipeline.adapters.base import DataSourceAdapter; import inspect; print('Abstract methods:', [m for m in dir(DataSourceAdapter) if getattr(getattr(DataSourceAdapter, m), '__isabstractmethod__', False)])"

Should show: fetch, transform, validate
  </verify>
  <done>
- DataSourceAdapter ABC exists with 3 abstract methods (fetch, transform, validate)
- Class attributes defined (source_name, schedule_frequency, default_lookback_minutes)
- Helper method publish_events implemented
- Comprehensive docstrings on class and all methods
- Type hints throughout
- Imports succeed without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Build AdapterRegistry with convention-based discovery</name>
  <files>backend/data_pipeline/adapters/registry.py</files>
  <action>
Create adapter registry that discovers adapters by convention.

**File: backend/data_pipeline/adapters/registry.py**

Implement singleton AdapterRegistry class with:

**Discovery method:**
- `discover_adapters() -> Dict[str, Type[DataSourceAdapter]]` - scan `data_pipeline/adapters/` for files matching `*_adapter.py`
- For each file, look for class matching `{Source}Adapter` (e.g., gdelt_adapter.py → GdeltAdapter)
- Verify class inherits from DataSourceAdapter
- Store in registry dict keyed by source_name class attribute
- Log discovered adapters (INFO level)
- Skip base.py and files without matching class

**Registry access methods:**
- `get_adapter(source_name: str) -> Optional[Type[DataSourceAdapter]]` - retrieve adapter class by name
- `list_adapters() -> List[str]` - return all registered adapter names
- `get_metadata(source_name: str) -> Dict[str, Any]` - return {source_name, schedule_frequency, default_lookback_minutes} from class attributes

**Health tracking (simple, not persistent):**
- `record_run(source_name: str, success: bool, events_count: int, duration_ms: int)` - update in-memory metrics
- `get_health(source_name: str) -> Dict[str, Any]` - return {last_run: datetime, last_success: bool, total_runs: int, success_rate: float}

Use module `inspect` to find classes. Use `importlib` to dynamically import adapter modules. Initialize registry as module-level singleton: `adapter_registry = AdapterRegistry()` that auto-discovers on import.

Convention violations should log warnings but not crash (graceful degradation). Missing adapter files just means empty registry, not an error.

Include docstrings explaining the discovery conventions clearly - this is THE reference for how to add new adapters.
  </action>
  <verify>
# Create dummy adapter to test discovery
mkdir -p backend/data_pipeline/adapters
cat > backend/data_pipeline/adapters/test_adapter.py << 'EOF'
from data_pipeline.adapters.base import DataSourceAdapter
class TestAdapter(DataSourceAdapter):
    source_name = "test"
    schedule_frequency = "*/15 * * * *"
    default_lookback_minutes = 15
    def fetch(self, *args): return []
    def transform(self, *args): return []
    def validate(self, *args): return (True, None)
EOF

python -c "from data_pipeline.adapters.registry import adapter_registry; print('Discovered:', adapter_registry.list_adapters()); print('Has test:', 'test' in adapter_registry.list_adapters())"

rm backend/data_pipeline/adapters/test_adapter.py

Should discover test adapter, then clean up.
  </verify>
  <done>
- AdapterRegistry class with singleton instance
- discover_adapters() scans for *_adapter.py files
- Convention: file {source}_adapter.py → class {Source}Adapter
- get_adapter(), list_adapters(), get_metadata() methods work
- Health tracking methods implemented (in-memory)
- Comprehensive docstrings on discovery conventions
- Graceful handling of missing/invalid adapters
- Test discovers dummy adapter successfully
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from data_pipeline.adapters.base import DataSourceAdapter"` succeeds
- [ ] `python -c "from data_pipeline.adapters.registry import adapter_registry"` succeeds
- [ ] Abstract methods verified: fetch, transform, validate
- [ ] Registry can discover and list adapters
- [ ] No import errors or type hint issues
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- DataSourceAdapter ABC defines clear contract
- AdapterRegistry discovers adapters by convention
- Documentation in docstrings explains how to add new adapters
- Foundation ready for GDELT refactor in 22-02
</success_criteria>

<output>
After completion, create `.planning/phases/22-data-source-architecture/22-01-SUMMARY.md`
</output>

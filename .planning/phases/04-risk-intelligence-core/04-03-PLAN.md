---
phase: 04-risk-intelligence-core
plan: 03
type: execute
---

<objective>
Implement event severity classification (SEV 1-5) for dashboard prioritization using NCISS-style weighted criteria assessment.

Purpose: Enable users to filter/sort events by severity level (Critical, High, Medium, Low, Minimal) independent of risk score.
Output: impact_classifier.py service, severity field added to Event model, integration into intelligence pipeline.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-risk-intelligence-core/04-RESEARCH.md
@.planning/phases/04-risk-intelligence-core/04-02-SUMMARY.md
@backend/data_pipeline/services/llm_intelligence.py
@backend/data_pipeline/services/llm_client.py
@backend/data_pipeline/tasks/intelligence_tasks.py
@backend/core/models.py

**Tech stack available:**
- LLM intelligence with structured analysis capabilities
- Event model with llm_analysis JSONField
- Celery tasks for async processing

**Established patterns:**
- LLMIntelligence.analyze_event_comprehensive() for structured LLM queries
- LLMClient for direct LLM API calls with schema enforcement
- Weighted scoring pattern from RiskAggregator (Plan 04-02)

**Constraining decisions (from RESEARCH):**
- Use NCISS pattern: weighted multi-criteria scoring (scope + duration + reversibility + economic impact)
- Map 0.0-1.0 severity score to SEV1-5 levels with clear thresholds
- Use LLM to assess criteria (not keyword matching) for context awareness
- Weights: scope=0.35, duration=0.25, reversibility=0.20, economic_impact=0.20

**Research findings:**
- NCISS (CISA) used for national cyber incident severity scoring - proven methodology
- SEV 1 (Critical): ≥0.80 score, SEV 2 (High): ≥0.60, SEV 3 (Medium): ≥0.40, SEV 4 (Low): ≥0.20, SEV 5 (Minimal): <0.20
- Severity classification independent of risk score (severity = impact, risk = probability × impact)
- Common pitfall: Using simple thresholds instead of multi-criteria assessment
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement impact_classifier.py with NCISS-style severity scoring</name>
  <files>backend/data_pipeline/services/impact_classifier.py</files>
  <action>
    Create backend/data_pipeline/services/impact_classifier.py implementing ImpactClassifier class:

    ```python
    from data_pipeline.services.llm_client import LLMClient

    class ImpactClassifier:
        """
        Event severity classification using NCISS-style weighted scoring.

        Classifies events into SEV1 (Critical) to SEV5 (Minimal) based on
        multi-criteria assessment: scope, duration, reversibility, economic impact.
        """

        # Criteria weights (MUST sum to 1.0)
        WEIGHTS = {
            'scope': 0.35,              # Geographic/population reach
            'duration': 0.25,           # Time to resolve/impact
            'reversibility': 0.20,      # Can it be undone
            'economic_impact': 0.20,    # Financial cost/disruption
        }

        # Severity thresholds (0.0-1.0 score → SEV level)
        SEVERITY_THRESHOLDS = [
            (0.80, 'SEV1_CRITICAL'),
            (0.60, 'SEV2_HIGH'),
            (0.40, 'SEV3_MEDIUM'),
            (0.20, 'SEV4_LOW'),
            (0.00, 'SEV5_MINIMAL'),
        ]

        @classmethod
        def classify_severity(cls, event: Event) -> str:
            """
            Classify event severity using LLM-extracted criteria.

            Args:
                event: Event instance with title and content

            Returns:
                Severity level: 'SEV1_CRITICAL', 'SEV2_HIGH', 'SEV3_MEDIUM', 'SEV4_LOW', 'SEV5_MINIMAL'
            """
            # Extract criteria using LLM
            criteria = cls._extract_severity_criteria(event)

            # Calculate weighted severity score
            severity_score = sum(
                criteria.get(criterion, 0.5) * weight
                for criterion, weight in cls.WEIGHTS.items()
            )

            # Map to SEV level
            return cls._score_to_severity(severity_score)

        @classmethod
        def _extract_severity_criteria(cls, event: Event) -> Dict[str, float]:
            """
            Use LLM to assess severity criteria for event.

            Returns:
                {
                    'scope': float (0.0=local, 0.5=national, 1.0=international),
                    'duration': float (0.0=hours, 0.5=weeks, 1.0=months/permanent),
                    'reversibility': float (0.0=easily reversed, 1.0=irreversible),
                    'economic_impact': float (0.0=minimal, 0.5=moderate, 1.0=major disruption)
                }
            """
            llm = LLMClient()

            prompt = f"""Analyze this Venezuela event and rate each severity criterion on a 0.0-1.0 scale:

Event: {event.title}
Description: {event.content.get('description', '') if isinstance(event.content, dict) else ''}

Criteria:
1. **Scope** (geographic/population reach):
   - 0.0 = Local (city/region)
   - 0.5 = National (Venezuela-wide)
   - 1.0 = International (affects other countries/global markets)

2. **Duration** (time to resolve or impact persists):
   - 0.0 = Hours to days
   - 0.5 = Weeks to months
   - 1.0 = Months to years or permanent

3. **Reversibility** (can damage/changes be undone):
   - 0.0 = Easily reversed (policy can change back)
   - 0.5 = Difficult to reverse (structural changes needed)
   - 1.0 = Irreversible (deaths, destroyed infrastructure, permanent shifts)

4. **Economic Impact** (financial cost/disruption):
   - 0.0 = Minimal (<$1M or negligible disruption)
   - 0.5 = Moderate ($1M-$100M or sector disruption)
   - 1.0 = Major (>$100M or economy-wide disruption)

Return ONLY valid JSON (no markdown):
{{"scope": X.X, "duration": X.X, "reversibility": X.X, "economic_impact": X.X}}"""

            try:
                criteria = llm.extract_json_structured(
                    prompt,
                    expected_fields=['scope', 'duration', 'reversibility', 'economic_impact']
                )
                return criteria
            except Exception as e:
                logger.warning(f"LLM severity extraction failed for Event {event.id}: {e}")
                # Fallback to medium severity (0.5) for all criteria
                return {criterion: 0.5 for criterion in cls.WEIGHTS.keys()}

        @staticmethod
        def _score_to_severity(score: float) -> str:
            """Map 0.0-1.0 score to SEV1-5 level."""
            for threshold, level in ImpactClassifier.SEVERITY_THRESHOLDS:
                if score >= threshold:
                    return level
            return 'SEV5_MINIMAL'
    ```

    **What to avoid and WHY:** Don't use keyword matching for severity ("crisis" → SEV1). Context matters - "economic crisis" could be SEV2-4 depending on actual impact. LLM assesses context.
  </action>
  <verify>python manage.py shell: from data_pipeline.services.impact_classifier import ImpactClassifier; from core.models import Event; event = Event.objects.first(); severity = ImpactClassifier.classify_severity(event); assert severity in ['SEV1_CRITICAL', 'SEV2_HIGH', 'SEV3_MEDIUM', 'SEV4_LOW', 'SEV5_MINIMAL']; assert sum(ImpactClassifier.WEIGHTS.values()) == 1.0</verify>
  <done>impact_classifier.py exists with ImpactClassifier class, classify_severity() returns SEV1-5 levels, _extract_severity_criteria() uses LLM for context-aware assessment, weights sum to 1.0, fallback to medium severity on LLM errors</done>
</task>

<task type="auto">
  <name>Task 2: Add severity field to Event model and create migration</name>
  <files>backend/core/models.py, backend/core/migrations/000X_add_severity.py</files>
  <action>
    1. Add severity field to Event model in backend/core/models.py:
       ```python
       class Event(models.Model):
           # ... existing fields ...

           # Severity classification (SEV 1-5)
           severity = models.CharField(
               max_length=20,
               blank=True,
               null=True,
               db_index=True,
               choices=[
                   ('SEV1_CRITICAL', 'SEV1 - Critical'),
                   ('SEV2_HIGH', 'SEV2 - High'),
                   ('SEV3_MEDIUM', 'SEV3 - Medium'),
                   ('SEV4_LOW', 'SEV4 - Low'),
                   ('SEV5_MINIMAL', 'SEV5 - Minimal'),
               ],
               help_text="Event severity classification (SEV1=Critical to SEV5=Minimal)"
           )

           # ... rest of model ...
       ```

    2. Create migration:
       ```bash
       python manage.py makemigrations -n add_severity
       ```

    3. Update Event.__str__() method to optionally show severity for debugging

    4. Update admin.py list_display to include severity field for filtering

    **What to avoid and WHY:** Don't use integer severity (1-5) - string choices ('SEV1_CRITICAL') are self-documenting and prevent confusion about ordering (is 1 high or low?).
  </action>
  <verify>python manage.py makemigrations shows new migration created, python manage.py check passes, python manage.py showmigrations shows add_severity migration pending</verify>
  <done>Event model has severity field with choices, migration created, admin.py updated to display severity, field indexed for dashboard filtering</done>
</task>

<task type="auto">
  <name>Task 3: Integrate severity classification into intelligence workflow</name>
  <files>backend/data_pipeline/tasks/intelligence_tasks.py</files>
  <action>
    Update intelligence analysis pipeline to classify severity:

    1. In analyze_event_intelligence() task:
       ```python
       # After LLM analysis and risk scoring
       from data_pipeline.services.impact_classifier import ImpactClassifier

       # Classify severity
       severity = ImpactClassifier.classify_severity(event)
       event.severity = severity
       event.save()
       ```

    2. Add batch severity classification task for existing events:
       ```python
       @shared_task(bind=True, name='batch_classify_severity')
       def batch_classify_severity(self, lookback_days=30):
           """
           Classify severity for recent events using ImpactClassifier.
           Useful after adding severity feature to existing events.
           """
           cutoff_date = timezone.now() - timedelta(days=lookback_days)
           events = Event.objects.filter(
               created_at__gte=cutoff_date,
               severity__isnull=True  # Only events without severity
           )

           classified_count = 0
           severity_counts = defaultdict(int)

           for event in events:
               severity = ImpactClassifier.classify_severity(event)
               event.severity = severity
               event.save()

               classified_count += 1
               severity_counts[severity] += 1

           return {
               "classified": classified_count,
               "distribution": dict(severity_counts)
           }
       ```

    3. Register batch_classify_severity in __init__.py

    4. Update batch_analyze_events() to include severity classification

    **What to avoid and WHY:** Don't classify severity without event content (title-only events). Guard with content checks, fallback to SEV3_MEDIUM if insufficient data.
  </action>
  <verify>python manage.py shell: from data_pipeline.tasks.intelligence_tasks import batch_classify_severity; result = batch_classify_severity.delay(lookback_days=7); result.get(); assert result.result['classified'] > 0; assert 'SEV1_CRITICAL' in result.result['distribution'] or 'SEV2_HIGH' in result.result['distribution']</verify>
  <done>intelligence_tasks.py integrates ImpactClassifier, severity set during LLM analysis, batch_classify_severity task created, task registered in __init__.py, batch_analyze_events includes severity classification</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] python manage.py check passes
- [ ] python manage.py migrate applies add_severity migration successfully
- [ ] Test severity classification: Create test event with major impact (e.g., "Maduro arrested"), classify_severity() returns SEV1 or SEV2
- [ ] Test severity distribution: Run batch_classify_severity on sample events, verify distribution shows range of SEV levels (not all SEV3)
- [ ] Test LLM criteria extraction: Verify _extract_severity_criteria() returns all 4 criteria with values 0.0-1.0
- [ ] Test fallback: Simulate LLM error, verify classify_severity() returns SEV3_MEDIUM (not crash)
- [ ] Verify severity index exists: Check migration includes db_index=True for dashboard filtering performance
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- ImpactClassifier implements NCISS-style multi-criteria severity scoring
- Event model has severity field with SEV1-5 choices
- Migration created and applies successfully
- Severity classification integrated into intelligence pipeline
- Batch classification task available for existing events
- LLM fallback prevents classification failures
- Severity field indexed for dashboard queries
</success_criteria>

<output>
After completion, create `.planning/phases/04-risk-intelligence-core/04-03-SUMMARY.md`:

# Phase 4 Plan 3: Event Severity Classification Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description
- `path/to/another.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 04-04-PLAN.md (Risk Intelligence API & Dashboard Integration)
</output>

---
phase: 18-gcp-native-pipeline-migration
plan: 01
type: execute
---

<objective>
Migrate all scheduled data ingestion tasks from Celery Beat to Cloud Scheduler + Cloud Functions for serverless orchestration.

Purpose: Eliminate Redis/Celery dependency for scheduled ingestion, enable independent function deployments, and reduce operational overhead through GCP-native auto-scaling.
Output: 6 deployed Cloud Functions with Cloud Scheduler triggers matching current ingestion frequencies (GDELT 15min, ReliefWeb daily, FRED daily, Comtrade monthly, World Bank quarterly, Sanctions daily).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/GCP-NATIVE-ORCHESTRATION-RESEARCH.md
@.planning/phases/18-gcp-native-pipeline-migration/18-CONTEXT.md

**Current ingestion tasks (Celery):**
@backend/data_pipeline/tasks/gdelt_sync_task.py
@backend/data_pipeline/tasks/reliefweb_tasks.py
@backend/data_pipeline/tasks/fred_tasks.py
@backend/data_pipeline/tasks/comtrade_tasks.py
@backend/data_pipeline/tasks/worldbank_tasks.py
@backend/data_pipeline/tasks/sanctions_tasks.py

**Key decisions from research:**
- Cloud Functions (not Cloud Run Jobs) for <5min execution time, predictable workloads
- Reuse 90% of existing task logic (same BigQuery clients, same retry patterns)
- Parallel validation period before cutover (zero data loss requirement)
- Cloud Scheduler OIDC auth (not API keys) for security
- functions-framework for local testing before deployment

**Architectural constraint:** Big-bang cutover philosophy means deploy all functions first, validate in parallel, then disable Celery in Plan 18-03. No gradual migration.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Cloud Functions for all 6 ingestion tasks</name>
  <files>functions/gdelt_sync/main.py, functions/gdelt_sync/requirements.txt, functions/reliefweb/main.py, functions/reliefweb/requirements.txt, functions/fred/main.py, functions/fred/requirements.txt, functions/comtrade/main.py, functions/comtrade/requirements.txt, functions/worldbank/main.py, functions/worldbank/requirements.txt, functions/sanctions/main.py, functions/sanctions/requirements.txt, functions/shared/bigquery_client.py, functions/shared/secrets.py</files>
  <action>
Create `functions/` directory at project root with 6 subdirectories for each ingestion task. Each function:

**Structure:**
- main.py with @functions_framework.http decorator
- requirements.txt with google-cloud-bigquery, google-cloud-secret-manager, functions-framework
- Reuse existing service layer code from backend/ (import via shared module or copy utils)

**GDELT (functions/gdelt_sync/main.py):**
- HTTP trigger (Cloud Scheduler POST)
- Parse lookback_minutes from request JSON (default: 15)
- Call gdelt_bigquery_service.get_venezuela_events()
- Insert to BigQuery events table
- Publish event IDs to Pub/Sub topic 'event-analysis' for LLM processing
- Return JSON: {events_created, events_skipped, events_fetched}

**ReliefWeb (functions/reliefweb/main.py):**
- HTTP trigger
- Parse lookback_days from request JSON (default: 1)
- Fetch from ReliefWeb API
- Insert to BigQuery events table
- Publish to Pub/Sub
- Return JSON with stats

**FRED (functions/fred/main.py):**
- HTTP trigger
- Parse lookback_days (default: 7)
- Fetch 6 economic series
- Insert to BigQuery fred_indicators table
- Generate economic events for threshold breaches
- Return JSON with stats

**Comtrade (functions/comtrade/main.py):**
- HTTP trigger
- Parse lookback_months (default: 3)
- Fetch trade data
- Insert to BigQuery
- Return JSON with stats

**World Bank (functions/worldbank/main.py):**
- HTTP trigger
- Parse lookback_years (default: 2)
- Fetch 10 development indicators
- Insert to BigQuery
- Return JSON with stats

**Sanctions (functions/sanctions/main.py):**
- HTTP trigger
- Parse lookback_days (default: 7)
- Fetch OFAC SDN list
- Update BigQuery sanctions data
- Return JSON with stats

**Shared utilities (functions/shared/):**
- bigquery_client.py: Reusable BigQuery client setup with ADC auth
- secrets.py: GCP Secret Manager integration for API keys (FRED, Comtrade, World Bank)

**Why this approach:** Each function is standalone (independently deployable), reuses proven logic from current Celery tasks, and matches research blueprint. Avoid Cloud Run Jobs (unnecessary complexity for simple scheduled tasks). Use functions-framework for local testing.

**What to avoid:** Don't create monolithic function (defeats independent deployment). Don't use Cloud Run instead of Functions (overkill for <5min tasks). Don't skip Pub/Sub publishing (breaks LLM analysis chain).
  </action>
  <verify>
```bash
# Local testing with functions-framework
cd functions/gdelt_sync && functions-framework --target=sync_gdelt_events --debug
curl -X POST http://localhost:8080 -H "Content-Type: application/json" -d '{"lookback_minutes": 15}'

# Verify each function has valid requirements.txt
for dir in functions/*/; do python -m pip install -r "$dir/requirements.txt" --dry-run; done

# Check imports resolve
python -c "import sys; sys.path.append('functions/shared'); from bigquery_client import get_client"
```
  </verify>
  <done>All 6 function directories created with main.py + requirements.txt, shared/ utilities exist, local functions-framework test returns valid JSON, imports resolve without errors</done>
</task>

<task type="auto">
  <name>Task 2: Deploy Cloud Functions and configure Cloud Scheduler jobs</name>
  <files>scripts/deploy_ingestion_functions.sh, .gcloudignore</files>
  <action>
Create deployment script `scripts/deploy_ingestion_functions.sh` that:

**Deploy each function:**
```bash
gcloud functions deploy gdelt-sync \
  --gen2 \
  --runtime python311 \
  --region us-central1 \
  --source functions/gdelt_sync \
  --entry-point sync_gdelt_events \
  --trigger-http \
  --allow-unauthenticated=false \
  --timeout 540s \
  --memory 512MB \
  --max-instances 10 \
  --set-env-vars GCP_PROJECT_ID=venezuelawatch-447923,BIGQUERY_DATASET=venezuelawatch_analytics \
  --service-account ingestion-runner@venezuelawatch-447923.iam.gserviceaccount.com
```

Repeat for all 6 functions with appropriate:
- timeout: 540s (GDELT, ReliefWeb, FRED, Sanctions), 900s (Comtrade, World Bank for API latency)
- memory: 512MB (adequate for BigQuery client)
- max-instances: 10 (prevent runaway costs)

**Create Cloud Scheduler jobs:**
```bash
# GDELT: Every 15 minutes
gcloud scheduler jobs create http gdelt-sync-job \
  --location us-central1 \
  --schedule "*/15 * * * *" \
  --uri https://us-central1-venezuelawatch-447923.cloudfunctions.net/gdelt-sync \
  --http-method POST \
  --headers "Content-Type=application/json" \
  --message-body '{"lookback_minutes": 15}' \
  --oidc-service-account-email scheduler@venezuelawatch-447923.iam.gserviceaccount.com \
  --oidc-token-audience https://us-central1-venezuelawatch-447923.cloudfunctions.net/gdelt-sync

# ReliefWeb: Daily at 00:00 UTC
gcloud scheduler jobs create http reliefweb-sync-job \
  --location us-central1 \
  --schedule "0 0 * * *" \
  --uri https://us-central1-venezuelawatch-447923.cloudfunctions.net/reliefweb-sync \
  --http-method POST \
  --message-body '{"lookback_days": 1}' \
  --oidc-service-account-email scheduler@venezuelawatch-447923.iam.gserviceaccount.com

# FRED: Daily at 01:00 UTC
gcloud scheduler jobs create http fred-sync-job \
  --schedule "0 1 * * *" \
  --uri .../fred-sync \
  --message-body '{"lookback_days": 7}'

# Comtrade: Monthly on 1st at 02:00 UTC
gcloud scheduler jobs create http comtrade-sync-job \
  --schedule "0 2 1 * *" \
  --uri .../comtrade-sync \
  --message-body '{"lookback_months": 3}'

# World Bank: Quarterly on 1st of Jan/Apr/Jul/Oct at 03:00 UTC
gcloud scheduler jobs create http worldbank-sync-job \
  --schedule "0 3 1 1,4,7,10 *" \
  --uri .../worldbank-sync \
  --message-body '{"lookback_years": 2}'

# Sanctions: Daily at 04:00 UTC (match current schedule)
gcloud scheduler jobs create http sanctions-sync-job \
  --schedule "0 4 * * *" \
  --uri .../sanctions-sync \
  --message-body '{"lookback_days": 7}'
```

**IAM permissions (ensure these exist):**
- scheduler@venezuelawatch-447923.iam.gserviceaccount.com needs roles/run.invoker
- ingestion-runner@venezuelawatch-447923.iam.gserviceaccount.com needs roles/bigquery.dataEditor, roles/secretmanager.secretAccessor, roles/pubsub.publisher

**What to avoid:** Don't use --allow-unauthenticated (security risk). Don't use API keys for auth (OIDC is more secure). Don't skip max-instances (prevent cost overruns). Don't deploy to multiple regions yet (adds complexity, not needed for MVP).
  </action>
  <verify>
```bash
# Verify functions deployed
gcloud functions list --gen2 --region us-central1 | grep -E "(gdelt-sync|reliefweb-sync|fred-sync|comtrade-sync|worldbank-sync|sanctions-sync)"

# Verify scheduler jobs created
gcloud scheduler jobs list --location us-central1

# Test manual invocation
gcloud scheduler jobs run gdelt-sync-job --location us-central1

# Check function logs for successful execution
gcloud functions logs read gdelt-sync --gen2 --region us-central1 --limit 50
```
  </verify>
  <done>All 6 functions show in `gcloud functions list`, all 6 scheduler jobs exist, manual test invocation returns 200 with valid JSON response, function logs show successful BigQuery inserts</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>6 Cloud Functions with Cloud Scheduler triggers running in parallel with existing Celery tasks</what-built>
  <how-to-verify>
1. **Monitor parallel execution for 24-48 hours:**
   ```bash
   # Watch Cloud Scheduler executions
   gcloud scheduler jobs list --location us-central1

   # Check function execution counts (should match schedule)
   gcloud monitoring time-series list \
     --filter 'metric.type="cloudfunctions.googleapis.com/function/execution_count"' \
     --interval-start-time "$(date -u -d '24 hours ago' +%Y-%m-%dT%H:%M:%SZ)" \
     --interval-end-time "$(date -u +%Y-%m-%dT%H:%M:%SZ)"

   # Compare BigQuery event counts: Celery vs Cloud Functions
   # (Both should be inserting events - verify no duplicates and no data loss)
   bq query --use_legacy_sql=false '
     SELECT
       DATE(created_at) as date,
       COUNT(*) as events_from_functions,
       COUNT(DISTINCT id) as unique_events
     FROM `venezuelawatch-447923.venezuelawatch_analytics.events`
     WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 48 HOUR)
     GROUP BY date
     ORDER BY date DESC
   '
   ```

2. **Verify GDELT sync (most frequent - every 15min):**
   - Cloud Function should execute ~96 times per day (24h * 4 per hour)
   - Check logs show successful BigQuery inserts
   - No errors in Cloud Logging

3. **Verify daily tasks (ReliefWeb, FRED, Sanctions) executed at correct times:**
   - ReliefWeb at 00:00 UTC
   - FRED at 01:00 UTC
   - Sanctions at 04:00 UTC

4. **Check no data loss:**
   - All events from both systems present in BigQuery
   - No duplicate events (deduplication logic working)
   - Event counts match expected volume

5. **Monitor costs:**
   - Cloud Functions within free tier ($0-2/month expected)
   - Cloud Scheduler ~$0.60/month (6 jobs)

**Success criteria:** Cloud Functions execute on schedule, BigQuery receives data from both Celery and Functions without duplicates, no errors in logs, costs within expected range.
  </how-to-verify>
  <resume-signal>Type "approved" to proceed with cutover, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 6 Cloud Functions deployed and accessible
- [ ] All 6 Cloud Scheduler jobs created with correct cron schedules
- [ ] Manual test of each function returns 200 with valid JSON
- [ ] Parallel execution validated for 24-48 hours with zero data loss
- [ ] No duplicate events in BigQuery
- [ ] Cloud Logging shows no errors
</verification>

<success_criteria>

- All tasks completed
- 6 Cloud Functions deployed to us-central1
- 6 Cloud Scheduler jobs configured with matching frequencies
- Parallel validation confirms zero data loss
- No duplicate events from dual ingestion
- Ready for Celery cutover in Plan 18-03
</success_criteria>

<output>
After completion, create `.planning/phases/18-gcp-native-pipeline-migration/18-01-SUMMARY.md`:

# Phase 18 Plan 01: Ingestion Layer Migration Summary

**Ingestion layer migrated to Cloud Scheduler + Cloud Functions, validated in parallel with Celery**

## Accomplishments

- Created 6 standalone Cloud Functions for scheduled ingestion (GDELT, ReliefWeb, FRED, Comtrade, World Bank, Sanctions)
- Deployed Cloud Scheduler jobs matching current Celery Beat frequencies
- Validated parallel execution for 24-48 hours with zero data loss
- Confirmed no duplicate events in BigQuery from dual ingestion

## Files Created/Modified

- `functions/gdelt_sync/main.py` - GDELT 15-minute sync function
- `functions/reliefweb/main.py` - ReliefWeb daily sync
- `functions/fred/main.py` - FRED economic data daily sync
- `functions/comtrade/main.py` - UN Comtrade monthly sync
- `functions/worldbank/main.py` - World Bank quarterly sync
- `functions/sanctions/main.py` - Sanctions daily refresh
- `functions/shared/bigquery_client.py` - Shared BigQuery utilities
- `functions/shared/secrets.py` - GCP Secret Manager integration
- `scripts/deploy_ingestion_functions.sh` - Deployment automation

## Decisions Made

- Cloud Functions Gen2 (not Gen1) for better performance and VPC connector support
- OIDC authentication (not API keys) for Cloud Scheduler â†’ Functions security
- 512MB memory, 540-900s timeout based on task complexity
- Parallel validation period to ensure zero data loss before cutover

## Issues Encountered

None (or document any issues with resolutions)

## Next Step

Ready for Plan 18-02: Processing Layer Migration (Pub/Sub + Cloud Tasks for LLM/Entity processing)
</output>

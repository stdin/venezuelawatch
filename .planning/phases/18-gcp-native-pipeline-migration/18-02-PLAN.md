---
phase: 18-gcp-native-pipeline-migration
plan: 02
type: execute
---

<objective>
Migrate LLM intelligence and entity extraction processing from Celery workers to event-driven Cloud Run + Pub/Sub + Cloud Tasks architecture.

Purpose: Enable auto-scaling for processing workloads (0→100 instances), eliminate Celery worker management overhead, and integrate with serverless ingestion functions via Pub/Sub event triggers.
Output: Django app deployed to Cloud Run with Pub/Sub subscriptions for event analysis triggers and Cloud Tasks for queue management, replacing Celery workers entirely.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/GCP-NATIVE-ORCHESTRATION-RESEARCH.md
@.planning/phases/18-gcp-native-pipeline-migration/18-CONTEXT.md

**Current processing tasks (Celery):**
@backend/data_pipeline/tasks/intelligence_tasks.py
@backend/data_pipeline/tasks/entity_extraction.py

**API integration points:**
@backend/api/views/chat.py
@backend/api/views/events.py

**Key decisions from research:**
- Pub/Sub for event-driven triggers (ingestion → processing)
- Cloud Tasks for LLM analysis queue (handles retries, rate limiting)
- Cloud Run for Django app (scales 0→100, no worker pool management)
- Keep Chat API on same Cloud Run instance (unified deployment)
- Redis downscaled to trending cache only (no Celery queue)

**Processing patterns:**
- Intelligence analysis: 30s-2min per event (LLM tool calling loop)
- Entity extraction: 5-10s per event (NER + fuzzy matching)
- Trigger: Pub/Sub message from ingestion functions → Cloud Run handler → Cloud Task enqueue
</context>

<tasks>

<task type="auto">
  <name>Task 1: Set up Pub/Sub topics and Cloud Tasks queues</name>
  <files>scripts/setup_processing_infrastructure.sh</files>
  <action>
Create infrastructure setup script `scripts/setup_processing_infrastructure.sh`:

**Create Pub/Sub topics and subscriptions:**
```bash
# Topic for event analysis triggers (published by ingestion functions)
gcloud pubsub topics create event-analysis \
  --project venezuelawatch-447923

# Push subscription to Cloud Run handler endpoint
gcloud pubsub subscriptions create event-analysis-sub \
  --topic event-analysis \
  --push-endpoint https://venezuelawatch-api-XXXXX-uc.a.run.app/api/internal/process-event \
  --push-auth-service-account pubsub-invoker@venezuelawatch-447923.iam.gserviceaccount.com \
  --ack-deadline 600 \
  --message-retention-duration 7d \
  --max-delivery-attempts 5

# Topic for entity extraction (published after intelligence analysis)
gcloud pubsub topics create entity-extraction \
  --project venezuelawatch-447923

gcloud pubsub subscriptions create entity-extraction-sub \
  --topic entity-extraction \
  --push-endpoint https://venezuelawatch-api-XXXXX-uc.a.run.app/api/internal/extract-entities \
  --push-auth-service-account pubsub-invoker@venezuelawatch-447923.iam.gserviceaccount.com
```

**Create Cloud Tasks queues:**
```bash
# Queue for LLM intelligence analysis (handles retries, rate limiting)
gcloud tasks queues create intelligence-analysis \
  --location us-central1 \
  --max-concurrent-dispatches 10 \
  --max-dispatches-per-second 5 \
  --max-attempts 3 \
  --min-backoff 60s \
  --max-backoff 3600s

# Queue for entity extraction
gcloud tasks queues create entity-extraction \
  --location us-central1 \
  --max-concurrent-dispatches 20 \
  --max-dispatches-per-second 10 \
  --max-attempts 3
```

**IAM permissions:**
```bash
# Grant Pub/Sub publisher role to ingestion functions service account
gcloud pubsub topics add-iam-policy-binding event-analysis \
  --member serviceAccount:ingestion-runner@venezuelawatch-447923.iam.gserviceaccount.com \
  --role roles/pubsub.publisher

# Grant Cloud Tasks enqueuer role to Cloud Run service account
gcloud projects add-iam-policy-binding venezuelawatch-447923 \
  --member serviceAccount:cloudrun-tasks@venezuelawatch-447923.iam.gserviceaccount.com \
  --role roles/cloudtasks.enqueuer

# Grant invoker role to Pub/Sub service account
gcloud run services add-iam-policy-binding venezuelawatch-api \
  --region us-central1 \
  --member serviceAccount:pubsub-invoker@venezuelawatch-447923.iam.gserviceaccount.com \
  --role roles/run.invoker
```

**Why this approach:** Pub/Sub decouples ingestion from processing (ingestion functions publish, don't wait). Cloud Tasks handles retries and rate limiting for LLM API. Cloud Run auto-scales based on incoming requests. Research validates this as optimal for async processing workloads.

**What to avoid:** Don't use pull subscriptions (adds polling overhead). Don't skip ack-deadline tuning (LLM analysis can take 1-2 min). Don't use default max-attempts (3 retries sufficient for transient errors).
  </action>
  <verify>
```bash
# Verify topics created
gcloud pubsub topics list --project venezuelawatch-447923 | grep -E "(event-analysis|entity-extraction)"

# Verify subscriptions created
gcloud pubsub subscriptions list --project venezuelawatch-447923

# Verify queues created
gcloud tasks queues list --location us-central1

# Test publishing to topic
gcloud pubsub topics publish event-analysis \
  --message '{"event_id": "test-123"}' \
  --project venezuelawatch-447923

# Check subscription received message
gcloud pubsub subscriptions pull event-analysis-sub --limit 1 --auto-ack
```
  </verify>
  <done>event-analysis and entity-extraction topics exist, subscriptions created with push endpoints, Cloud Tasks queues created with rate limits, IAM permissions granted, test message publishes successfully</done>
</task>

<task type="auto">
  <name>Task 2: Migrate intelligence and entity tasks to event-driven handlers</name>
  <files>backend/api/views/internal.py, backend/data_pipeline/tasks/intelligence_tasks.py, backend/data_pipeline/tasks/entity_extraction.py</files>
  <action>
Create new internal API handlers in `backend/api/views/internal.py` for Pub/Sub push endpoints:

**Pub/Sub handler for event analysis:**
```python
from ninja import Router
from django.http import JsonResponse
from google.cloud import tasks_v2
import json
import base64

internal_router = Router()

@internal_router.post('/process-event')
def process_event_pubsub(request):
    """
    Pub/Sub push endpoint for event analysis triggers.

    Receives event IDs from ingestion functions, enqueues to Cloud Tasks.
    """
    # Parse Pub/Sub message
    envelope = json.loads(request.body.decode('utf-8'))
    if 'message' not in envelope:
        return JsonResponse({'error': 'Invalid Pub/Sub message'}, status=400)

    pubsub_message = envelope['message']
    event_data = json.loads(base64.b64decode(pubsub_message['data']))
    event_id = event_data.get('event_id')

    if not event_id:
        return JsonResponse({'error': 'Missing event_id'}, status=400)

    # Enqueue to Cloud Tasks for LLM analysis
    client = tasks_v2.CloudTasksClient()
    parent = client.queue_path('venezuelawatch-447923', 'us-central1', 'intelligence-analysis')

    task = {
        'http_request': {
            'http_method': tasks_v2.HttpMethod.POST,
            'url': 'https://venezuelawatch-api-XXXXX-uc.a.run.app/api/internal/analyze-intelligence',
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({'event_id': event_id}).encode(),
            'oidc_token': {
                'service_account_email': 'cloudrun-tasks@venezuelawatch-447923.iam.gserviceaccount.com'
            }
        }
    }

    client.create_task(request={'parent': parent, 'task': task})

    return JsonResponse({'status': 'enqueued', 'event_id': event_id}, status=200)

@internal_router.post('/analyze-intelligence')
def analyze_intelligence_task(request):
    """
    Cloud Tasks handler for LLM intelligence analysis.

    Replaces Celery analyze_event_intelligence task.
    """
    data = json.loads(request.body.decode('utf-8'))
    event_id = data.get('event_id')

    # Reuse existing LLM intelligence logic from intelligence_tasks.py
    from data_pipeline.services.llm_intelligence import LLMIntelligence
    from api.services.bigquery_service import bigquery_service

    event = bigquery_service.get_event_by_id(event_id)
    if not event:
        return JsonResponse({'error': 'Event not found'}, status=404)

    # Run LLM analysis (same logic as Celery task)
    llm_service = LLMIntelligence()
    analysis = llm_service.analyze_comprehensive(event['content'])

    # Update event in BigQuery with analysis results
    bigquery_service.update_event_metadata(event_id, analysis)

    # Publish to entity-extraction topic
    from google.cloud import pubsub_v1
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path('venezuelawatch-447923', 'entity-extraction')
    publisher.publish(topic_path, json.dumps({'event_id': event_id}).encode())

    return JsonResponse({'status': 'analyzed', 'event_id': event_id}, status=200)

@internal_router.post('/extract-entities')
def extract_entities_pubsub(request):
    """
    Pub/Sub handler for entity extraction.

    Replaces Celery extract_entities task.
    """
    envelope = json.loads(request.body.decode('utf-8'))
    pubsub_message = envelope['message']
    event_data = json.loads(base64.b64decode(pubsub_message['data']))
    event_id = event_data.get('event_id')

    # Reuse existing entity extraction logic
    from data_pipeline.services.entity_service import extract_and_save_entities
    extract_and_save_entities(event_id)

    return JsonResponse({'status': 'extracted', 'event_id': event_id}, status=200)
```

**Update intelligence_tasks.py and entity_extraction.py:**
- Keep existing logic functions (llm_intelligence.analyze_comprehensive, entity_service methods)
- Mark Celery @shared_task decorators as deprecated with warnings
- Add docstring notes: "Replaced by Cloud Run handlers in api/views/internal.py"

**Why this approach:** Reuse 100% of existing business logic (LLM analysis, entity extraction). Only replace Celery task orchestration with Pub/Sub+Tasks. Cloud Run handlers are HTTP endpoints (standard Django views). Research confirms this minimizes migration risk.

**What to avoid:** Don't rewrite LLM analysis logic (error-prone). Don't delete Celery tasks yet (cleanup in Plan 18-03). Don't make handlers public (internal/ prefix, OIDC auth only).
  </action>
  <verify>
```bash
# Test Pub/Sub → Cloud Run flow locally
python manage.py runserver

# Simulate Pub/Sub message
curl -X POST http://localhost:8000/api/internal/process-event \
  -H "Content-Type: application/json" \
  -d '{
    "message": {
      "data": "'$(echo -n '{"event_id": "test-event-123"}' | base64)'",
      "messageId": "test-123",
      "publishTime": "2026-01-09T00:00:00Z"
    }
  }'

# Verify task enqueued to Cloud Tasks
gcloud tasks list --queue intelligence-analysis --location us-central1

# Check BigQuery event updated with analysis
bq query --use_legacy_sql=false '
  SELECT id, metadata.sentiment, metadata.risk_score
  FROM `venezuelawatch-447923.venezuelawatch_analytics.events`
  WHERE id = "test-event-123"
'
```
  </verify>
  <done>internal.py handlers created, Pub/Sub messages parsed correctly, Cloud Tasks enqueued, LLM analysis executes and updates BigQuery, entity extraction triggered via Pub/Sub, no errors in logs</done>
</task>

<task type="auto">
  <name>Task 3: Update Chat API and ingestion functions to publish Pub/Sub events</name>
  <files>backend/api/views/chat.py, functions/gdelt_sync/main.py, functions/reliefweb/main.py, functions/fred/main.py, functions/shared/pubsub_publisher.py</files>
  <action>
**Update Chat API to trigger analysis via Pub/Sub:**

In `backend/api/views/chat.py`, replace Celery task calls with Pub/Sub publishing:

```python
# OLD (Celery):
# from data_pipeline.tasks.intelligence_tasks import analyze_event_intelligence
# analyze_event_intelligence.apply_async(args=[event_id])

# NEW (Pub/Sub):
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path('venezuelawatch-447923', 'event-analysis')

def trigger_event_analysis(event_id: str):
    """Publish event analysis trigger to Pub/Sub."""
    message_data = json.dumps({'event_id': event_id}).encode('utf-8')
    future = publisher.publish(topic_path, message_data)
    future.result()  # Wait for publish confirmation
```

Replace all `analyze_event_intelligence.apply_async()` calls with `trigger_event_analysis(event_id)`.

**Update ingestion Cloud Functions to publish events:**

Add to each function's main.py (already drafted in Plan 18-01, now verify implementation):

```python
from google.cloud import pubsub_v1

def publish_event_for_analysis(event_id: str):
    """Publish event ID to Pub/Sub for intelligence analysis."""
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path('venezuelawatch-447923', 'event-analysis')
    message_data = json.dumps({'event_id': event_id}).encode('utf-8')
    publisher.publish(topic_path, message_data)

# In main handler, after BigQuery insert:
for event_id in created_event_ids:
    publish_event_for_analysis(event_id)
```

**Create shared Pub/Sub publisher utility:**

In `functions/shared/pubsub_publisher.py`:
```python
from google.cloud import pubsub_v1
import json
import os

class PubSubPublisher:
    def __init__(self):
        self.client = pubsub_v1.PublisherClient()
        self.project_id = os.environ.get('GCP_PROJECT_ID', 'venezuelawatch-447923')

    def publish_event_analysis(self, event_id: str):
        """Publish event for intelligence analysis."""
        topic_path = self.client.topic_path(self.project_id, 'event-analysis')
        message = json.dumps({'event_id': event_id}).encode('utf-8')
        future = self.client.publish(topic_path, message)
        return future.result()  # Block until published

publisher = PubSubPublisher()
```

**Why this approach:** Pub/Sub decouples ingestion from processing (functions don't wait for LLM). Chat API triggers analysis same way as ingestion. Shared utility ensures consistent publishing pattern. Research validates pub/sub for event-driven async workflows.

**What to avoid:** Don't wait for analysis completion in sync requests (blocks user). Don't publish to wrong topic (breaks flow). Don't skip error handling on publish failures.
  </action>
  <verify>
```bash
# Test Chat API triggers Pub/Sub
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What are the latest GDELT events?"}'

# Verify Pub/Sub message published
gcloud pubsub subscriptions pull event-analysis-sub --limit 5 --auto-ack

# Test Cloud Function publishes events
gcloud scheduler jobs run gdelt-sync-job --location us-central1

# Check Cloud Tasks queue has tasks
gcloud tasks list --queue intelligence-analysis --location us-central1 --limit 10

# Verify end-to-end: ingestion → Pub/Sub → Tasks → analysis → BigQuery
# Should see events with populated metadata.sentiment, metadata.risk_score
```
  </verify>
  <done>Chat API publishes to Pub/Sub instead of calling Celery, all ingestion functions publish event IDs after BigQuery insert, Pub/Sub messages delivered to Cloud Run handlers, Cloud Tasks enqueue analysis tasks, end-to-end flow verified</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Pub/Sub topics and subscriptions created
- [ ] Cloud Tasks queues configured with rate limits
- [ ] Cloud Run internal handlers respond to Pub/Sub pushes
- [ ] LLM intelligence analysis executes via Cloud Tasks
- [ ] Entity extraction triggered via Pub/Sub after analysis
- [ ] Chat API publishes to Pub/Sub (not Celery)
- [ ] Ingestion functions publish events for analysis
- [ ] End-to-end flow: ingestion → Pub/Sub → Tasks → analysis → BigQuery update
</verification>

<success_criteria>

- All tasks completed
- Processing layer migrated from Celery to Pub/Sub + Cloud Tasks
- Django app deployed to Cloud Run with internal handlers
- Zero data loss during migration
- LLM analysis and entity extraction execute successfully
- Ready for Celery removal in Plan 18-03
</success_criteria>

<output>
After completion, create `.planning/phases/18-gcp-native-pipeline-migration/18-02-SUMMARY.md`:

# Phase 18 Plan 02: Processing Layer Migration Summary

**LLM intelligence and entity extraction migrated to event-driven Cloud Run + Pub/Sub + Cloud Tasks**

## Accomplishments

- Created Pub/Sub topics (event-analysis, entity-extraction) with push subscriptions to Cloud Run
- Created Cloud Tasks queues (intelligence-analysis, entity-extraction) with retry policies
- Migrated LLM intelligence task to Cloud Run HTTP handler triggered by Cloud Tasks
- Migrated entity extraction to Pub/Sub push handler
- Updated Chat API to trigger analysis via Pub/Sub (not Celery)
- Updated all ingestion functions to publish events for analysis

## Files Created/Modified

- `backend/api/views/internal.py` - Pub/Sub and Cloud Tasks handlers
- `functions/shared/pubsub_publisher.py` - Shared Pub/Sub publishing utility
- `backend/api/views/chat.py` - Replaced Celery with Pub/Sub triggers
- `functions/gdelt_sync/main.py` - Added Pub/Sub publishing
- `functions/reliefweb/main.py` - Added Pub/Sub publishing
- `functions/fred/main.py` - Added Pub/Sub publishing
- `scripts/setup_processing_infrastructure.sh` - Infrastructure automation

## Decisions Made

- Pub/Sub push subscriptions (not pull) for lower latency and simpler code
- Cloud Tasks for LLM analysis queue (handles retries, rate limiting automatically)
- Keep existing LLM/entity logic 100% unchanged (only replace orchestration)
- OIDC authentication for internal endpoints (not public)

## Issues Encountered

None (or document any issues with resolutions)

## Next Step

Ready for Plan 18-03: Cutover & Infrastructure Cleanup (disable Celery, remove dependencies)
</output>
